{
  "tree": {
    "name": "backend",
    "type": "folder",
    "children": [
      {
        "name": "data",
        "type": "folder",
        "children": [
          {
            "name": "index",
            "type": "folder",
            "children": []
          },
          {
            "name": "sandbox",
            "type": "folder",
            "children": []
          }
        ]
      },
      {
        "name": "logs",
        "type": "folder",
        "children": []
      },
      {
        "name": "models",
        "type": "folder",
        "children": [
          {
            "name": "bge-m3",
            "type": "folder",
            "children": [
              {
                "name": "1_Pooling",
                "type": "folder",
                "children": []
              },
              {
                "name": "2_Normalize",
                "type": "folder",
                "children": []
              }
            ]
          },
          {
            "name": "download_bge_m3.py",
            "type": "file"
          }
        ]
      },
      {
        "name": "src",
        "type": "folder",
        "children": [
          {
            "name": "__init__.py",
            "type": "file"
          },
          {
            "name": "agents",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "orchestrator.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "app",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "config.py",
                "type": "file"
              },
              {
                "name": "main.py",
                "type": "file"
              },
              {
                "name": "routes",
                "type": "folder",
                "children": [
                  {
                    "name": "__init__.py",
                    "type": "file"
                  },
                  {
                    "name": "chat.py",
                    "type": "file"
                  },
                  {
                    "name": "debug.py",
                    "type": "file"
                  },
                  {
                    "name": "graph.py",
                    "type": "file"
                  },
                  {
                    "name": "indexing.py",
                    "type": "file"
                  },
                  {
                    "name": "search.py",
                    "type": "file"
                  },
                  {
                    "name": "v1_1_routes.py",
                    "type": "file"
                  }
                ]
              },
              {
                "name": "websocket_handler.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "embeddings",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "embedding_pipeline.py",
                "type": "file"
              },
              {
                "name": "embedding_service.py",
                "type": "file"
              },
              {
                "name": "vector_store.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "graphs",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "asg_builder.py",
                "type": "file"
              },
              {
                "name": "cfg_builder.py",
                "type": "file"
              },
              {
                "name": "neo4j_client.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "indexing",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "ast_parser.py",
                "type": "file"
              },
              {
                "name": "chunker.py",
                "type": "file"
              },
              {
                "name": "manifest.py",
                "type": "file"
              },
              {
                "name": "walker.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "intelligence",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "code_health_db.py",
                "type": "file"
              },
              {
                "name": "health_scanner.py",
                "type": "file"
              },
              {
                "name": "import_resolver.py",
                "type": "file"
              },
              {
                "name": "static_analyzer.py",
                "type": "file"
              },
              {
                "name": "symbol_tracer.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "memory",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "error_memory.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "patching",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "diff_generator.py",
                "type": "file"
              },
              {
                "name": "multi_file_planner.py",
                "type": "file"
              },
              {
                "name": "validator.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "reasoning",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "llm_client.py",
                "type": "file"
              }
            ]
          },
          {
            "name": "retrieval",
            "type": "folder",
            "children": [
              {
                "name": "__init__.py",
                "type": "file"
              },
              {
                "name": "context_packer.py",
                "type": "file"
              },
              {
                "name": "error_path_retrieval.py",
                "type": "file"
              },
              {
                "name": "ranking.py",
                "type": "file"
              },
              {
                "name": "semantic_search.py",
                "type": "file"
              }
            ]
          }
        ]
      },
      {
        "name": "tests",
        "type": "folder",
        "children": [
          {
            "name": "test_api.py",
            "type": "file"
          },
          {
            "name": "test_embeddings.py",
            "type": "file"
          },
          {
            "name": "test_error_memory.py",
            "type": "file"
          },
          {
            "name": "test_ranking.py",
            "type": "file"
          },
          {
            "name": "test_static_analyzer.py",
            "type": "file"
          },
          {
            "name": "test_walker.py",
            "type": "file"
          }
        ]
      }
    ]
  },
  "files": {
    "models\\download_bge_m3.py": "\"\"\"\nBGE-M3 Embedding Model Downloader\n\nThis script downloads the BGE-M3 model from HuggingFace\nand saves it locally for offline usage.\n\"\"\"\n\nimport os\nimport sys\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    from rich.console import Console\n    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n    import torch\nexcept ImportError:\n    print(\"\u274c Missing dependencies. Please install requirements first:\")\n    print(\"   pip install sentence-transformers rich torch\")\n    sys.exit(1)\n\nconsole = Console()\n\n# Model configuration\nMODEL_NAME = \"BAAI/bge-m3\"\nMODEL_PATH = Path(__file__).parent / \"bge-m3\"\n\n\ndef check_gpu_availability() -> bool:\n    \"\"\"Check if CUDA GPU is available.\"\"\"\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        console.print(f\"\u2705 GPU detected: [green]{gpu_name}[/green]\")\n        return True\n    else:\n        console.print(\"\u26a0\ufe0f  No GPU detected. Model will use CPU (slower).\")\n        return False\n\n\ndef calculate_dir_hash(directory: Path) -> str:\n    \"\"\"Calculate hash of all files in directory for verification.\"\"\"\n    hasher = hashlib.sha256()\n    \n    for filepath in sorted(directory.rglob(\"*\")):\n        if filepath.is_file():\n            with open(filepath, \"rb\") as f:\n                hasher.update(f.read())\n    \n    return hasher.hexdigest()\n\n\ndef download_model() -> bool:\n    \"\"\"Download BGE-M3 model from HuggingFace.\"\"\"\n    try:\n        console.print(f\"\\n\ud83d\udd3d Downloading BGE-M3 model from HuggingFace...\")\n        console.print(f\"   Source: [cyan]{MODEL_NAME}[/cyan]\")\n        console.print(f\"   Destination: [cyan]{MODEL_PATH}[/cyan]\\n\")\n        \n        # Create model directory if it doesn't exist\n        MODEL_PATH.mkdir(parents=True, exist_ok=True)\n        \n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            console=console,\n        ) as progress:\n            task = progress.add_task(\"Downloading model...\", total=None)\n            \n            # Download model\n            model = SentenceTransformer(MODEL_NAME)\n            \n            progress.update(task, description=\"Saving model locally...\")\n            \n            # Save to local path\n            model.save(str(MODEL_PATH))\n            \n            progress.update(task, description=\"\u2705 Download complete!\", completed=True)\n        \n        console.print(f\"\\n\u2705 Model successfully downloaded to: [green]{MODEL_PATH}[/green]\")\n        return True\n        \n    except Exception as e:\n        console.print(f\"\\n\u274c Error downloading model: [red]{str(e)}[/red]\")\n        return False\n\n\ndef verify_model() -> bool:\n    \"\"\"Verify the downloaded model works correctly.\"\"\"\n    try:\n        console.print(\"\\n\ud83d\udd0d Verifying model...\")\n        \n        # Load model\n        model = SentenceTransformer(str(MODEL_PATH))\n        \n        # Test embedding generation\n        test_text = \"This is a test sentence for BGE-M3 embedding model.\"\n        embedding = model.encode(test_text)\n        \n        console.print(f\"   \u2705 Model loaded successfully\")\n        console.print(f\"   \u2705 Embedding dimension: [cyan]{len(embedding)}[/cyan]\")\n        console.print(f\"   \u2705 Model is ready to use!\")\n        \n        return True\n        \n    except Exception as e:\n        console.print(f\"\\n\u274c Model verification failed: [red]{str(e)}[/red]\")\n        return False\n\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    console.print(\"\\n\" + \"=\"*60)\n    console.print(\"  BGE-M3 Embedding Model Downloader\")\n    console.print(\"=\"*60 + \"\\n\")\n    \n    # Check if model already exists\n    if MODEL_PATH.exists() and any(MODEL_PATH.iterdir()):\n        console.print(f\"\u26a0\ufe0f  Model directory already exists: [yellow]{MODEL_PATH}[/yellow]\")\n        response = input(\"\\nDo you want to re-download? (y/N): \").strip().lower()\n        \n        if response != 'y':\n            console.print(\"\\n\u2705 Using existing model. Verifying...\")\n            if verify_model():\n                console.print(\"\\n\u2728 Model is ready! You can now run the Vibe Agent.\\n\")\n                return 0\n            else:\n                console.print(\"\\n\u274c Existing model is corrupted. Please re-download.\\n\")\n                return 1\n    \n    # Check GPU\n    has_gpu = check_gpu_availability()\n    \n    # Download model\n    if not download_model():\n        return 1\n    \n    # Verify model\n    if not verify_model():\n        return 1\n    \n    console.print(\"\\n\" + \"=\"*60)\n    console.print(\"  \u2728 Setup Complete!\")\n    console.print(\"=\"*60)\n    console.print(\"\\nNext steps:\")\n    console.print(\"  1. Configure your .env file\")\n    console.print(\"  2. Start the backend: cd backend && uvicorn src.app.main:app --reload\")\n    console.print(\"  3. Start the frontend: cd frontend && npm run dev\")\n    console.print(\"\\n\")\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
    "src\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\agents\\orchestrator.py": "\"\"\"\nAgent Orchestrator\n\nCoordinates specialized agents for different tasks.\n\"\"\"\n\nfrom typing import Dict, Any, Optional, List\nfrom enum import Enum\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgentType(Enum):\n    \"\"\"Types of specialized agents.\"\"\"\n    QUERY = \"query\"  # Understands and refines queries\n    RETRIEVAL = \"retrieval\"  # Retrieves relevant code\n    BUG_LOCALIZATION = \"bug_localization\"  # Locates bugs\n    ROOT_CAUSE = \"root_cause\"  # Analyzes root causes\n    REASONING = \"reasoning\"  # General reasoning\n    PATCH = \"patch\"  # Generates patches\n    REFACTOR = \"refactor\"  # Suggests refactorings\n\n\nclass AgentOrchestrator:\n    \"\"\"\n    Orchestrates specialized agents to solve complex tasks.\n    \"\"\"\n    \n    def __init__(self, llm_client, retrieval_system, memory_db, neo4j_client):\n        \"\"\"\n        Initialize orchestrator.\n        \n        Args:\n            llm_client: LLM client instance\n            retrieval_system: Retrieval system instance\n            memory_db: Error memory database\n            neo4j_client: Neo4j client for graph queries\n        \"\"\"\n        self.llm = llm_client\n        self.retrieval = retrieval_system\n        self.memory = memory_db\n        self.graph = neo4j_client\n        \n        # Initialize agents (would be separate classes in production)\n        self.agents = {\n            AgentType.QUERY: self._create_query_agent(),\n            AgentType.RETRIEVAL: self._create_retrieval_agent(),\n            AgentType.BUG_LOCALIZATION: self._create_bug_agent(),\n            AgentType.ROOT_CAUSE: self._create_root_cause_agent(),\n            AgentType.REASONING: self._create_reasoning_agent(),\n            AgentType.PATCH: self._create_patch_agent(),\n            AgentType.REFACTOR: self._create_refactor_agent(),\n        }\n        \n        logger.info(f\"Initialized orchestrator with {len(self.agents)} agents\")\n    \n    def _create_query_agent(self):\n        \"\"\"Create query understanding agent.\"\"\"\n        return {\n            'name': 'Query Agent',\n            'role': 'Understand and refine user queries',\n            'system_prompt': '''You are a query understanding agent. Your role is to:\n1. Parse and understand user queries about code\n2. Identify the intent (bug fix, explanation, refactoring, etc.)\n3. Extract key entities (files, functions, errors)\n4. Refine vague queries into specific searchable terms\n5. Generate search keywords for code retrieval\n\nOutput a JSON object with: intent, entities, keywords, refined_query.'''\n        }\n    \n    def _create_retrieval_agent(self):\n        \"\"\"Create retrieval agent.\"\"\"\n        return {\n            'name': 'Retrieval Agent',\n            'role': 'Retrieve relevant code from vector store and graph',\n            'system_prompt': '''You are a code retrieval agent. Your role is to:\n1. Use semantic search to find relevant code chunks\n2. Expand context using code graph relationships\n3. Rank results by relevance\n4. Ensure comprehensive coverage of related code\n\nReturn the most relevant code chunks.'''\n        }\n    \n    def _create_bug_agent(self):\n        \"\"\"Create bug localization agent.\"\"\"\n        return {\n            'name': 'Bug Localization Agent',\n            'role': 'Locate bugs in code',\n            'system_prompt': '''You are a bug localization agent. Your role is to:\n1. Analyze error messages and stack traces\n2. Identify the exact line where the bug occurs\n3. Find related code that contributes to the bug\n4. Score code locations by likelihood of being buggy\n5. Provide reasoning for each location\n\nOutput a ranked list of suspicious code locations with explanations.'''\n        }\n    \n    def _create_root_cause_agent(self):\n        \"\"\"Create root cause analysis agent.\"\"\"\n        return {\n            'name': 'Root Cause Agent',\n            'role': 'Analyze root causes of bugs',\n            'system_prompt': '''You are a root cause analysis agent. Your role is to:\n1. Analyze bug locations and surrounding code\n2. Trace data flow and control flow\n3. Identify the fundamental cause of the issue\n4. Explain why the bug occurs\n5. Suggest the type of fix needed\n\nProvide a detailed root cause analysis with reasoning.'''\n        }\n    \n    def _create_reasoning_agent(self):\n        \"\"\"Create general reasoning agent.\"\"\"\n        return {\n            'name': 'Reasoning Agent',\n            'role': 'General code reasoning and analysis',\n            'system_prompt': '''You are a code reasoning agent. Your role is to:\n1. Understand complex code structures\n2. Explain how code works\n3. Identify patterns and anti-patterns\n4. Make logical deductions about behavior\n5. Answer questions about code functionality\n\nProvide clear, accurate explanations.'''\n        }\n    \n    def _create_patch_agent(self):\n        \"\"\"Create patch generation agent.\"\"\"\n        return {\n            'name': 'Patch Agent',\n            'role': 'Generate code patches to fix bugs',\n            'system_prompt': '''You are a patch generation agent. Your role is to:\n1. Generate precise code patches to fix identified bugs\n2. Ensure patches are minimal and focused\n3. Maintain code style consistency\n4. Add comments explaining the fix\n5. Provide before/after diffs\n\nOutput valid code that can be directly applied.'''\n        }\n    \n    def _create_refactor_agent(self):\n        \"\"\"Create refactoring agent.\"\"\"\n        return {\n            'name': 'Refactor Agent',\n            'role': 'Suggest code refactorings',\n            'system_prompt': '''You are a refactoring agent. Your role is to:\n1. Identify code that needs refactoring\n2. Suggest specific refactoring techniques\n3. Generate refactored code\n4. Explain the benefits of each refactoring\n5. Ensure functionality is preserved\n\nProvide actionable refactoring suggestions.'''\n        }\n    \n    def execute_task(\n        self,\n        task_type: str,\n        query: str,\n        context: Optional[Dict[str, Any]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Execute a task using appropriate agents.\n        \n        Args:\n            task_type: Type of task (debug, explain, refactor, etc.)\n            query: User query\n            context: Optional context information\n        \n        Returns:\n            Task result\n        \"\"\"\n        logger.info(f\"Executing task: {task_type}\")\n        \n        context = context or {}\n        \n        # Step 1: Query understanding\n        query_result = self._run_query_agent(query)\n        \n        # Step 2: Retrieval\n        retrieval_result = self._run_retrieval_agent(query_result)\n        \n        # Step 3: Task-specific processing\n        if task_type == 'debug':\n            return self._execute_debug_task(query, query_result, retrieval_result, context)\n        elif task_type == 'explain':\n            return self._execute_explain_task(query, retrieval_result)\n        elif task_type == 'refactor':\n            return self._execute_refactor_task(query, retrieval_result)\n        else:\n            return {'error': f'Unknown task type: {task_type}'}\n    \n    def _run_query_agent(self, query: str) -> Dict[str, Any]:\n        \"\"\"Run query understanding agent.\"\"\"\n        agent = self.agents[AgentType.QUERY]\n        \n        # In production, this would call LLM\n        # For now, simple parsing\n        return {\n            'intent': 'debug',\n            'keywords': query.split(),\n            'refined_query': query\n        }\n    \n    def _run_retrieval_agent(self, query_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Run retrieval agent.\"\"\"\n        # Would use actual retrieval system\n        return {\n            'chunks': [],\n            'total': 0\n        }\n    \n    def _execute_debug_task(\n        self,\n        query: str,\n        query_result: Dict[str, Any],\n        retrieval_result: Dict[str, Any],\n        context: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute debugging task.\"\"\"\n        # Step 1: Bug localization\n        bug_locations = self._run_bug_localization(query, retrieval_result, context)\n        \n        # Step 2: Root cause analysis\n        root_cause = self._run_root_cause_analysis(bug_locations, retrieval_result)\n        \n        # Step 3: Patch generation\n        patch = self._run_patch_generation(root_cause, bug_locations)\n        \n        return {\n            'task': 'debug',\n            'bug_locations': bug_locations,\n            'root_cause': root_cause,\n            'patch': patch\n        }\n    \n    def _execute_explain_task(\n        self,\n        query: str,\n        retrieval_result: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute explanation task.\"\"\"\n        # Use reasoning agent\n        explanation = self._run_reasoning_agent(query, retrieval_result)\n        \n        return {\n            'task': 'explain',\n            'explanation': explanation\n        }\n    \n    def _execute_refactor_task(\n        self,\n        query: str,\n        retrieval_result: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute refactoring task.\"\"\"\n        # Use refactor agent\n        refactoring = self._run_refactor_agent(query, retrieval_result)\n        \n        return {\n            'task': 'refactor',\n            'refactoring': refactoring\n        }\n    \n    def _run_bug_localization(\n        self,\n        query: str,\n        retrieval_result: Dict[str, Any],\n        context: Dict[str, Any]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Run bug localization agent.\"\"\"\n        # Would use LLM with bug agent prompt\n        return []\n    \n    def _run_root_cause_analysis(\n        self,\n        bug_locations: List[Dict[str, Any]],\n        retrieval_result: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Run root cause analysis agent.\"\"\"\n        # Would use LLM with root cause agent prompt\n        return {}\n    \n    def _run_patch_generation(\n        self,\n        root_cause: Dict[str, Any],\n        bug_locations: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"Run patch generation agent.\"\"\"\n        # Would use LLM with patch agent prompt\n        return {}\n    \n    def _run_reasoning_agent(\n        self,\n        query: str,\n        retrieval_result: Dict[str, Any]\n    ) -> str:\n        \"\"\"Run reasoning agent.\"\"\"\n        # Would use LLM with reasoning agent prompt\n        return \"\"\n    \n    def _run_refactor_agent(\n        self,\n        query: str,\n        retrieval_result: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Run refactor agent.\"\"\"\n        # Would use LLM with refactor agent prompt\n        return {}\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    print(\"\\n\ud83e\udd16 Agent Orchestrator\")\n    print(\"Initialized with 7 specialized agents:\")\n    \n    for agent_type in AgentType:\n        print(f\"  - {agent_type.value}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\agents\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\app\\config.py": "\"\"\"\nConfiguration Management\n\nLoads and manages all application settings from environment variables.\n\"\"\"\n\nfrom typing import Optional\nfrom pydantic_settings import BaseSettings\nfrom pydantic import Field\nfrom functools import lru_cache\nfrom pathlib import Path\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application configuration loaded from environment variables.\"\"\"\n    \n    # Target Repository\n    target_repo_path: str = Field(default=\"\", env=\"TARGET_REPO_PATH\")\n    \n    # LLM Configuration\n    llm_model_type: str = Field(default=\"gemini-2.0-flash-exp\", env=\"LLM_MODEL_TYPE\")\n    llm_fallback_model_type: str = Field(default=\"gemini-1.5-pro\", env=\"LLM_FALLBACK_MODEL_TYPE\")\n    gemini_api_key: str = Field(default=\"\", env=\"GEMINI_API_KEY\")\n    \n    # Embedding Model\n    embedding_model_type: str = Field(default=\"bge-m3\", env=\"EMBEDDING_MODEL_TYPE\")\n    embedding_model_path: str = Field(default=\"./models/bge-m3\", env=\"EMBEDDING_MODEL_PATH\")\n    use_local_embeddings: bool = Field(default=True, env=\"USE_LOCAL_EMBEDDINGS\")\n    \n    # Token Limits\n    max_tokens_per_request: int = Field(default=70000, env=\"MAX_TOKENS_PER_REQUEST\")\n    system_prompt_reserved_tokens: int = Field(default=3000, env=\"SYSTEM_PROMPT_RESERVED_TOKENS\")\n    max_output_tokens: int = Field(default=8192, env=\"MAX_OUTPUT_TOKENS\")\n    \n    # Rate Limiting\n    rate_limit_req_per_min: int = Field(default=50, env=\"RATE_LIMIT_REQ_PER_MIN\")\n    rate_limit_tokens_per_min: int = Field(default=2000000, env=\"RATE_LIMIT_TOKENS_PER_MIN\")\n    \n    # Redis\n    redis_url: str = Field(default=\"redis://localhost:6379/0\", env=\"REDIS_URL\")\n    \n    # Vector Database (FAISS)\n    vector_db_path: str = Field(default=\"./data/vector_db\", env=\"VECTOR_DB_PATH\")\n    \n    # Graph Database (Neo4j)\n    neo4j_url: str = Field(default=\"bolt://localhost:7687\", env=\"NEO4J_URL\")\n    neo4j_username: str = Field(default=\"neo4j\", env=\"NEO4J_USERNAME\")\n    neo4j_password: str = Field(default=\"changeme\", env=\"NEO4J_PASSWORD\")\n    \n    # Memory Database (SQLite)\n    memory_db_path: str = Field(default=\"./data/memory.db\", env=\"MEMORY_DB_PATH\")\n    code_health_db_path: str = Field(default=\"./data/code_health.db\", env=\"CODE_HEALTH_DB_PATH\")\n    \n    # Indexing Settings\n    chunk_size: int = Field(default=400, env=\"CHUNK_SIZE\")\n    chunk_overlap: int = Field(default=70, env=\"CHUNK_OVERLAP\")\n    generate_summaries: bool = Field(default=True, env=\"GENERATE_SUMMARIES\")\n    \n    # Retrieval Settings\n    semantic_search_top_k: int = Field(default=50, env=\"SEMANTIC_SEARCH_TOP_K\")\n    enable_graph_expansion: bool = Field(default=True, env=\"ENABLE_GRAPH_EXPANSION\")\n    enable_error_path_retrieval: bool = Field(default=True, env=\"ENABLE_ERROR_PATH_RETRIEVAL\")\n    use_code_health_ranking: bool = Field(default=True, env=\"USE_CODE_HEALTH_RANKING\")\n    \n    # Advanced Features\n    enable_offline_mode: bool = Field(default=False, env=\"ENABLE_OFFLINE_MODE\")\n    enable_llm_cache: bool = Field(default=True, env=\"ENABLE_LLM_CACHE\")\n    llm_cache_ttl: int = Field(default=86400, env=\"LLM_CACHE_TTL\")  # 24 hours\n    enable_recovery: bool = Field(default=True, env=\"ENABLE_RECOVERY\")\n    \n    # Debugging\n    enable_tier_escalation: bool = Field(default=True, env=\"ENABLE_TIER_ESCALATION\")\n    enable_retry_tree: bool = Field(default=True, env=\"ENABLE_RETRY_TREE\")\n    max_retry_attempts: int = Field(default=3, env=\"MAX_RETRY_ATTEMPTS\")\n    \n    # API Settings\n    api_host: str = Field(default=\"0.0.0.0\", env=\"API_HOST\")\n    api_port: int = Field(default=8000, env=\"API_PORT\")\n    enable_cors: bool = Field(default=True, env=\"ENABLE_CORS\")\n    cors_origins: str = Field(default=\"http://localhost:3000,http://localhost:3001\", env=\"CORS_ORIGINS\")\n    \n    # Logging\n    log_level: str = Field(default=\"INFO\", env=\"LOG_LEVEL\")\n    log_file: str = Field(default=\"./logs/vibe-agent.log\", env=\"LOG_FILE\")\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n    \n    def get_cors_origins_list(self) -> list[str]:\n        \"\"\"Parse CORS origins from comma-separated string.\"\"\"\n        return [origin.strip() for origin in self.cors_origins.split(\",\")]\n    \n    def ensure_directories(self):\n        \"\"\"Ensure all required directories exist.\"\"\"\n        directories = [\n            Path(self.vector_db_path).parent,\n            Path(self.memory_db_path).parent,\n            Path(self.code_health_db_path).parent,\n            Path(self.log_file).parent,\n            \"data/sandbox\",\n            \"data/index\",\n        ]\n        \n        for directory in directories:\n            Path(directory).mkdir(parents=True, exist_ok=True)\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"\n    Get cached settings instance.\n    \n    Returns:\n        Settings instance\n    \"\"\"\n    settings = Settings()\n    settings.ensure_directories()\n    return settings\n",
    "src\\app\\main.py": "\"\"\"\nFastAPI Main Application\n\nEntry point for the Vibe Agent backend API.\n\"\"\"\n\nfrom fastapi import FastAPI, WebSocket\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nimport logging\nfrom pathlib import Path\n\nfrom .config import get_settings\n\n\n# Setup logging\ndef setup_logging():\n    \"\"\"Configure application logging.\"\"\"\n    settings = get_settings()\n    \n    # Ensure log directory exists\n    log_path = Path(settings.log_file)\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    logging.basicConfig(\n        level=getattr(logging, settings.log_level.upper()),\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(settings.log_file),\n            logging.StreamHandler()\n        ]\n    )\n\n\nsetup_logging()\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Lifespan context manager for startup and shutdown events.\n    \"\"\"\n    # Startup\n    logger.info(\"Starting Vibe Agent Backend...\")\n    settings = get_settings()\n    settings.ensure_directories()\n    logger.info(f\"Configuration loaded from: {settings.Config.env_file}\")\n    logger.info(f\"Target repository: {settings.target_repo_path or 'Not set'}\")\n    \n    yield\n    \n    # Shutdown\n    logger.info(\"Shutting down Vibe Agent Backend...\")\n\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Vibe Coding AI Agent\",\n    description=\"Autonomous Full-Stack Repository Intelligence System\",\n    version=\"1.1.0\",\n    lifespan=lifespan\n)\n\n# CORS middleware\nsettings = get_settings()\nif settings.enable_cors:\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=settings.get_cors_origins_list(),\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\n        \"status\": \"ok\",\n        \"service\": \"vibe-agent-backend\",\n        \"version\": \"1.1.0\"\n    }\n\n\n# Root endpoint\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint with API information.\"\"\"\n    return {\n        \"name\": \"Vibe Coding AI Agent API\",\n        \"version\": \"1.1.0\",\n        \"description\": \"Autonomous Repository Intelligence System\",\n        \"docs\": \"/docs\",\n        \"health\": \"/health\"\n    }\n\n\n# Error handlers\n@app.exception_handler(Exception)\nasync def global_exception_handler(request, exc):\n    \"\"\"Global exception handler.\"\"\"\n    logger.error(f\"Unhandled exception: {exc}\", exc_info=True)\n    return JSONResponse(\n        status_code=500,\n        content={\n            \"error\": \"Internal server error\",\n            \"message\": str(exc)\n        }\n    )\n\n\n# Import and register v1.0 routers\nfrom .routes import indexing, debug, chat, search, graph\n\napp.include_router(indexing.router, prefix=\"/index_file\", tags=[\"indexing\"])\napp.include_router(debug.router, prefix=\"/debug\", tags=[\"debug\"])\napp.include_router(chat.router, prefix=\"/chat\", tags=[\"chat\"])\napp.include_router(search.router, prefix=\"/search\", tags=[\"search\"])\napp.include_router(graph.router, prefix=\"/graph\", tags=[\"graph\"])\n\n# Import and register v1.1 routes\nfrom .routes.v1_1_routes import router as v1_1_router\napp.include_router(v1_1_router, prefix=\"/api/v1.1\", tags=[\"v1.1\"])\n\n# WebSocket routes\nfrom .websocket_handler import (\n    websocket_indexing_progress,\n    websocket_chat_stream,\n    websocket_agent_execution,\n)\n\n@app.websocket(\"/ws/indexing/{session_id}\")\nasync def ws_indexing(websocket: WebSocket, session_id: str):\n    \"\"\"WebSocket endpoint for indexing progress.\"\"\"\n    await websocket_indexing_progress(websocket, session_id)\n\n@app.websocket(\"/ws/chat/{session_id}\")\nasync def ws_chat(websocket: WebSocket, session_id: str):\n    \"\"\"WebSocket endpoint for chat streaming.\"\"\"\n    await websocket_chat_stream(websocket, session_id)\n\n@app.websocket(\"/ws/agent/{task_id}\")\nasync def ws_agent(websocket: WebSocket, task_id: str):\n    \"\"\"WebSocket endpoint for agent execution.\"\"\"\n    await websocket_agent_execution(websocket, task_id)\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    \n    uvicorn.run(\n        \"main:app\",\n        host=settings.api_host,\n        port=settings.api_port,\n        reload=True\n    )\n",
    "src\\app\\websocket_handler.py": "\"\"\"\nWebSocket Handler for Real-time Updates\n\nProvides WebSocket endpoints for:\n- Real-time indexing progress\n- Live search results\n- Agent execution streaming\n- Conversation updates\n\"\"\"\n\nfrom fastapi import WebSocket, WebSocketDisconnect\nfrom typing import Dict, Set\nimport json\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionManager:\n    \"\"\"Manage WebSocket connections.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize connection manager.\"\"\"\n        self.active_connections: Dict[str, Set[WebSocket]] = {}\n    \n    async def connect(self, websocket: WebSocket, channel: str):\n        \"\"\"\n        Accept WebSocket connection and add to channel.\n        \n        Args:\n            websocket: WebSocket connection\n            channel: Channel name (e.g., 'indexing', 'chat', 'search')\n        \"\"\"\n        await websocket.accept()\n        \n        if channel not in self.active_connections:\n            self.active_connections[channel] = set()\n        \n        self.active_connections[channel].add(websocket)\n        logger.info(f\"Client connected to channel: {channel}\")\n    \n    def disconnect(self, websocket: WebSocket, channel: str):\n        \"\"\"\n        Remove WebSocket connection from channel.\n        \n        Args:\n            websocket: WebSocket connection\n            channel: Channel name\n        \"\"\"\n        if channel in self.active_connections:\n            self.active_connections[channel].discard(websocket)\n            logger.info(f\"Client disconnected from channel: {channel}\")\n    \n    async def send_personal_message(self, message: dict, websocket: WebSocket):\n        \"\"\"Send message to specific client.\"\"\"\n        await websocket.send_json(message)\n    \n    async def broadcast(self, message: dict, channel: str):\n        \"\"\"\n        Broadcast message to all clients in channel.\n        \n        Args:\n            message: Message data\n            channel: Channel name\n        \"\"\"\n        if channel not in self.active_connections:\n            return\n        \n        disconnected = set()\n        \n        for connection in self.active_connections[channel]:\n            try:\n                await connection.send_json(message)\n            except Exception as e:\n                logger.error(f\"Error broadcasting to client: {e}\")\n                disconnected.add(connection)\n        \n        # Remove disconnected clients\n        for conn in disconnected:\n            self.active_connections[channel].discard(conn)\n\n\n# Global connection manager\nmanager = ConnectionManager()\n\n\nasync def websocket_indexing_progress(websocket: WebSocket, session_id: str):\n    \"\"\"\n    WebSocket endpoint for indexing progress.\n    \n    Streams real-time updates about indexing progress.\n    \"\"\"\n    await manager.connect(websocket, f\"indexing:{session_id}\")\n    \n    try:\n        while True:\n            # This would be called by the indexing pipeline\n            # For now, simulate progress updates\n            data = await websocket.receive_text()\n            \n            # Echo back (in production, this would be server-driven)\n            await manager.send_personal_message({\n                \"type\": \"indexing_progress\",\n                \"session_id\": session_id,\n                \"data\": json.loads(data)\n            }, websocket)\n    \n    except WebSocketDisconnect:\n        manager.disconnect(websocket, f\"indexing:{session_id}\")\n        logger.info(f\"Indexing WebSocket closed for session: {session_id}\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket, f\"indexing:{session_id}\")\n\n\nasync def websocket_chat_stream(websocket: WebSocket, session_id: str):\n    \"\"\"\n    WebSocket endpoint for chat streaming.\n    \n    Streams LLM responses token-by-token.\n    \"\"\"\n    await manager.connect(websocket, f\"chat:{session_id}\")\n    \n    try:\n        while True:\n            # Receive message from client\n            data = await websocket.receive_json()\n            \n            # Process message and stream response\n            if data.get(\"type\") == \"chat_message\":\n                # This would call LLM client with streaming\n                # For now, simulate streaming response\n                message = data.get(\"message\", \"\")\n                \n                # Simulate token streaming\n                response_tokens = f\"Response to: {message}\".split()\n                \n                for token in response_tokens:\n                    await manager.send_personal_message({\n                        \"type\": \"chat_token\",\n                        \"session_id\": session_id,\n                        \"token\": token + \" \",\n                    }, websocket)\n                    \n                    await asyncio.sleep(0.05)  # Simulate delay\n                \n                # Send completion\n                await manager.send_personal_message({\n                    \"type\": \"chat_complete\",\n                    \"session_id\": session_id,\n                }, websocket)\n    \n    except WebSocketDisconnect:\n        manager.disconnect(websocket, f\"chat:{session_id}\")\n        logger.info(f\"Chat WebSocket closed for session: {session_id}\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket, f\"chat:{session_id}\")\n\n\nasync def websocket_agent_execution(websocket: WebSocket, task_id: str):\n    \"\"\"\n    WebSocket endpoint for agent execution updates.\n    \n    Streams agent execution progress and intermediate results.\n    \"\"\"\n    await manager.connect(websocket, f\"agent:{task_id}\")\n    \n    try:\n        while True:\n            data = await websocket.receive_json()\n            \n            if data.get(\"type\") == \"start_task\":\n                # This would trigger agent orchestration\n                # Stream updates as agents work\n                \n                # Simulate agent execution steps\n                steps = [\n                    \"Parsing query...\",\n                    \"Retrieving relevant code...\",\n                    \"Analyzing context...\",\n                    \"Generating response...\",\n                    \"Complete!\"\n                ]\n                \n                for idx, step in enumerate(steps):\n                    await manager.send_personal_message({\n                        \"type\": \"agent_progress\",\n                        \"task_id\": task_id,\n                        \"step\": idx + 1,\n                        \"total_steps\": len(steps),\n                        \"message\": step,\n                        \"progress\": (idx + 1) / len(steps),\n                    }, websocket)\n                    \n                    await asyncio.sleep(0.5)\n    \n    except WebSocketDisconnect:\n        manager.disconnect(websocket, f\"agent:{task_id}\")\n        logger.info(f\"Agent WebSocket closed for task: {task_id}\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket, f\"agent:{task_id}\")\n\n\n# Helper function to broadcast indexing updates\nasync def broadcast_indexing_update(session_id: str, update_data: dict):\n    \"\"\"\n    Broadcast indexing update to all listeners.\n    \n    Called by indexing pipeline to push updates.\n    \"\"\"\n    await manager.broadcast({\n        \"type\": \"indexing_update\",\n        \"session_id\": session_id,\n        \"data\": update_data,\n    }, f\"indexing:{session_id}\")\n\n\n# Helper function to broadcast chat tokens\nasync def broadcast_chat_token(session_id: str, token: str):\n    \"\"\"\n    Broadcast chat token to session.\n    \n    Called by LLM client during streaming.\n    \"\"\"\n    await manager.broadcast({\n        \"type\": \"chat_token\",\n        \"session_id\": session_id,\n        \"token\": token,\n    }, f\"chat:{session_id}\")\n",
    "src\\app\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\app\\routes\\chat.py": "\"\"\"\nChat API Routes\n\nEndpoints for chat interactions with streaming support.\n\"\"\"\n\nfrom fastapi import APIRouter, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport json\nimport asyncio\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/chat\", tags=[\"chat\"])\n\n\nclass Message(BaseModel):\n    \"\"\"Chat message model.\"\"\"\n    role: str  # 'user' or 'assistant'\n    content: str\n\n\nclass ChatRequest(BaseModel):\n    \"\"\"Chat request model.\"\"\"\n    messages: List[Message]\n    stream: bool = False\n    temperature: float = 0.7\n    session_id: Optional[str] = None\n\n\nclass ChatResponse(BaseModel):\n    \"\"\"Chat response model.\"\"\"\n    message: str\n    session_id: str\n    usage: Dict[str, int]\n\n\n@router.post(\"/completion\")\nasync def chat_completion(request: ChatRequest):\n    \"\"\"\n    Generate chat completion.\n    \n    Args:\n        request: Chat request\n    \n    Returns:\n        Chat response or streaming response\n    \"\"\"\n    if request.stream:\n        return StreamingResponse(\n            stream_chat_response(request),\n            media_type=\"text/event-stream\"\n        )\n    \n    # Non-streaming response\n    try:\n        # This would use actual LLM client\n        response_text = \"This is a placeholder response.\"\n        \n        return {\n            'message': response_text,\n            'session_id': request.session_id or 'default',\n            'usage': {\n                'total_tokens': 100,\n                'prompt_tokens': 50,\n                'completion_tokens': 50\n            }\n        }\n    \n    except Exception as e:\n        logger.error(f\"Chat completion failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\nasync def stream_chat_response(request: ChatRequest):\n    \"\"\"\n    Stream chat response using Server-Sent Events.\n    \n    Yields:\n        SSE formatted chunks\n    \"\"\"\n    try:\n        # This would use actual LLM client streaming\n        message_chunks = [\n            \"This \",\n            \"is \",\n            \"a \",\n            \"streaming \",\n            \"response.\"\n        ]\n        \n        for chunk in message_chunks:\n            # SSE format\n            data = json.dumps({'content': chunk})\n            yield f\"data: {data}\\n\\n\"\n            \n            await asyncio.sleep(0.1)  # Simulate delay\n        \n        # End of stream\n        yield \"data: [DONE]\\n\\n\"\n    \n    except Exception as e:\n        logger.error(f\"Streaming failed: {e}\")\n        error_data = json.dumps({'error': str(e)})\n        yield f\"data: {error_data}\\n\\n\"\n\n\n@router.get(\"/history/{session_id}\")\nasync def get_chat_history(session_id: str, limit: int = 50):\n    \"\"\"\n    Get chat history for a session.\n    \n    Args:\n        session_id: Session ID\n        limit: Maximum messages to return\n    \n    Returns:\n        Chat history\n    \"\"\"\n    # This would query error memory database\n    return {\n        'session_id': session_id,\n        'messages': [],\n        'total': 0\n    }\n\n\n@router.delete(\"/history/{session_id}\")\nasync def clear_chat_history(session_id: str):\n    \"\"\"Clear chat history for a session.\"\"\"\n    # This would delete from database\n    return {\n        'session_id': session_id,\n        'cleared': True\n    }\n",
    "src\\app\\routes\\debug.py": "\"\"\"\nDebug API Routes\n\nEndpoints for error debugging and bug localization.\n\"\"\"\n\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Dict, Any, Optional, List\nimport logging\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/debug\", tags=[\"debug\"])\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error data model.\"\"\"\n    error_type: str\n    error_message: str\n    file_path: Optional[str] = None\n    line_number: Optional[int] = None\n    stack_trace: Optional[str] = None\n    context_code: Optional[str] = None\n\n\nclass DebugRequest(BaseModel):\n    \"\"\"Debug request model.\"\"\"\n    error: ErrorData\n    auto_fix: bool = False\n\n\nclass BugLocation(BaseModel):\n    \"\"\"Bug location model.\"\"\"\n    file: str\n    line: int\n    confidence: float\n    reason: str\n\n\n@router.post(\"/error\")\nasync def debug_error(request: DebugRequest):\n    \"\"\"\n    Debug an error and locate bugs.\n    \n    Args:\n        request: Debug request\n    \n    Returns:\n        Debug results with bug locations and fix suggestions\n    \"\"\"\n    try:\n        session_id = str(uuid.uuid4())\n        \n        logger.info(f\"Debug session {session_id}: {request.error.error_type}\")\n        \n        # This would use actual agent orchestration\n        # 1. Save error snapshot\n        # 2. Retrieve similar errors\n        # 3. Locate bugs\n        # 4. Analyze root cause\n        # 5. Generate patch (if auto_fix)\n        \n        result = {\n            'session_id': session_id,\n            'error_type': request.error.error_type,\n            'bug_locations': [],\n            'root_cause': None,\n            'patch': None if not request.auto_fix else None\n        }\n        \n        return result\n    \n    except Exception as e:\n        logger.error(f\"Debug failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/similar/{error_hash}\")\nasync def get_similar_errors(error_hash: str, limit: int = 10):\n    \"\"\"\n    Get similar errors that were previously resolved.\n    \n    Args:\n        error_hash: Error hash\n        limit: Maximum results\n    \n    Returns:\n        Similar errors\n    \"\"\"\n    # This would query error memory database\n    return {\n        'error_hash': error_hash,\n        'similar_errors': [],\n        'total': 0\n    }\n\n\n@router.get(\"/hotspots\")\nasync def get_bug_hotspots(limit: int = 20):\n    \"\"\"\n    Get bug hotspots in the codebase.\n    \n    Args:\n        limit: Maximum hotspots to return\n    \n    Returns:\n        Bug hotspots\n    \"\"\"\n    # This would query code health database\n    return {\n        'hotspots': [],\n        'total': 0\n    }\n",
    "src\\app\\routes\\graph.py": "\"\"\"\nGraph API Routes\n\nEndpoints for code graph exploration.\n\"\"\"\n\nfrom fastapi import APIRouter\nfrom typing import Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/graph\", tags=[\"graph\"])\n\n\n@router.get(\"/overview\")\nasync def get_graph_overview():\n    \"\"\"\n    Get overview of the code graph.\n    \n    Returns:\n        Graph statistics and summary\n    \"\"\"\n    # This would query Neo4j\n    return {\n        'nodes': {\n            'functions': 0,\n            'classes': 0,\n            'files': 0\n        },\n        'relationships': {\n            'calls': 0,\n            'contains': 0,\n            'imports': 0\n        },\n        'total_nodes': 0,\n        'total_relationships': 0\n    }\n\n\n@router.get(\"/function/{function_name}/callers\")\nasync def get_function_callers(function_name: str, file_path: Optional[str] = None):\n    \"\"\"\n    Get functions that call a specific function.\n    \n    Args:\n        function_name: Function name\n        file_path: Optional file path filter\n    \n    Returns:\n        Caller functions\n    \"\"\"\n    # This would query Neo4j ASG\n    return {\n        'function': function_name,\n        'file': file_path,\n        'callers': []\n    }\n\n\n@router.get(\"/function/{function_name}/callees\")\nasync def get_function_callees(function_name: str, file_path: Optional[str] = None):\n    \"\"\"\n    Get functions called by a specific function.\n    \n    Args:\n        function_name: Function name\n        file_path: Optional file path filter\n    \n    Returns:\n        Called functions\n    \"\"\"\n    # This would query Neo4j ASG\n    return {\n        'function': function_name,\n        'file': file_path,\n        'callees': []\n    }\n\n\n@router.get(\"/file/{file_path:path}/dependencies\")\nasync def get_file_dependencies(file_path: str):\n    \"\"\"\n    Get dependencies for a file.\n    \n    Args:\n        file_path: File path\n    \n    Returns:\n        File dependencies\n    \"\"\"\n    # This would query dependency graph\n    return {\n        'file': file_path,\n        'dependencies': [],\n        'dependents': []\n    }\n",
    "src\\app\\routes\\indexing.py": "\"\"\"\nIndexing API Routes\n\nEndpoints for code indexing operations.\n\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom typing import Dict, Any, Optional\nimport logging\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/index_file\", tags=[\"indexing\"])\n\n\nclass IndexRequest(BaseModel):\n    \"\"\"Request model for indexing.\"\"\"\n    repo_path: str\n    force_reindex: bool = False\n\n\nclass IndexStatus(BaseModel):\n    \"\"\"Status model for indexing.\"\"\"\n    session_id: str\n    status: str\n    progress: float\n    message: str\n\n\n# Global state for tracking indexing sessions\nindexing_sessions: Dict[str, Dict[str, Any]] = {}\n\n\n@router.post(\"/start\")\nasync def start_indexing(request: IndexRequest, background_tasks: BackgroundTasks):\n    \"\"\"\n    Start indexing a repository.\n    \n    Args:\n        request: Indexing request\n        background_tasks: FastAPI background tasks\n    \n    Returns:\n        Session information\n    \"\"\"\n    session_id = str(uuid.uuid4())\n    \n    # Initialize session\n    indexing_sessions[session_id] = {\n        'status': 'started',\n        'progress': 0.0,\n        'repo_path': request.repo_path\n    }\n    \n    # Start indexing in background\n    background_tasks.add_task(run_indexing, session_id, request.repo_path, request.force_reindex)\n    \n    logger.info(f\"Started indexing session {session_id} for {request.repo_path}\")\n    \n    return {\n        'session_id': session_id,\n        'status': 'started',\n        'message': 'Indexing started'\n    }\n\n\n@router.get(\"/status/{session_id}\")\nasync def get_indexing_status(session_id: str):\n    \"\"\"Get indexing status for a session.\"\"\"\n    if session_id not in indexing_sessions:\n        raise HTTPException(status_code=404, detail=\"Session not found\")\n    \n    session = indexing_sessions[session_id]\n    \n    return {\n        'session_id': session_id,\n        'status': session.get('status', 'unknown'),\n        'progress': session.get('progress', 0.0),\n        'message': session.get('message', '')\n    }\n\n\n@router.get(\"/stats\")\nasync def get_index_stats():\n    \"\"\"Get indexing statistics.\"\"\"\n    # This would query actual databases\n    return {\n        'total_files': 0,\n        'total_functions': 0,\n        'total_chunks': 0,\n        'vector_count': 0,\n        'graph_nodes': 0\n    }\n\n\nasync def run_indexing(session_id: str, repo_path: str, force_reindex: bool):\n    \"\"\"\n    Run indexing pipeline.\n    \n    This is a background task that orchestrates the entire indexing process.\n    \"\"\"\n    try:\n        # Update status\n        indexing_sessions[session_id]['status'] = 'running'\n        indexing_sessions[session_id]['message'] = 'Walking directory tree...'\n        indexing_sessions[session_id]['progress'] = 0.1\n        \n        # Step 1: File walking (would call actual walker)\n        # walker = FileWalker(repo_path)\n        # files = walker.walk()\n        \n        indexing_sessions[session_id]['message'] = 'Parsing AST...'\n        indexing_sessions[session_id]['progress'] = 0.3\n        \n        # Step 2: AST parsing\n        # ...\n        \n        indexing_sessions[session_id]['message'] = 'Generating chunks...'\n        indexing_sessions[session_id]['progress'] = 0.5\n        \n        # Step 3: Chunking\n        # ...\n        \n        indexing_sessions[session_id]['message'] = 'Creating embeddings...'\n        indexing_sessions[session_id]['progress'] = 0.7\n        \n        # Step 4: Embeddings\n        # ...\n        \n        indexing_sessions[session_id]['message'] = 'Building graph...'\n        indexing_sessions[session_id]['progress'] = 0.9\n        \n        # Step 5: Graph construction\n        # ...\n        \n        # Complete\n        indexing_sessions[session_id]['status'] = 'completed'\n        indexing_sessions[session_id]['progress'] = 1.0\n        indexing_sessions[session_id]['message'] = 'Indexing complete'\n        \n        logger.info(f\"Completed indexing session {session_id}\")\n        \n    except Exception as e:\n        logger.error(f\"Indexing failed for session {session_id}: {e}\")\n        indexing_sessions[session_id]['status'] = 'failed'\n        indexing_sessions[session_id]['message'] = str(e)\n",
    "src\\app\\routes\\search.py": "\"\"\"\nSearch API Routes\n\nEndpoints for semantic code search.\n\"\"\"\n\nfrom fastapi import APIRouter, Query\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/search\", tags=[\"search\"])\n\n\nclass SearchRequest(BaseModel):\n    \"\"\"Search request model.\"\"\"\n    query: str\n    language: Optional[str] = None\n    top_k: int = 20\n    min_score: float = 0.0\n\n\nclass SearchResult(BaseModel):\n    \"\"\"Search result model.\"\"\"\n    chunk_id: str\n    file_path: str\n    start_line: int\n    end_line: int\n    content: str\n    score: float\n    language: str\n\n\n@router.post(\"/semantic\")\nasync def semantic_search(request: SearchRequest):\n    \"\"\"\n    Perform semantic code search.\n    \n    Args:\n        request: Search request\n    \n    Returns:\n        Search results\n    \"\"\"\n    try:\n        # This would use actual semantic search\n        results = []\n        \n        logger.info(f\"Semantic search: '{request.query}' (top_k={request.top_k})\")\n        \n        return {\n            'query': request.query,\n            'results': results,\n            'total': len(results)\n        }\n    \n    except Exception as e:\n        logger.error(f\"Search failed: {e}\")\n        return {\n            'query': request.query,\n            'results': [],\n            'error': str(e)\n        }\n\n\n@router.get(\"/function/{function_name}\")\nasync def search_function(function_name: str, file_path: Optional[str] = None):\n    \"\"\"\n    Search for a specific function.\n    \n    Args:\n        function_name: Function name to search for\n        file_path: Optional file path filter\n    \n    Returns:\n        Function information\n    \"\"\"\n    # This would query Neo4j graph\n    return {\n        'function_name': function_name,\n        'file_path': file_path,\n        'found': False\n    }\n\n\n@router.get(\"/file/{file_path:path}\")\nasync def search_file(file_path: str):\n    \"\"\"\n    Get all chunks for a file.\n    \n    Args:\n        file_path: File path\n    \n    Returns:\n        File chunks\n    \"\"\"\n    # This would query vector store metadata\n    return {\n        'file_path': file_path,\n        'chunks': [],\n        'total': 0\n    }\n",
    "src\\app\\routes\\v1_1_routes.py": "\"\"\"\nAdditional API Routes for v1.1\n\nMissing endpoints:\n- Patch application\n- Symbol lineage\n- CFG queries\n\"\"\"\n\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(tags=[\"v1.1\"])\n\n\n# ============================================================================\n# PATCH ROUTES\n# ============================================================================\n\nclass ApplyPatchRequest(BaseModel):\n    \"\"\"Request to apply a patch.\"\"\"\n    file_path: str\n    patch_content: str\n    should_validate: bool = True\n    dry_run: bool = False\n\n\n@router.post(\"/patch/apply\")\nasync def apply_patch(request: ApplyPatchRequest):\n    \"\"\"\n    Apply a code patch.\n    \n    Args:\n        request: Patch application request\n    \n    Returns:\n        Application result\n    \"\"\"\n    try:\n        # This would use PatchValidator and apply logic\n        result = {\n            'success': True,\n            'file': request.file_path,\n            'dry_run': request.dry_run,\n            'validated': request.should_validate,\n            'validation_results': {},\n        }\n        \n        if request.should_validate:\n            # Run validation\n            result['validation_results'] = {\n                'syntax': {'passed': True},\n                'types': {'passed': True},\n                'lint': {'passed': True},\n            }\n        \n        if not request.dry_run:\n            # Actually apply patch\n            logger.info(f\"Applied patch to {request.file_path}\")\n        \n        return result\n    \n    except Exception as e:\n        logger.error(f\"Patch application failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/patch/validate\")\nasync def validate_patch(\n    file_path: str,\n    patch_content: str,\n    language: str\n):\n    \"\"\"Validate a patch without applying it.\"\"\"\n    # This would use PatchValidator\n    return {\n        'valid': True,\n        'checks': {\n            'syntax': {'passed': True},\n            'types': {'passed': True},\n            'lint': {'passed': True},\n        },\n        'errors': [],\n        'warnings': [],\n    }\n\n\n# ============================================================================\n# SYMBOL LINEAGE ROUTES\n# ============================================================================\n\n@router.get(\"/symbols/lineage/{symbol_name}\")\nasync def get_symbol_lineage(\n    symbol_name: str,\n    context_file: Optional[str] = None\n):\n    \"\"\"\n    Get lineage information for a symbol.\n    \n    Args:\n        symbol_name: Symbol to trace\n        context_file: Optional context file\n    \n    Returns:\n        Symbol lineage data\n    \"\"\"\n    # This would use SymbolLineageTracer\n    return {\n        'symbol': symbol_name,\n        'definitions': [],\n        'imports': [],\n        'usages': [],\n        'lineage_chain': [],\n        'impact_analysis': {\n            'total_usages': 0,\n            'files_affected': 0,\n            'impact_score': 0,\n        },\n    }\n\n\n@router.get(\"/symbols/rename-candidates\")\nasync def get_rename_candidates(\n    old_name: str,\n    new_name: str\n):\n    \"\"\"Get all locations that need to change for a rename.\"\"\"\n    # This would use SymbolLineageTracer\n    return {\n        'old_name': old_name,\n        'new_name': new_name,\n        'candidates': [],\n        'total_changes': 0,\n    }\n\n\n# ============================================================================\n# CFG ROUTES\n# ============================================================================\n\n@router.get(\"/cfg/function/{function_name}\")\nasync def get_function_cfg(\n    function_name: str,\n    file_path: Optional[str] = None\n):\n    \"\"\"\n    Get Control Flow Graph for a function.\n    \n    Args:\n        function_name: Function name\n        file_path: Optional file path\n    \n    Returns:\n        CFG data\n    \"\"\"\n    # This would use CFGBuilder\n    return {\n        'function': function_name,\n        'file': file_path,\n        'entry_block': 'block_0',\n        'exit_block': 'block_n',\n        'blocks': {},\n        'total_blocks': 0,\n        'has_loops': False,\n        'has_branches': False,\n        'cyclomatic_complexity': 1,\n    }\n\n\n# ============================================================================\n# ADVANCED SEARCH ROUTES\n# ============================================================================\n\n@router.post(\"/search/error-path\")\nasync def search_error_path(\n    error_function: str,\n    error_file: str,\n    max_depth: int = 3\n):\n    \"\"\"\n    Search for error propagation path.\n    \n    Traces execution path from error location upward through call graph.\n    \"\"\"\n    # This would use advanced error-path retrieval\n    return {\n        'error_function': error_function,\n        'error_file': error_file,\n        'path': [],\n        'depth': 0,\n        'confidence': 0.0,\n    }\n\n\n@router.get(\"/search/similar-code\")\nasync def search_similar_code(\n    file_path: str,\n    start_line: int,\n    end_line: int,\n    top_k: int = 10\n):\n    \"\"\"Find code similar to a specific snippet.\"\"\"\n    # This would use semantic search on specific code block\n    return {\n        'query_location': {\n            'file': file_path,\n            'start_line': start_line,\n            'end_line': end_line,\n        },\n        'similar_code': [],\n        'total': 0,\n    }\n\n\n# ============================================================================\n# MEMORY ROUTES\n# ============================================================================\n\n@router.get(\"/memory/history/{session_id}\")\nasync def get_memory_history(\n    session_id: str,\n    limit: int = 50,\n    include_resolutions: bool = True\n):\n    \"\"\"\n    Get complete memory history for a session.\n    \n    Includes conversation history, errors, and resolutions.\n    \"\"\"\n    return {\n        'session_id': session_id,\n        'conversation': [],\n        'errors': [],\n        'resolutions': [],\n        'total_turns': 0,\n    }\n\n\n@router.get(\"/memory/error-patterns\")\nasync def get_error_patterns(limit: int = 20):\n    \"\"\"Get common error patterns from memory.\"\"\"\n    return {\n        'patterns': [],\n        'total': 0,\n    }\n",
    "src\\app\\routes\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\embeddings\\embedding_pipeline.py": "\"\"\"\nEmbedding Pipeline\n\nOrchestrates embedding generation for all code chunks.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nfrom pathlib import Path\n\nfrom .embedding_service import EmbeddingService\nfrom .vector_store import VectorStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingPipeline:\n    \"\"\"\n    Pipeline for embedding code chunks and storing in vector database.\n    \"\"\"\n    \n    def __init__(\n        self,\n        embedding_service: EmbeddingService,\n        vector_store: VectorStore\n    ):\n        \"\"\"\n        Initialize embedding pipeline.\n        \n        Args:\n            embedding_service: Embedding service instance\n            vector_store: Vector store instance\n        \"\"\"\n        self.embedding_service = embedding_service\n        self.vector_store = vector_store\n    \n    def process_manifest(\n        self,\n        manifest: Dict[str, Any],\n        batch_size: int = 32\n    ) -> Dict[str, int]:\n        \"\"\"\n        Process manifest and embed all chunks.\n        \n        Args:\n            manifest: Indexing manifest\n            batch_size: Batch size for embedding generation\n        \n        Returns:\n            Statistics dictionary\n        \"\"\"\n        logger.info(\"Starting embedding pipeline...\")\n        \n        total_chunks = 0\n        total_files = 0\n        \n        files = manifest.get('files', [])\n        \n        for file_data in files:\n            chunks = file_data.get('chunks', [])\n            \n            if not chunks:\n                continue\n            \n            total_files += 1\n            \n            # Extract chunk texts\n            texts = [chunk['content'] for chunk in chunks]\n            \n            # Generate embeddings in batches\n            logger.info(f\"Processing {len(texts)} chunks from {file_data['relative_path']}\")\n            \n            embeddings = self.embedding_service.encode_batch(\n                texts,\n                batch_size=batch_size,\n                normalize=True\n            )\n            \n            # Prepare metadata\n            metadata_list = []\n            for chunk in chunks:\n                metadata = {\n                    'chunk_id': chunk['chunk_id'],\n                    'file_path': chunk['file_path'],\n                    'start_line': chunk['start_line'],\n                    'end_line': chunk['end_line'],\n                    'language': chunk['language'],\n                    'token_count': chunk['token_count']\n                }\n                metadata_list.append(metadata)\n            \n            # Add to vector store\n            self.vector_store.add(embeddings, metadata_list)\n            \n            total_chunks += len(chunks)\n        \n        logger.info(f\"Embedding pipeline complete\")\n        logger.info(f\"  Files processed: {total_files}\")\n        logger.info(f\"  Chunks embedded: {total_chunks}\")\n        \n        return {\n            'files_processed': total_files,\n            'chunks_embedded': total_chunks\n        }\n    \n    def embed_functions(\n        self,\n        manifest: Dict[str, Any],\n        batch_size: int = 32\n    ) -> int:\n        \"\"\"\n        Generate and store function-level embeddings.\n        \n        Args:\n            manifest: Indexing manifest\n            batch_size: Batch size\n        \n        Returns:\n            Number of functions embedded\n        \"\"\"\n        logger.info(\"Embedding functions...\")\n        \n        function_count = 0\n        \n        for file_data in manifest.get('files', []):\n            functions = file_data.get('functions', [])\n            \n            if not functions:\n                continue\n            \n            # For function embeddings, we'll use function signatures + docstrings\n            # In production, we'd extract actual function content\n            texts = [f\"Function: {func['name']}\" for func in functions]\n            \n            embeddings = self.embedding_service.encode_batch(texts, batch_size=batch_size)\n            \n            # Prepare metadata\n            metadata_list = []\n            for func in functions:\n                metadata = {\n                    'type': 'function',\n                    'name': func['name'],\n                    'file_path': file_data['path'],\n                    'start_line': func['start_line'],\n                    'end_line': func['end_line'],\n                    'language': func.get('language', 'unknown')\n                }\n                metadata_list.append(metadata)\n            \n            self.vector_store.add(embeddings, metadata_list)\n            function_count += len(functions)\n        \n        logger.info(f\"Embedded {function_count} functions\")\n        \n        return function_count\n    \n    def save(self):\n        \"\"\"Save vector store to disk.\"\"\"\n        self.vector_store.save()\n        logger.info(\"Vector store saved\")\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    import json\n    from ..app.config import get_settings\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python embedding_pipeline.py <manifest.json>\")\n        sys.exit(1)\n    \n    manifest_path = sys.argv[1]\n    \n    with open(manifest_path, 'r') as f:\n        manifest = json.load(f)\n    \n    settings = get_settings()\n    \n    # Initialize services\n    embedding_service = EmbeddingService(\n        model_path=settings.embedding_model_path,\n        use_gpu=True\n    )\n    \n    vector_store = VectorStore(\n        dimension=embedding_service.embedding_dim,\n        index_path=settings.vector_db_path\n    )\n    \n    # Run pipeline\n    pipeline = EmbeddingPipeline(embedding_service, vector_store)\n    \n    stats = pipeline.process_manifest(manifest, batch_size=32)\n    \n    # Also embed functions\n    func_count = pipeline.embed_functions(manifest, batch_size=32)\n    \n    # Save\n    pipeline.save()\n    \n    print(f\"\\n\u2705 Embedding pipeline complete\")\n    print(f\"   Files: {stats['files_processed']}\")\n    print(f\"   Chunks: {stats['chunks_embedded']}\")\n    print(f\"   Functions: {func_count}\")\n    print(f\"\\n\ud83d\udcbe Saved to: {settings.vector_db_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\embeddings\\embedding_service.py": "\"\"\"\nEmbedding Service\n\nGenerates embeddings using BGE-M3 model (local).\n\"\"\"\n\nfrom typing import List, Union, Optional\nimport numpy as np\nimport logging\nfrom pathlib import Path\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    import torch\n    HAS_EMBEDDINGS = True\nexcept ImportError:\n    HAS_EMBEDDINGS = False\n    print(\"Warning: sentence-transformers not installed\")\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingService:\n    \"\"\"\n    Local embedding generation using BGE-M3.\n    \"\"\"\n    \n    def __init__(self, model_path: str, use_gpu: bool = True):\n        \"\"\"\n        Initialize embedding service.\n        \n        Args:\n            model_path: Path to local BGE-M3 model\n            use_gpu: Whether to use GPU if available\n        \"\"\"\n        if not HAS_EMBEDDINGS:\n            raise RuntimeError(\"sentence-transformers not installed\")\n        \n        self.model_path = Path(model_path)\n        \n        if not self.model_path.exists():\n            raise FileNotFoundError(f\"Model not found at {model_path}. Run: python models/download_bge_m3.py\")\n        \n        # Determine device\n        if use_gpu and torch.cuda.is_available():\n            self.device = 'cuda'\n            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n        else:\n            self.device = 'cpu'\n            logger.info(\"Using CPU for embeddings\")\n        \n        # Load model\n        logger.info(f\"Loading BGE-M3 model from {model_path}...\")\n        self.model = SentenceTransformer(str(self.model_path))\n        self.model.to(self.device)\n        \n        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n        logger.info(f\"Model loaded. Embedding dimension: {self.embedding_dim}\")\n        \n        # Cache for repeated texts\n        self._cache: dict[str, np.ndarray] = {}\n    \n    def encode(\n        self,\n        texts: Union[str, List[str]],\n        batch_size: int = 32,\n        normalize: bool = True,\n        use_cache: bool = True\n    ) -> np.ndarray:\n        \"\"\"\n        Generate embeddings for text(s).\n        \n        Args:\n            texts: Single text or list of texts\n            batch_size: Batch size for processing\n            normalize: Whether to L2 normalize embeddings\n            use_cache: Whether to use cache for repeated texts\n        \n        Returns:\n            Numpy array of embeddings (single or batch)\n        \"\"\"\n        # Handle single text\n        single_text = isinstance(texts, str)\n        if single_text:\n            texts = [texts]\n        \n        # Check cache\n        if use_cache:\n            embeddings = []\n            uncached_texts = []\n            uncached_indices = []\n            \n            for i, text in enumerate(texts):\n                if text in self._cache:\n                    embeddings.append(self._cache[text])\n                else:\n                    uncached_texts.append(text)\n                    uncached_indices.append(i)\n            \n            # Generate embeddings for uncached texts\n            if uncached_texts:\n                new_embeddings = self.model.encode(\n                    uncached_texts,\n                    batch_size=batch_size,\n                    normalize_embeddings=normalize,\n                    show_progress_bar=False\n                )\n                \n                # Cache new embeddings\n                for text, emb in zip(uncached_texts, new_embeddings):\n                    self._cache[text] = emb\n                \n                # Merge cached and new embeddings in correct order\n                result = [None] * len(texts)\n                \n                # Fill cached\n                cached_idx = 0\n                for i in range(len(texts)):\n                    if i not in uncached_indices:\n                        result[i] = embeddings[cached_idx]\n                        cached_idx += 1\n                \n                # Fill uncached\n                for i, emb in zip(uncached_indices, new_embeddings):\n                    result[i] = emb\n                \n                embeddings = np.array(result)\n            else:\n                embeddings = np.array(embeddings)\n        else:\n            # Direct encoding without cache\n            embeddings = self.model.encode(\n                texts,\n                batch_size=batch_size,\n                normalize_embeddings=normalize,\n                show_progress_bar=False\n            )\n        \n        # Return single embedding if single text\n        if single_text:\n            return embeddings[0]\n        \n        return embeddings\n    \n    def encode_batch(\n        self,\n        texts: List[str],\n        batch_size: int = 32,\n        normalize: bool = True\n    ) -> np.ndarray:\n        \"\"\"Convenience method for batch encoding.\"\"\"\n        return self.encode(texts, batch_size=batch_size, normalize=normalize, use_cache=False)\n    \n    def similarity(self, text1: str, text2: str) -> float:\n        \"\"\"\n        Calculate cosine similarity between two texts.\n        \n        Args:\n            text1: First text\n            text2: Second text\n        \n        Returns:\n            Cosine similarity score\n        \"\"\"\n        emb1 = self.encode(text1, normalize=True)\n        emb2 = self.encode(text2, normalize=True)\n        \n        return float(np.dot(emb1, emb2))\n    \n    def clear_cache(self):\n        \"\"\"Clear embedding cache.\"\"\"\n        self._cache.clear()\n        logger.info(\"Embedding cache cleared\")\n    \n    def get_cache_size(self) -> int:\n        \"\"\"Get number of cached embeddings.\"\"\"\n        return len(self._cache)\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    from ..app.config import get_settings\n    \n    settings = get_settings()\n    \n    # Initialize service\n    service = EmbeddingService(\n        model_path=settings.embedding_model_path,\n        use_gpu=True\n    )\n    \n    # Test encoding\n    test_texts = [\n        \"This is a test function.\",\n        \"def process_data(input: str) -> dict:\",\n        \"Calculate the sum of two numbers.\"\n    ]\n    \n    print(f\"\\n\ud83d\udd22 Generating embeddings for {len(test_texts)} texts...\")\n    \n    embeddings = service.encode(test_texts)\n    \n    print(f\"\u2705 Generated {len(embeddings)} embeddings\")\n    print(f\"   Embedding shape: {embeddings[0].shape}\")\n    print(f\"   Dimension: {service.embedding_dim}\")\n    \n    # Test similarity\n    sim = service.similarity(test_texts[0], test_texts[1])\n    print(f\"\\n\ud83d\udcca Similarity between first two texts: {sim:.4f}\")\n    \n    # Cache info\n    print(f\"\\n\ud83d\udcbe Cache size: {service.get_cache_size()}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\embeddings\\vector_store.py": "\"\"\"\nFAISS Vector Store\n\nManages vector storage and similarity search using FAISS.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Tuple\nimport numpy as np\nimport logging\nfrom pathlib import Path\nimport json\nimport pickle\n\ntry:\n    import faiss\n    HAS_FAISS = True\nexcept ImportError:\n    HAS_FAISS = False\n    print(\"Warning: faiss not installed\")\n\nlogger = logging.getLogger(__name__)\n\n\nclass VectorStore:\n    \"\"\"\n    FAISS-based vector store for code chunks.\n    \"\"\"\n    \n    def __init__(self, dimension: int, index_path: Optional[str] = None):\n        \"\"\"\n        Initialize vector store.\n        \n        Args:\n            dimension: Embedding dimension\n            index_path: Path to save/load index\n        \"\"\"\n        if not HAS_FAISS:\n            raise RuntimeError(\"faiss not installed\")\n        \n        self.dimension = dimension\n        self.index_path = Path(index_path) if index_path else None\n        \n        # Create FAISS index (using IndexFlatIP for inner product/cosine similarity)\n        self.index = faiss.IndexFlatIP(dimension)\n        \n        # Metadata storage (chunk_id -> metadata)\n        self.metadata: Dict[int, Dict[str, Any]] = {}\n        self.id_counter = 0\n        \n        logger.info(f\"Initialized FAISS index with dimension {dimension}\")\n        \n        # Load existing index if path provided\n        if self.index_path and self.index_path.exists():\n            self.load()\n    \n    def add(\n        self,\n        embeddings: np.ndarray,\n        metadata_list: List[Dict[str, Any]]\n    ) -> List[int]:\n        \"\"\"\n        Add embeddings to the index.\n        \n        Args:\n            embeddings: Numpy array of embeddings (n x dimension)\n            metadata_list: List of metadata dicts for each embedding\n        \n        Returns:\n            List of assigned IDs\n        \"\"\"\n        if embeddings.shape[1] != self.dimension:\n            raise ValueError(f\"Embedding dimension mismatch: {embeddings.shape[1]} != {self.dimension}\")\n        \n        if len(metadata_list) != len(embeddings):\n            raise ValueError(\"Number of embeddings and metadata must match\")\n        \n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(embeddings)\n        \n        # Add to index\n        self.index.add(embeddings)\n        \n        # Store metadata\n        ids = []\n        for metadata in metadata_list:\n            self.metadata[self.id_counter] = metadata\n            ids.append(self.id_counter)\n            self.id_counter += 1\n        \n        logger.info(f\"Added {len(embeddings)} embeddings to index\")\n        \n        return ids\n    \n    def search(\n        self,\n        query_embedding: np.ndarray,\n        k: int = 10,\n        return_scores: bool = True\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for similar vectors.\n        \n        Args:\n            query_embedding: Query embedding (1D array)\n            k: Number of results to return\n            return_scores: Whether to include similarity scores\n        \n        Returns:\n            List of results with metadata and optional scores\n        \"\"\"\n        # Reshape to 2D if needed\n        if query_embedding.ndim == 1:\n            query_embedding = query_embedding.reshape(1, -1)\n        \n        # Normalize query\n        faiss.normalize_L2(query_embedding)\n        \n        # Search\n        scores, indices = self.index.search(query_embedding, k)\n        \n        # Build results\n        results = []\n        for i, idx in enumerate(indices[0]):\n            if idx == -1:  # No more results\n                break\n            \n            result = self.metadata.get(int(idx), {}).copy()\n            \n            if return_scores:\n                result['score'] = float(scores[0][i])\n            \n            result['id'] = int(idx)\n            results.append(result)\n        \n        return results\n    \n    def search_batch(\n        self,\n        query_embeddings: np.ndarray,\n        k: int = 10\n    ) -> List[List[Dict[str, Any]]]:\n        \"\"\"\n        Batch search for multiple queries.\n        \n        Args:\n            query_embeddings: Query embeddings (n x dimension)\n            k: Number of results per query\n        \n        Returns:\n            List of result lists\n        \"\"\"\n        # Normalize queries\n        faiss.normalize_L2(query_embeddings)\n        \n        # Search\n        scores, indices = self.index.search(query_embeddings, k)\n        \n        # Build results for each query\n        all_results = []\n        \n        for q_idx in range(len(query_embeddings)):\n            results = []\n            \n            for i, idx in enumerate(indices[q_idx]):\n                if idx == -1:\n                    break\n                \n                result = self.metadata.get(int(idx), {}).copy()\n                result['score'] = float(scores[q_idx][i])\n                result['id'] = int(idx)\n                results.append(result)\n            \n            all_results.append(results)\n        \n        return all_results\n    \n    def save(self, path: Optional[str] = None):\n        \"\"\"\n        Save index and metadata to disk.\n        \n        Args:\n            path: Optional path override\n        \"\"\"\n        save_path = Path(path) if path else self.index_path\n        \n        if not save_path:\n            raise ValueError(\"No save path specified\")\n        \n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Save FAISS index\n        index_file = str(save_path / \"index.faiss\")\n        faiss.write_index(self.index, index_file)\n        \n        # Save metadata\n        metadata_file = str(save_path / \"metadata.pkl\")\n        with open(metadata_file, 'wb') as f:\n            pickle.dump({\n                'metadata': self.metadata,\n                'id_counter': self.id_counter,\n                'dimension': self.dimension\n            }, f)\n        \n        logger.info(f\"Saved index to {save_path}\")\n    \n    def load(self, path: Optional[str] = None):\n        \"\"\"\n        Load index and metadata from disk.\n        \n        Args:\n            path: Optional path override\n        \"\"\"\n        load_path = Path(path) if path else self.index_path\n        \n        if not load_path:\n            raise ValueError(\"No load path specified\")\n        \n        # Load FAISS index\n        index_file = str(load_path / \"index.faiss\")\n        if Path(index_file).exists():\n            self.index = faiss.read_index(index_file)\n        else:\n            logger.warning(f\"Index file not found: {index_file}\")\n            return\n        \n        # Load metadata\n        metadata_file = str(load_path / \"metadata.pkl\")\n        if Path(metadata_file).exists():\n            with open(metadata_file, 'rb') as f:\n                data = pickle.load(f)\n                self.metadata = data['metadata']\n                self.id_counter = data['id_counter']\n                self.dimension = data['dimension']\n        else:\n            logger.warning(f\"Metadata file not found: {metadata_file}\")\n        \n        logger.info(f\"Loaded index from {load_path}\")\n        logger.info(f\"  Total vectors: {self.index.ntotal}\")\n        logger.info(f\"  Metadata entries: {len(self.metadata)}\")\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get index statistics.\"\"\"\n        return {\n            'total_vectors': self.index.ntotal,\n            'dimension': self.dimension,\n            'metadata_count': len(self.metadata),\n            'index_type': type(self.index).__name__\n        }\n    \n    def clear(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index.reset()\n        self.metadata.clear()\n        self.id_counter = 0\n        logger.info(\"Index cleared\")\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    \n    # Create test data\n    dimension = 1024\n    num_vectors = 100\n    \n    print(f\"\\n\ud83d\udd27 Creating FAISS index (dimension={dimension})...\")\n    \n    store = VectorStore(dimension=dimension, index_path=\"./data/vector_db\")\n    \n    # Generate random embeddings\n    embeddings = np.random.randn(num_vectors, dimension).astype('float32')\n    \n    # Create metadata\n    metadata = [\n        {\n            'chunk_id': f\"chunk_{i}\",\n            'file': f\"file_{i % 10}.py\",\n            'start_line': i * 10,\n            'end_line': (i + 1) * 10\n        }\n        for i in range(num_vectors)\n    ]\n    \n    # Add to index\n    print(f\"\ud83d\udce5 Adding {num_vectors} vectors...\")\n    ids = store.add(embeddings, metadata)\n    \n    # Search\n    query = np.random.randn(1, dimension).astype('float32')\n    print(f\"\\n\ud83d\udd0d Searching (k=5)...\")\n    results = store.search(query, k=5)\n    \n    print(f\"\\n\u2705 Found {len(results)} results:\")\n    for r in results:\n        print(f\"  {r['chunk_id']} (score: {r['score']:.4f})\")\n    \n    # Stats\n    stats = store.get_stats()\n    print(f\"\\n\ud83d\udcca Index Stats:\")\n    for key, value in stats.items():\n        print(f\"  {key}: {value}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\embeddings\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\graphs\\asg_builder.py": "\"\"\"\nASG (Abstract Semantic Graph) Builder\n\nExtracts semantic relationships and builds graph in Neo4j.\n\"\"\"\n\nfrom typing import Dict, List, Any, Optional, Set\nimport logging\nimport re\nimport sys\nfrom .neo4j_client import Neo4jClient\n\nlogger = logging.getLogger(__name__)\n\n\n\nclass ASGBuilder:\n    \"\"\"\n    Build Abstract Semantic Graph from code.\n    \n    Nodes: Function, Class, Variable, Import\n    Edges: CALLS, DEFINES, IMPORTS, REFERENCES, OVERRIDES, EXTENDS\n    \"\"\"\n    \n    def __init__(self, neo4j_client: Neo4jClient):\n        \"\"\"\n        Initialize ASG builder.\n        \n        Args:\n            neo4j_client: Neo4j client instance\n        \"\"\"\n        self.client = neo4j_client\n        self.node_cache: Dict[str, str] = {}  # (type, identifier) -> node_id\n    \n    def build_from_manifest(self, manifest: Dict[str, Any]):\n        \"\"\"\n        Build ASG from indexing manifest.\n        \n        Args:\n            manifest: Complete indexing manifest\n        \"\"\"\n        logger.info(\"Building ASG from manifest...\")\n        \n        files = manifest.get('files', [])\n        \n        # First pass: Create nodes\n        for file_data in files:\n            self._process_file(file_data)\n        \n        # Second pass: Create relationships\n        for file_data in files:\n            self._create_relationships(file_data)\n        \n        logger.info(\"ASG build complete\")\n    \n    def _process_file(self, file_data: Dict[str, Any]):\n        \"\"\"Process a single file and create nodes.\"\"\"\n        file_path = file_data['path']\n        \n        # Create file node\n        file_node_id = self.client.create_file_node({\n            'path': file_path,\n            'language': file_data['language'],\n            'size_bytes': file_data['size_bytes']\n        })\n        \n        self.node_cache[('file', file_path)] = file_node_id\n        \n        # Create function nodes\n        for func in file_data.get('functions', []):\n            func_id = self._create_function_node(func, file_path)\n            \n            # Link function to file\n            self.client.create_relationship(\n                file_node_id,\n                func_id,\n                'CONTAINS'\n            )\n        \n        # Create class nodes\n        for cls in file_data.get('classes', []):\n            cls_id = self._create_class_node(cls, file_path)\n            \n            # Link class to file\n            self.client.create_relationship(\n                file_node_id,\n                cls_id,\n                'CONTAINS'\n            )\n    \n    def _create_function_node(self, func: Dict[str, Any], file_path: str) -> str:\n        \"\"\"Create a function node.\"\"\"\n        func_identifier = f\"{file_path}::{func['name']}\"\n        \n        if ('function', func_identifier) in self.node_cache:\n            return self.node_cache[('function', func_identifier)]\n        \n        # Extract signature (simplified)\n        signature = f\"{func['name']}()\"\n        \n        node_id = self.client.create_function_node({\n            'name': func['name'],\n            'file': file_path,\n            'start_line': func['start_line'],\n            'end_line': func['end_line'],\n            'signature': signature,\n            'language': func.get('language', 'unknown'),\n            'complexity': 0  # Will be filled by static analyzer\n        })\n        \n        self.node_cache[('function', func_identifier)] = node_id\n        \n        return node_id\n    \n    def _create_class_node(self, cls: Dict[str, Any], file_path: str) -> str:\n        \"\"\"Create a class node.\"\"\"\n        cls_identifier = f\"{file_path}::{cls['name']}\"\n        \n        if ('class', cls_identifier) in self.node_cache:\n            return self.node_cache[('class', cls_identifier)]\n        \n        node_id = self.client.create_class_node({\n            'name': cls['name'],\n            'file': file_path,\n            'start_line': cls['start_line'],\n            'end_line': cls['end_line'],\n            'language': cls.get('language', 'unknown')\n        })\n        \n        self.node_cache[('class', cls_identifier)] = node_id\n        \n        return node_id\n    \n    def _create_relationships(self, file_data: Dict[str, Any]):\n        \"\"\"Create relationships between nodes (second pass).\"\"\"\n        file_path = file_data['path']\n        \n        # For now, we'll extract basic CALLS relationships from function content\n        # In a production system, this would use AST analysis\n        \n        for func in file_data.get('functions', []):\n            func_identifier = f\"{file_path}::{func['name']}\"\n            func_id = self.node_cache.get(('function', func_identifier))\n            \n            if not func_id:\n                continue\n            \n            # Extract function calls from chunks\n            # This is simplified - real implementation would use AST\n            calls = self._extract_function_calls(file_data, func)\n            \n            for called_func in calls:\n                callee_id = self.node_cache.get(('function', called_func))\n                \n                if callee_id:\n                    try:\n                        self.client.create_relationship(\n                            func_id,\n                            callee_id,\n                            'CALLS'\n                        )\n                    except Exception as e:\n                        logger.debug(f\"Could not create CALLS relationship: {e}\")\n    \n    def _extract_function_calls(self, file_data: Dict[str, Any], function: Dict[str, Any]) -> Set[str]:\n        \"\"\"\n        Extract function calls (simplified).\n        \n        In production, this would use the AST to find actual function calls.\n        For now, we'll use a simple heuristic.\n        \"\"\"\n        calls: Set[str] = set()\n        \n        # Get chunks that overlap with this function\n        start = function['start_line']\n        end = function['end_line']\n        \n        for chunk in file_data.get('chunks', []):\n            if chunk['start_line'] <= end and chunk['end_line'] >= start:\n                # Simple pattern matching for function calls\n                # Pattern: word followed by (\n                pattern = r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\('\n                matches = re.findall(pattern, chunk['content'])\n                \n                for match in matches:\n                    # Try to find this function in the same file\n                    for other_func in file_data.get('functions', []):\n                        if other_func['name'] == match and other_func['name'] != function['name']:\n                            identifier = f\"{file_data['path']}::{match}\"\n                            calls.add(identifier)\n        \n        return calls\n    \n    def get_call_graph(self, function_name: str) -> Dict[str, Any]:\n        \"\"\"\n        Get call graph for a function.\n        \n        Args:\n            function_name: Function name\n        \n        Returns:\n            Dictionary with callers and callees\n        \"\"\"\n        func = self.client.find_function(function_name)\n        \n        if not func:\n            return {'error': 'Function not found'}\n        \n        callers = self.client.get_function_callers(func['id'])\n        callees = self.client.get_function_callees(func['id'])\n        \n        return {\n            'function': func,\n            'callers': callers,\n            'callees': callees\n        }\n    \n    def find_all_paths(\n        self,\n        from_function: str,\n        to_function: str,\n        max_depth: int = 5\n    ) -> List[List[str]]:\n        \"\"\"\n        Find all paths between two functions.\n        \n        Args:\n            from_function: Source function name\n            to_function: Target function name\n            max_depth: Maximum path depth\n        \n        Returns:\n            List of paths (each path is a list of function names)\n        \"\"\"\n        query = \"\"\"\n            MATCH path = (f1:Function {name: $from})-[:CALLS*1..5]->(f2:Function {name: $to})\n            RETURN [node in nodes(path) | node.name] as path_names\n            LIMIT 10\n        \"\"\"\n        \n        results = self.client.execute_cypher(query, {\n            'from': from_function,\n            'to': to_function\n        })\n        \n        return [r['path_names'] for r in results]\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    \n    from ..app.config import get_settings\n    import json\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python asg_builder.py <manifest.json>\")\n        sys.exit(1)\n    \n    manifest_path = sys.argv[1]\n    \n    with open(manifest_path, 'r') as f:\n        manifest = json.load(f)\n    \n    settings = get_settings()\n    \n    with Neo4jClient(settings.neo4j_url, settings.neo4j_username, settings.neo4j_password) as client:\n        # Clear existing data (optional)\n        # client.clear_database()\n        \n        # Create indexes\n        client.create_indexes()\n        \n        # Build ASG\n        builder = ASGBuilder(client)\n        builder.build_from_manifest(manifest)\n        \n        # Show stats\n        stats = client.get_graph_stats()\n        print(\"\\n\u2705 ASG built successfully\")\n        print(f\"\\n\ud83d\udcca Graph Stats:\")\n        for key, value in stats.items():\n            print(f\"  {key}: {value}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\graphs\\cfg_builder.py": "\"\"\"\nCFG (Control Flow Graph) Builder\n\nConstructs control flow graphs for functions showing:\n- Basic blocks (sequences of statements)\n- Control flow edges (jumps, branches, loops)\n- Exception handling paths\n\"\"\"\n\nfrom typing import Dict, List, Any, Set, Optional\nimport logging\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass BasicBlock:\n    \"\"\"Represents a basic block in CFG.\"\"\"\n    id: str\n    statements: List[str]\n    start_line: int\n    end_line: int\n    successors: List[str]  # IDs of successor blocks\n    block_type: str  # 'entry', 'exit', 'normal', 'branch', 'loop', 'exception'\n\n\nclass CFGBuilder:\n    \"\"\"\n    Build Control Flow Graphs for functions.\n    \n    A CFG represents all paths that might be traversed through a function\n    during its execution.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize CFG builder.\"\"\"\n        self.blocks: Dict[str, BasicBlock] = {}\n        self.current_id = 0\n    \n    def build_from_ast(self, ast_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Build CFG from AST data.\n        \n        Args:\n            ast_data: AST representation of function\n        \n        Returns:\n            CFG data structure\n        \"\"\"\n        function_name = ast_data.get('name', 'unknown')\n        body = ast_data.get('body', [])\n        \n        logger.info(f\"Building CFG for function: {function_name}\")\n        \n        # Create entry block\n        entry_block = self._create_block('entry', [], 0, 0)\n        \n        # Process function body\n        current_block = entry_block\n        for statement in body:\n            current_block = self._process_statement(statement, current_block)\n        \n        # Create exit block\n        exit_block = self._create_block('exit', [], 0, 0)\n        \n        # Connect last block to exit\n        if current_block:\n            current_block.successors.append(exit_block.id)\n        \n        return {\n            'function': function_name,\n            'entry_block': entry_block.id,\n            'exit_block': exit_block.id,\n            'blocks': {bid: self._block_to_dict(block) for bid, block in self.blocks.items()},\n            'total_blocks': len(self.blocks),\n            'has_loops': self._has_loops(),\n            'has_branches': self._has_branches(),\n            'has_exceptions': self._has_exceptions(),\n        }\n    \n    def _create_block(\n        self,\n        block_type: str,\n        statements: List[str],\n        start_line: int,\n        end_line: int\n    ) -> BasicBlock:\n        \"\"\"Create a new basic block.\"\"\"\n        block_id = f\"block_{self.current_id}\"\n        self.current_id += 1\n        \n        block = BasicBlock(\n            id=block_id,\n            statements=statements,\n            start_line=start_line,\n            end_line=end_line,\n            successors=[],\n            block_type=block_type\n        )\n        \n        self.blocks[block_id] = block\n        return block\n    \n    def _process_statement(\n        self,\n        statement: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"\n        Process a statement and update CFG.\n        \n        Args:\n            statement: AST statement node\n            current_block: Current basic block\n        \n        Returns:\n            Updated current block\n        \"\"\"\n        stmt_type = statement.get('type', 'unknown')\n        \n        if stmt_type == 'if_statement':\n            return self._process_if(statement, current_block)\n        elif stmt_type == 'while_statement':\n            return self._process_while(statement, current_block)\n        elif stmt_type == 'for_statement':\n            return self._process_for(statement, current_block)\n        elif stmt_type == 'try_statement':\n            return self._process_try(statement, current_block)\n        elif stmt_type == 'return_statement':\n            return self._process_return(statement, current_block)\n        else:\n            # Normal statement - add to current block\n            current_block.statements.append(str(statement))\n            return current_block\n    \n    def _process_if(\n        self,\n        if_stmt: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"Process if statement creating branch.\"\"\"\n        # Create branch block\n        branch_block = self._create_block('branch', [], 0, 0)\n        current_block.successors.append(branch_block.id)\n        \n        # Process then branch\n        then_body = if_stmt.get('then_body', [])\n        then_block = self._create_block('normal', [], 0, 0)\n        branch_block.successors.append(then_block.id)\n        \n        # Process else branch if exists\n        else_body = if_stmt.get('else_body', [])\n        if else_body:\n            else_block = self._create_block('normal', [], 0, 0)\n            branch_block.successors.append(else_block.id)\n        \n        # Create merge block\n        merge_block = self._create_block('normal', [], 0, 0)\n        then_block.successors.append(merge_block.id)\n        if else_body:\n            else_block.successors.append(merge_block.id)\n        else:\n            branch_block.successors.append(merge_block.id)\n        \n        return merge_block\n    \n    def _process_while(\n        self,\n        while_stmt: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"Process while loop.\"\"\"\n        # Create loop header (condition check)\n        loop_header = self._create_block('loop', [], 0, 0)\n        current_block.successors.append(loop_header.id)\n        \n        # Create loop body\n        loop_body = self._create_block('normal', [], 0, 0)\n        loop_header.successors.append(loop_body.id)\n        \n        # Loop body goes back to header\n        loop_body.successors.append(loop_header.id)\n        \n        # Create exit block\n        exit_block = self._create_block('normal', [], 0, 0)\n        loop_header.successors.append(exit_block.id)\n        \n        return exit_block\n    \n    def _process_for(\n        self,\n        for_stmt: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"Process for loop.\"\"\"\n        # Similar to while\n        return self._process_while(for_stmt, current_block)\n    \n    def _process_try(\n        self,\n        try_stmt: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"Process try-except statement.\"\"\"\n        # Create try block\n        try_block = self._create_block('exception', [], 0, 0)\n        current_block.successors.append(try_block.id)\n        \n        # Create except blocks\n        handlers = try_stmt.get('handlers', [])\n        except_blocks = []\n        for handler in handlers:\n            except_block = self._create_block('exception', [], 0, 0)\n            try_block.successors.append(except_block.id)\n            except_blocks.append(except_block)\n        \n        # Create merge block\n        merge_block = self._create_block('normal', [], 0, 0)\n        try_block.successors.append(merge_block.id)\n        for eb in except_blocks:\n            eb.successors.append(merge_block.id)\n        \n        return merge_block\n    \n    def _process_return(\n        self,\n        return_stmt: Dict[str, Any],\n        current_block: BasicBlock\n    ) -> BasicBlock:\n        \"\"\"Process return statement.\"\"\"\n        current_block.statements.append(\"return\")\n        # Returns don't have successors in the normal flow\n        return current_block\n    \n    def _block_to_dict(self, block: BasicBlock) -> Dict[str, Any]:\n        \"\"\"Convert block to dictionary.\"\"\"\n        return {\n            'id': block.id,\n            'type': block.block_type,\n            'statements': block.statements,\n            'start_line': block.start_line,\n            'end_line': block.end_line,\n            'successors': block.successors,\n            'statement_count': len(block.statements),\n        }\n    \n    def _has_loops(self) -> bool:\n        \"\"\"Check if CFG contains loops.\"\"\"\n        return any(b.block_type == 'loop' for b in self.blocks.values())\n    \n    def _has_branches(self) -> bool:\n        \"\"\"Check if CFG contains branches.\"\"\"\n        return any(b.block_type == 'branch' for b in self.blocks.values())\n    \n    def _has_exceptions(self) -> bool:\n        \"\"\"Check if CFG contains exception handling.\"\"\"\n        return any(b.block_type == 'exception' for b in self.blocks.values())\n    \n    def analyze_complexity(self) -> int:\n        \"\"\"\n        Calculate cyclomatic complexity from CFG.\n        \n        M = E - N + 2P\n        where E = edges, N = nodes, P = connected components\n        \"\"\"\n        edges = sum(len(block.successors) for block in self.blocks.values())\n        nodes = len(self.blocks)\n        components = 1  # Assuming single connected component\n        \n        complexity = edges - nodes + 2 * components\n        return max(1, complexity)\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    # Example usage\n    builder = CFGBuilder()\n    \n    # Example AST data\n    ast_data = {\n        'name': 'example_function',\n        'body': [\n            {'type': 'assignment', 'line': 1},\n            {\n                'type': 'if_statement',\n                'condition': 'x > 0',\n                'then_body': [{'type': 'assignment', 'line': 3}],\n                'else_body': [{'type': 'assignment', 'line': 5}],\n            },\n            {'type': 'return_statement', 'line': 6},\n        ],\n    }\n    \n    cfg = builder.build_from_ast(ast_data)\n    \n    print(f\"\\n\ud83d\udcca CFG Analysis for '{cfg['function']}':\")\n    print(f\"  Total blocks: {cfg['total_blocks']}\")\n    print(f\"  Has loops: {cfg['has_loops']}\")\n    print(f\"  Has branches: {cfg['has_branches']}\")\n    print(f\"  Cyclomatic complexity: {builder.analyze_complexity()}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\graphs\\neo4j_client.py": "\"\"\"\nNeo4j Client\n\nHandles all interactions with Neo4j graph database.\n\"\"\"\n\nfrom typing import Dict, List, Any, Optional\nimport logging\nfrom neo4j import GraphDatabase, Driver, Session\n\nlogger = logging.getLogger(__name__)\n\n\nclass Neo4jClient:\n    \"\"\"\n    Client for Neo4j graph database operations.\n    \"\"\"\n    \n    def __init__(self, uri: str, username: str, password: str):\n        \"\"\"\n        Initialize Neo4j client.\n        \n        Args:\n            uri: Neo4j connection URI (bolt://...)\n            username: Database username\n            password: Database password\n        \"\"\"\n        self.uri = uri\n        self.driver: Optional[Driver] = None\n        \n        try:\n            self.driver = GraphDatabase.driver(uri, auth=(username, password))\n            logger.info(f\"Connected to Neo4j at {uri}\")\n        except Exception as e:\n            logger.error(f\"Failed to connect to Neo4j: {e}\")\n            raise\n    \n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.driver:\n            self.driver.close()\n            logger.info(\"Neo4j connection closed\")\n    \n    def verify_connectivity(self) -> bool:\n        \"\"\"Verify database connection.\"\"\"\n        try:\n            with self.driver.session() as session:\n                result = session.run(\"RETURN 1\")\n                result.single()\n            return True\n        except Exception as e:\n            logger.error(f\"Connectivity check failed: {e}\")\n            return False\n    \n    def create_indexes(self):\n        \"\"\"Create database indexes for performance.\"\"\"\n        with self.driver.session() as session:\n            # Function indexes\n            session.run(\"\"\"\n                CREATE INDEX function_name_idx IF NOT EXISTS\n                FOR (f:Function) ON (f.name)\n            \"\"\")\n            \n            session.run(\"\"\"\n                CREATE INDEX function_file_idx IF NOT EXISTS\n                FOR (f:Function) ON (f.file)\n            \"\"\")\n            \n            # Class indexes\n            session.run(\"\"\"\n                CREATE INDEX class_name_idx IF NOT EXISTS\n                FOR (c:Class) ON (c.name)\n            \"\"\")\n            \n            # File indexes\n            session.run(\"\"\"\n                CREATE INDEX file_path_idx IF NOT EXISTS\n                FOR (f:File) ON (f.path)\n            \"\"\")\n            \n            logger.info(\"Created Neo4j indexes\")\n    \n    def clear_database(self):\n        \"\"\"Clear all nodes and relationships (use with caution!).\"\"\"\n        with self.driver.session() as session:\n            session.run(\"MATCH (n) DETACH DELETE n\")\n            logger.warning(\"Cleared entire Neo4j database\")\n    \n    def create_function_node(self, function_data: Dict[str, Any]) -> str:\n        \"\"\"\n        Create a function node.\n        \n        Args:\n            function_data: Function metadata\n        \n        Returns:\n            Node ID\n        \"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                CREATE (f:Function {\n                    name: $name,\n                    file: $file,\n                    start_line: $start_line,\n                    end_line: $end_line,\n                    signature: $signature,\n                    language: $language,\n                    complexity: $complexity\n                })\n                RETURN elementId(f) as id\n            \"\"\", **function_data)\n            \n            return result.single()[\"id\"]\n    \n    def create_class_node(self, class_data: Dict[str, Any]) -> str:\n        \"\"\"Create a class node.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                CREATE (c:Class {\n                    name: $name,\n                    file: $file,\n                    start_line: $start_line,\n                    end_line: $end_line,\n                    language: $language\n                })\n                RETURN elementId(c) as id\n            \"\"\", **class_data)\n            \n            return result.single()[\"id\"]\n    \n    def create_file_node(self, file_data: Dict[str, Any]) -> str:\n        \"\"\"Create a file node.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MERGE (f:File {path: $path})\n                ON CREATE SET \n                    f.language = $language,\n                    f.size_bytes = $size_bytes,\n                    f.created_at = datetime()\n                RETURN elementId(f) as id\n            \"\"\", **file_data)\n            \n            return result.single()[\"id\"]\n    \n    def create_relationship(\n        self,\n        from_node: str,\n        to_node: str,\n        rel_type: str,\n        properties: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Create a relationship between nodes.\n        \n        Args:\n            from_node: Source node ID\n            to_node: Target node ID\n            rel_type: Relationship type (CALLS, DEFINES, etc.)\n            properties: Optional relationship properties\n        \"\"\"\n        props = properties or {}\n        \n        with self.driver.session() as session:\n            # Using elementId for Neo4j 5+\n            query = f\"\"\"\n                MATCH (a), (b)\n                WHERE elementId(a) = $from_id AND elementId(b) = $to_id\n                CREATE (a)-[r:{rel_type}]->(b)\n            \"\"\"\n            \n            if props:\n                query += \" SET \" + \", \".join([f\"r.{k} = ${k}\" for k in props.keys()])\n            \n            session.run(query, from_id=from_node, to_id=to_node, **props)\n    \n    def find_function(self, name: str, file: Optional[str] = None) -> Optional[Dict[str, Any]]:\n        \"\"\"Find a function by name and optionally file.\"\"\"\n        with self.driver.session() as session:\n            if file:\n                result = session.run(\"\"\"\n                    MATCH (f:Function {name: $name, file: $file})\n                    RETURN elementId(f) as id, f\n                    LIMIT 1\n                \"\"\", name=name, file=file)\n            else:\n                result = session.run(\"\"\"\n                    MATCH (f:Function {name: $name})\n                    RETURN elementId(f) as id, f\n                    LIMIT 1\n                \"\"\", name=name)\n            \n            record = result.single()\n            if record:\n                return {\"id\": record[\"id\"], **dict(record[\"f\"])}\n            return None\n    \n    def get_function_callers(self, function_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all functions that call a specific function.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (caller:Function)-[:CALLS]->(f)\n                WHERE elementId(f) = $function_id\n                RETURN elementId(caller) as id, caller\n            \"\"\", function_id=function_id)\n            \n            return [{\"id\": r[\"id\"], **dict(r[\"caller\"])} for r in result]\n    \n    def get_function_callees(self, function_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all functions called by a specific function.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (f)-[:CALLS]->(callee:Function)\n                WHERE elementId(f) = $function_id\n                RETURN elementId(callee) as id, callee\n            \"\"\", function_id=function_id)\n            \n            return [{\"id\": r[\"id\"], **dict(r[\"callee\"])} for r in result]\n    \n    def get_file_functions(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all functions in a file.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (f:Function {file: $file})\n                RETURN elementId(f) as id, f\n                ORDER BY f.start_line\n            \"\"\", file=file_path)\n            \n            return [{\"id\": r[\"id\"], **dict(r[\"f\"])} for r in result]\n    \n    def get_graph_stats(self) -> Dict[str, int]:\n        \"\"\"Get database statistics.\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (n)\n                RETURN \n                    count(n) as total_nodes,\n                    count{(n:Function)} as functions,\n                    count{(n:Class)} as classes,\n                    count{(n:File)} as files,\n                    count { MATCH ()-[r]->() RETURN r } AS total_relationships\n            \"\"\")\n            \n            record = result.single()\n            return dict(record) if record else {}\n    \n    def execute_cypher(self, query: str, parameters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Execute a custom Cypher query.\n        \n        Args:\n            query: Cypher query string\n            parameters: Query parameters\n        \n        Returns:\n            List of result records\n        \"\"\"\n        with self.driver.session() as session:\n            result = session.run(query, parameters or {})\n            return [dict(record) for record in result]\n    \n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    from ..app.config import get_settings\n    \n    settings = get_settings()\n    \n    with Neo4jClient(settings.neo4j_url, settings.neo4j_username, settings.neo4j_password) as client:\n        if client.verify_connectivity():\n            print(\"\u2705 Connected to Neo4j successfully\")\n            \n            # Get stats\n            stats = client.get_graph_stats()\n            print(f\"\\n\ud83d\udcca Database Stats:\")\n            for key, value in stats.items():\n                print(f\"  {key}: {value}\")\n        else:\n            print(\"\u274c Failed to connect to Neo4j\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\graphs\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\indexing\\ast_parser.py": "\"\"\"\nAST Parser - Parse source code into Abstract Syntax Trees using Tree-sitter\n\nSupports: Python, TypeScript, JavaScript\nProvides normalized JSON representation of AST\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom pathlib import Path\nimport json\n\ntry:\n    from tree_sitter import Language, Parser\n    import tree_sitter_python as tspython\n    import tree_sitter_typescript as tstype\n    import tree_sitter_javascript as tsjs\n    HAS_TREE_SITTER = True\nexcept ImportError:\n    HAS_TREE_SITTER = False\n    print(\"Warning: tree-sitter not available. Install with:\")\n    print(\"  pip install tree-sitter tree-sitter-python tree-sitter-typescript tree-sitter-javascript\")\n\n\nclass ASTParser:\n    \"\"\"\n    Parse source code files into AST using Tree-sitter.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize parsers for supported languages.\"\"\"\n        if not HAS_TREE_SITTER:\n            raise RuntimeError(\"tree-sitter not installed\")\n        \n        # Initialize languages\n        self.languages = {\n            'python': Language(tspython.language()),\n            'typescript': Language(tstype.language_typescript()),\n            'tsx': Language(tstype.language_tsx()),\n            'javascript': Language(tsjs.language()),\n        }\n        \n        # Initialize parsers\n        self.parsers = {}\n        for name, lang in self.languages.items():\n            parser = Parser(lang)\n            self.parsers[name] = parser\n    \n    def get_parser(self, language: str) -> Optional[Parser]:\n        \"\"\"Get parser for a specific language.\"\"\"\n        lang_map = {\n            'python': 'python',\n            'typescript': 'typescript',\n            'javascript': 'javascript',\n        }\n        \n        parser_name = lang_map.get(language)\n        return self.parsers.get(parser_name)\n    \n    def parse_file(self, file_path: str, language: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Parse a file and return AST.\n        \n        Args:\n            file_path: Path to source file\n            language: Programming language (python, typescript, javascript)\n        \n        Returns:\n            Dictionary containing AST data or None if parsing fails\n        \"\"\"\n        parser = self.get_parser(language)\n        if not parser:\n            return None\n        \n        try:\n            # Read source code\n            with open(file_path, 'rb') as f:\n                source_code = f.read()\n            \n            # Parse\n            tree = parser.parse(source_code)\n            \n            # Convert to JSON-serializable format\n            ast_json = self._node_to_dict(tree.root_node, source_code)\n            \n            return {\n                'file': str(file_path),\n                'language': language,\n                'ast': ast_json,\n                'has_errors': tree.root_node.has_error\n            }\n        \n        except Exception as e:\n            print(f\"Error parsing {file_path}: {e}\")\n            return None\n    \n    def _node_to_dict(self, node, source_code: bytes) -> Dict[str, Any]:\n        \"\"\"\n        Convert a Tree-sitter node to a dictionary.\n        \n        Args:\n            node: Tree-sitter node\n            source_code: Original source code bytes\n        \n        Returns:\n            Dictionary representation of the node\n        \"\"\"\n        result = {\n            'type': node.type,\n            'start_point': node.start_point,\n            'end_point': node.end_point,\n            'start_byte': node.start_byte,\n            'end_byte': node.end_byte,\n        }\n        \n        # Add text for leaf nodes\n        if node.child_count == 0:\n            text = source_code[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')\n            result['text'] = text\n        \n        # Add children\n        if node.child_count > 0:\n            result['children'] = [\n                self._node_to_dict(child, source_code)\n                for child in node.children\n            ]\n        \n        return result\n    \n    def extract_functions(self, ast_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract function definitions from AST.\n        \n        Args:\n            ast_data: AST dictionary from parse_file\n        \n        Returns:\n            List of function metadata\n        \"\"\"\n        functions = []\n        \n        def traverse(node: Dict[str, Any], depth: int = 0):\n            node_type = node.get('type', '')\n            \n            # Python function\n            if node_type == 'function_definition':\n                func_info = {\n                    'name': self._get_function_name(node),\n                    'start_line': node['start_point'][0] + 1,\n                    'end_line': node['end_point'][0] + 1,\n                    'type': 'function',\n                    'language': ast_data.get('language')\n                }\n                functions.append(func_info)\n            \n            # TypeScript/JavaScript function\n            elif node_type in ['function_declaration', 'method_definition', 'arrow_function']:\n                func_info = {\n                    'name': self._get_function_name(node),\n                    'start_line': node['start_point'][0] + 1,\n                    'end_line': node['end_point'][0] + 1,\n                    'type': 'function',\n                    'language': ast_data.get('language')\n                }\n                functions.append(func_info)\n            \n            # Recursively traverse children\n            for child in node.get('children', []):\n                traverse(child, depth + 1)\n        \n        if 'ast' in ast_data:\n            traverse(ast_data['ast'])\n        \n        return functions\n    \n    def _get_function_name(self, node: Dict[str, Any]) -> str:\n        \"\"\"Extract function name from node.\"\"\"\n        for child in node.get('children', []):\n            if child.get('type') == 'identifier':\n                return child.get('text', 'anonymous')\n        return 'anonymous'\n    \n    def extract_classes(self, ast_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract class definitions from AST.\n        \n        Args:\n            ast_data: AST dictionary from parse_file\n        \n        Returns:\n            List of class metadata\n        \"\"\"\n        classes = []\n        \n        def traverse(node: Dict[str, Any]):\n            node_type = node.get('type', '')\n            \n            if node_type in ['class_definition', 'class_declaration']:\n                class_info = {\n                    'name': self._get_class_name(node),\n                    'start_line': node['start_point'][0] + 1,\n                    'end_line': node['end_point'][0] + 1,\n                    'type': 'class',\n                    'language': ast_data.get('language')\n                }\n                classes.append(class_info)\n            \n            for child in node.get('children', []):\n                traverse(child)\n        \n        if 'ast' in ast_data:\n            traverse(ast_data['ast'])\n        \n        return classes\n    \n    def _get_class_name(self, node: Dict[str, Any]) -> str:\n        \"\"\"Extract class name from node.\"\"\"\n        for child in node.get('children', []):\n            if child.get('type') == 'identifier':\n                return child.get('text', 'Anonymous')\n        return 'Anonymous'\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 3:\n        print(\"Usage: python ast_parser.py <file_path> <language>\")\n        print(\"Languages: python, typescript, javascript\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    language = sys.argv[2]\n    \n    parser = ASTParser()\n    result = parser.parse_file(file_path, language)\n    \n    if result:\n        print(f\"\\n\u2705 Parsed successfully\")\n        print(f\"Has errors: {result['has_errors']}\")\n        \n        # Extract functions\n        functions = parser.extract_functions(result)\n        print(f\"\\n\ud83d\udce6 Found {len(functions)} functions:\")\n        for func in functions:\n            print(f\"  - {func['name']} (lines {func['start_line']}-{func['end_line']})\")\n        \n        # Extract classes\n        classes = parser.extract_classes(result)\n        print(f\"\\n\ud83d\udce6 Found {len(classes)} classes:\")\n        for cls in classes:\n            print(f\"  - {cls['name']} (lines {cls['start_line']}-{cls['end_line']})\")\n    else:\n        print(\"\u274c Parsing failed\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\indexing\\chunker.py": "\"\"\"\nCode Chunker - Split source code into token-aware overlapping chunks\n\nChunks respect function/class boundaries and maintain context\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nimport hashlib\nimport re\n\n\n@dataclass\nclass Chunk:\n    \"\"\"Represents a code chunk.\"\"\"\n    chunk_id: str\n    file_path: str\n    start_line: int\n    end_line: int\n    content: str\n    token_count: int\n    language: str\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n\nclass CodeChunker:\n    \"\"\"\n    Split source code into overlapping chunks with token-aware boundaries.\n    \"\"\"\n    \n    def __init__(\n        self,\n        chunk_size: int = 400,\n        chunk_overlap: int = 70,\n        respect_boundaries: bool = True\n    ):\n        \"\"\"\n        Initialize the code chunker.\n        \n        Args:\n            chunk_size: Target chunk size in tokens\n            chunk_overlap: Number of overlapping tokens between chunks\n            respect_boundaries: Whether to respect function/class boundaries\n        \"\"\"\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.respect_boundaries = respect_boundaries\n    \n    def estimate_tokens(self, text: str) -> int:\n        \"\"\"\n        Estimate token count for text.\n        Uses simple word-based estimation (rough approximation).\n        \n        Args:\n            text: Text to estimate\n        \n        Returns:\n            Estimated token count\n        \"\"\"\n        # Simple estimation: split by whitespace and punctuation\n        # Roughly: 1 token \u2248 0.75 words for code\n        words = len(re.findall(r'\\w+', text))\n        return int(words * 1.3)  # Code tends to have more tokens per word\n    \n    def generate_chunk_id(self, file_path: str, start_line: int, end_line: int) -> str:\n        \"\"\"\n        Generate a unique chunk ID.\n        \n        Args:\n            file_path: Path to source file\n            start_line: Starting line number\n            end_line: Ending line number\n        \n        Returns:\n            SHA1 hash of filepath + line span\n        \"\"\"\n        content = f\"{file_path}:{start_line}-{end_line}\"\n        return hashlib.sha1(content.encode()).hexdigest()\n    \n    def chunk_file(\n        self,\n        file_path: str,\n        content: str,\n        language: str,\n        functions: Optional[List[Dict[str, Any]]] = None\n    ) -> List[Chunk]:\n        \"\"\"\n        Chunk a file into overlapping segments.\n        \n        Args:\n            file_path: Path to source file\n            content: File content\n            language: Programming language\n            functions: Optional list of function boundaries\n        \n        Returns:\n            List of Chunk objects\n        \"\"\"\n        lines = content.splitlines()\n        chunks: List[Chunk] = []\n        \n        if self.respect_boundaries and functions:\n            # Chunk respecting function boundaries\n            chunks = self._chunk_with_boundaries(file_path, lines, language, functions)\n        else:\n            # Simple overlapping chunks\n            chunks = self._chunk_simple(file_path, lines, language)\n        \n        return chunks\n    \n    def _chunk_simple(\n        self,\n        file_path: str,\n        lines: List[str],\n        language: str\n    ) -> List[Chunk]:\n        \"\"\"Create simple overlapping chunks.\"\"\"\n        chunks: List[Chunk] = []\n        total_lines = len(lines)\n        \n        start_line = 0\n        \n        while start_line < total_lines:\n            # Determine end line based on token count\n            end_line = start_line\n            current_tokens = 0\n            \n            while end_line < total_lines and current_tokens < self.chunk_size:\n                line_tokens = self.estimate_tokens(lines[end_line])\n                current_tokens += line_tokens\n                end_line += 1\n            \n            # Create chunk\n            chunk_lines = lines[start_line:end_line]\n            chunk_content = '\\n'.join(chunk_lines)\n            \n            chunk = Chunk(\n                chunk_id=self.generate_chunk_id(file_path, start_line + 1, end_line),\n                file_path=file_path,\n                start_line=start_line + 1,  # 1-indexed\n                end_line=end_line,\n                content=chunk_content,\n                token_count=self.estimate_tokens(chunk_content),\n                language=language\n            )\n            \n            chunks.append(chunk)\n            \n            # Move start position with overlap\n            # Calculate overlap in lines\n            overlap_tokens = 0\n            overlap_lines = 0\n            \n            for i in range(end_line - 1, start_line - 1, -1):\n                overlap_tokens += self.estimate_tokens(lines[i])\n                overlap_lines += 1\n                \n                if overlap_tokens >= self.chunk_overlap:\n                    break\n            \n            start_line = end_line - overlap_lines\n            \n            # Prevent infinite loop\n            if start_line >= end_line:\n                start_line = end_line\n        \n        return chunks\n    \n    def _chunk_with_boundaries(\n        self,\n        file_path: str,\n        lines: List[str],\n        language: str,\n        functions: List[Dict[str, Any]]\n    ) -> List[Chunk]:\n        \"\"\"\n        Create chunks respecting function boundaries.\n        \n        Each function gets its own chunk if possible.\n        Large functions are still split.\n        \"\"\"\n        chunks: List[Chunk] = []\n        \n        # Sort functions by start line\n        sorted_functions = sorted(functions, key=lambda f: f['start_line'])\n        \n        last_end = 0\n        \n        for func in sorted_functions:\n            func_start = func['start_line'] - 1  # 0-indexed\n            func_end = func['end_line']\n            \n            # Add gap between functions if it exists\n            if last_end < func_start:\n                gap_lines = lines[last_end:func_start]\n                if gap_lines:\n                    gap_content = '\\n'.join(gap_lines)\n                    gap_tokens = self.estimate_tokens(gap_content)\n                    \n                    if gap_tokens > 50:  # Only create chunk if substantial\n                        chunk = Chunk(\n                            chunk_id=self.generate_chunk_id(file_path, last_end + 1, func_start),\n                            file_path=file_path,\n                            start_line=last_end + 1,\n                            end_line=func_start,\n                            content=gap_content,\n                            token_count=gap_tokens,\n                            language=language\n                        )\n                        chunks.append(chunk)\n            \n            # Add function chunk\n            func_lines = lines[func_start:func_end]\n            func_content = '\\n'.join(func_lines)\n            func_tokens = self.estimate_tokens(func_content)\n            \n            # If function is too large, split it\n            if func_tokens > self.chunk_size * 1.5:\n                sub_chunks = self._chunk_simple(file_path, func_lines, language)\n                chunks.extend(sub_chunks)\n            else:\n                chunk = Chunk(\n                    chunk_id=self.generate_chunk_id(file_path, func_start + 1, func_end),\n                    file_path=file_path,\n                    start_line=func_start + 1,\n                    end_line=func_end,\n                    content=func_content,\n                    token_count=func_tokens,\n                    language=language\n                )\n                chunks.append(chunk)\n            \n            last_end = func_end\n        \n        # Add remaining content after last function\n        if last_end < len(lines):\n            remaining_lines = lines[last_end:]\n            remaining_content = '\\n'.join(remaining_lines)\n            remaining_tokens = self.estimate_tokens(remaining_content)\n            \n            if remaining_tokens > 50:\n                chunk = Chunk(\n                    chunk_id=self.generate_chunk_id(file_path, last_end + 1, len(lines)),\n                    file_path=file_path,\n                    start_line=last_end + 1,\n                    end_line=len(lines),\n                    content=remaining_content,\n                    token_count=remaining_tokens,\n                    language=language\n                )\n                chunks.append(chunk)\n        \n        return chunks\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python chunker.py <file_path>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    chunker = CodeChunker(chunk_size=400, chunk_overlap=70)\n    chunks = chunker.chunk_file(file_path, content, 'python')\n    \n    print(f\"\\n\u2705 Created {len(chunks)} chunks\")\n    print(f\"\\nChunk summary:\")\n    for i, chunk in enumerate(chunks, 1):\n        print(f\"  Chunk {i}: lines {chunk.start_line}-{chunk.end_line} ({chunk.token_count} tokens)\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\indexing\\manifest.py": "\"\"\"\nManifest Generator - Combine all indexing results into a unified manifest\n\nAggregates file metadata, AST data, chunks, and summaries into a single structure.\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nimport logging\n\nfrom walker import FileWalker, FileMetadata\nfrom ast_parser import ASTParser\nfrom chunker import CodeChunker, Chunk\n\nlogger = logging.getLogger(__name__)\n\n\nclass ManifestGenerator:\n    \"\"\"\n    Generate comprehensive manifest from indexing results.\n    \"\"\"\n    \n    def __init__(\n        self,\n        walker: FileWalker,\n        parser: ASTParser,\n        chunker: CodeChunker\n    ):\n        \"\"\"\n        Initialize manifest generator.\n        \n        Args:\n            walker: File walker instance\n            parser: AST parser instance\n            chunker: Code chunker instance\n        \"\"\"\n        self.walker = walker\n        self.parser = parser\n        self.chunker = chunker\n    \n    def generate(\n        self,\n        include_ast: bool = False,\n        include_chunks: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate complete manifest.\n        \n        Args:\n            include_ast: Whether to include full AST data (large)\n            include_chunks: Whether to include chunk data\n        \n        Returns:\n            Complete manifest dictionary\n        \"\"\"\n        logger.info(\"Starting manifest generation...\")\n        \n        # Walk directory tree\n        files = self.walker.walk()\n        logger.info(f\"Discovered {len(files)} files\")\n        \n        # Process each file\n        processed_files = []\n        total_chunks = 0\n        total_functions = 0\n        total_classes = 0\n        \n        for file_meta in files:\n            logger.debug(f\"Processing {file_meta.relative_path}\")\n            \n            file_data = {\n                'path': file_meta.path,\n                'relative_path': file_meta.relative_path,\n                'size_bytes': file_meta.size_bytes,\n                'modified_time': file_meta.modified_time,\n                'language': file_meta.language,\n            }\n            \n            # Parse AST\n            try:\n                ast_data = self.parser.parse_file(file_meta.path, file_meta.language)\n                \n                if ast_data:\n                    # Extract functions and classes\n                    functions = self.parser.extract_functions(ast_data)\n                    classes = self.parser.extract_classes(ast_data)\n                    \n                    file_data['has_parse_errors'] = ast_data.get('has_errors', False)\n                    file_data['functions'] = functions\n                    file_data['classes'] = classes\n                    file_data['function_count'] = len(functions)\n                    file_data['class_count'] = len(classes)\n                    \n                    total_functions += len(functions)\n                    total_classes += len(classes)\n                    \n                    # Include full AST if requested\n                    if include_ast:\n                        file_data['ast'] = ast_data.get('ast')\n                    \n                    # Generate chunks\n                    if include_chunks:\n                        with open(file_meta.path, 'r', encoding='utf-8') as f:\n                            content = f.read()\n                        \n                        chunks = self.chunker.chunk_file(\n                            file_meta.path,\n                            content,\n                            file_meta.language,\n                            functions\n                        )\n                        \n                        file_data['chunks'] = [chunk.to_dict() for chunk in chunks]\n                        file_data['chunk_count'] = len(chunks)\n                        total_chunks += len(chunks)\n                else:\n                    file_data['has_parse_errors'] = True\n                    file_data['functions'] = []\n                    file_data['classes'] = []\n                    file_data['function_count'] = 0\n                    file_data['class_count'] = 0\n                    \n            except Exception as e:\n                logger.error(f\"Error processing {file_meta.path}: {e}\")\n                file_data['processing_error'] = str(e)\n            \n            processed_files.append(file_data)\n        \n        # Build manifest\n        manifest = {\n            'metadata': {\n                'root_path': str(self.walker.root_path),\n                'generated_at': datetime.now().isoformat(),\n                'version': '1.0.0',\n            },\n            'statistics': {\n                'total_files': len(files),\n                'total_functions': total_functions,\n                'total_classes': total_classes,\n                'total_chunks': total_chunks,\n                'languages': self._count_by_language(files),\n            },\n            'files': processed_files,\n        }\n        \n        logger.info(\"Manifest generation complete\")\n        logger.info(f\"  Files: {len(files)}\")\n        logger.info(f\"  Functions: {total_functions}\")\n        logger.info(f\"  Classes: {total_classes}\")\n        logger.info(f\"  Chunks: {total_chunks}\")\n        \n        return manifest\n    \n    def _count_by_language(self, files: List[FileMetadata]) -> Dict[str, int]:\n        \"\"\"Count files by language.\"\"\"\n        counts: Dict[str, int] = {}\n        for file in files:\n            lang = file.language\n            counts[lang] = counts.get(lang, 0) + 1\n        return counts\n    \n    def save_manifest(self, manifest: Dict[str, Any], output_path: str):\n        \"\"\"\n        Save manifest to JSON file.\n        \n        Args:\n            manifest: Manifest dictionary\n            output_path: Output file path\n        \"\"\"\n        output_file = Path(output_path)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n        \n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(manifest, f, indent=2, ensure_ascii=False)\n        \n        logger.info(f\"Manifest saved to {output_path}\")\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python manifest.py <repo_path> [output.json]\")\n        sys.exit(1)\n    \n    repo_path = sys.argv[1]\n    output_path = sys.argv[2] if len(sys.argv) > 2 else \"manifest.json\"\n    \n    # Initialize components\n    walker = FileWalker(repo_path)\n    parser = ASTParser()\n    chunker = CodeChunker(chunk_size=400, chunk_overlap=70)\n    \n    # Generate manifest\n    generator = ManifestGenerator(walker, parser, chunker)\n    manifest = generator.generate(include_ast=False, include_chunks=True)\n    \n    # Save to file\n    generator.save_manifest(manifest, output_path)\n    \n    print(f\"\\n\u2705 Manifest generated successfully\")\n    print(f\"\ud83d\udcc4 Saved to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\indexing\\walker.py": "\"\"\"\nFile Walker - Recursively discover and catalog source files\n\nSupports: Python (.py), TypeScript (.ts, .tsx), JavaScript (.js, .jsx)\nExcludes: node_modules, .git, dist, __pycache__, .venv, etc.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Set, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nimport json\n\n\n@dataclass\nclass FileMetadata:\n    \"\"\"Metadata for a discovered file.\"\"\"\n    path: str\n    relative_path: str\n    size_bytes: int\n    modified_time: str\n    language: str\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n\nclass FileWalker:\n    \"\"\"\n    Recursively walk a directory tree and catalog source code files.\n    \"\"\"\n    \n    # Supported file extensions\n    SUPPORTED_EXTENSIONS = {\n        '.py': 'python',\n        '.ts': 'typescript',\n        '.tsx': 'typescript',\n        '.js': 'javascript',\n        '.jsx': 'javascript',\n    }\n    \n    # Directories to exclude\n    EXCLUDED_DIRS = {\n        'node_modules',\n        '.git',\n        'dist',\n        'build',\n        '__pycache__',\n        '.venv',\n        'venv',\n        'env',\n        '.next',\n        'out',\n        'coverage',\n        '.pytest_cache',\n        '.mypy_cache',\n        '.tox',\n        'eggs',\n        '.eggs',\n        'wheels',\n    }\n    \n    # File patterns to exclude\n    EXCLUDED_PATTERNS = {\n        '.pyc',\n        '.pyo',\n        '.so',\n        '.dylib',\n        '.dll',\n        '.egg',\n        '.log',\n        '.tmp',\n        '.swp',\n        '.swo',\n        '.DS_Store',\n    }\n    \n    def __init__(self, root_path: str):\n        \"\"\"\n        Initialize the file walker.\n        \n        Args:\n            root_path: Root directory to start walking from\n        \"\"\"\n        self.root_path = Path(root_path).resolve()\n        \n        if not self.root_path.exists():\n            raise ValueError(f\"Path does not exist: {root_path}\")\n        \n        if not self.root_path.is_dir():\n            raise ValueError(f\"Path is not a directory: {root_path}\")\n    \n    def should_exclude_dir(self, dir_name: str) -> bool:\n        \"\"\"Check if a directory should be excluded.\"\"\"\n        return dir_name in self.EXCLUDED_DIRS or dir_name.startswith('.')\n    \n    def should_exclude_file(self, file_path: Path) -> bool:\n        \"\"\"Check if a file should be excluded.\"\"\"\n        # Check patterns\n        for pattern in self.EXCLUDED_PATTERNS:\n            if file_path.name.endswith(pattern):\n                return True\n        \n        # Check extension\n        if file_path.suffix not in self.SUPPORTED_EXTENSIONS:\n            return True\n        \n        return False\n    \n    def get_language(self, file_path: Path) -> str:\n        \"\"\"Determine the programming language from file extension.\"\"\"\n        return self.SUPPORTED_EXTENSIONS.get(file_path.suffix, 'unknown')\n    \n    def get_file_metadata(self, file_path: Path) -> FileMetadata:\n        \"\"\"Extract metadata for a file.\"\"\"\n        stat = file_path.stat()\n        \n        return FileMetadata(\n            path=str(file_path),\n            relative_path=str(file_path.relative_to(self.root_path)),\n            size_bytes=stat.st_size,\n            modified_time=datetime.fromtimestamp(stat.st_mtime).isoformat(),\n            language=self.get_language(file_path)\n        )\n    \n    def walk(self) -> List[FileMetadata]:\n        \"\"\"\n        Walk the directory tree and collect file metadata.\n        \n        Returns:\n            List of FileMetadata objects for all discovered source files\n        \"\"\"\n        discovered_files: List[FileMetadata] = []\n        \n        for root, dirs, files in os.walk(self.root_path, followlinks=False):\n            root_path = Path(root)\n            \n            # Filter out excluded directories (modify in-place to prune walk)\n            dirs[:] = [d for d in dirs if not self.should_exclude_dir(d)]\n            \n            # Process files\n            for filename in files:\n                file_path = root_path / filename\n                \n                # Skip excluded files\n                if self.should_exclude_file(file_path):\n                    continue\n                \n                # Add file metadata\n                try:\n                    metadata = self.get_file_metadata(file_path)\n                    discovered_files.append(metadata)\n                except (OSError, PermissionError) as e:\n                    print(f\"Warning: Could not access {file_path}: {e}\")\n                    continue\n        \n        return discovered_files\n    \n    def generate_manifest(self, output_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generate a complete manifest of discovered files.\n        \n        Args:\n            output_path: Optional path to save manifest JSON\n        \n        Returns:\n            Manifest dictionary\n        \"\"\"\n        files = self.walk()\n        \n        manifest = {\n            'root_path': str(self.root_path),\n            'total_files': len(files),\n            'languages': self._count_by_language(files),\n            'timestamp': datetime.now().isoformat(),\n            'files': [f.to_dict() for f in files]\n        }\n        \n        # Save to file if requested\n        if output_path:\n            output_file = Path(output_path)\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            with open(output_file, 'w', encoding='utf-8') as f:\n                json.dump(manifest, f, indent=2, ensure_ascii=False)\n        \n        return manifest\n    \n    def _count_by_language(self, files: List[FileMetadata]) -> Dict[str, int]:\n        \"\"\"Count files by programming language.\"\"\"\n        counts: Dict[str, int] = {}\n        \n        for file in files:\n            lang = file.language\n            counts[lang] = counts.get(lang, 0) + 1\n        \n        return counts\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python walker.py <directory_path> [output_manifest.json]\")\n        sys.exit(1)\n    \n    root_path = sys.argv[1]\n    output_path = sys.argv[2] if len(sys.argv) > 2 else None\n    \n    walker = FileWalker(root_path)\n    manifest = walker.generate_manifest(output_path)\n    \n    print(f\"\\n\u2705 Discovered {manifest['total_files']} files\")\n    print(f\"\\nLanguage breakdown:\")\n    for lang, count in manifest['languages'].items():\n        print(f\"  {lang}: {count}\")\n    \n    if output_path:\n        print(f\"\\n\ud83d\udcc4 Manifest saved to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\indexing\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\intelligence\\code_health_db.py": "\"\"\"\nCode Health Database\n\nSQLite database for storing code health metrics and history.\n\"\"\"\n\nimport sqlite3\nfrom typing import Dict, List, Any, Optional\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass CodeHealthDB:\n    \"\"\"\n    Database for code health tracking.\n    \"\"\"\n    \n    def __init__(self, db_path: str):\n        \"\"\"\n        Initialize database connection.\n        \n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = Path(db_path)\n        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        self.conn = sqlite3.connect(str(self.db_path))\n        self.conn.row_factory = sqlite3.Row\n        \n        self._create_tables()\n    \n    def _create_tables(self):\n        \"\"\"Create database tables.\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Code smells table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS code_smells (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                file TEXT NOT NULL,\n                line INTEGER NOT NULL,\n                smell_type TEXT NOT NULL,\n                severity TEXT NOT NULL,\n                message TEXT,\n                suggestion TEXT,\n                detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                resolved BOOLEAN DEFAULT 0\n            )\n        ''')\n        \n        # Function risk scores table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS function_risk_scores (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                function_id TEXT NOT NULL,\n                file TEXT NOT NULL,\n                function_name TEXT NOT NULL,\n                health_score REAL NOT NULL,\n                complexity INTEGER NOT NULL,\n                bug_count INTEGER DEFAULT 0,\n                churn_rate REAL DEFAULT 0.0,\n                last_modified TIMESTAMP,\n                calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Change impact table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS change_impact (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                change_id TEXT NOT NULL,\n                file TEXT NOT NULL,\n                affected_functions TEXT,  -- JSON array\n                risk_level TEXT NOT NULL,\n                impact_score REAL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Bug hotspots table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS bug_hotspots (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                file TEXT NOT NULL,\n                function_name TEXT,\n                bug_frequency INTEGER DEFAULT 1,\n                last_bug_at TIMESTAMP,\n                severity TEXT,\n                notes TEXT,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Refactor history table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS refactor_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                function_id TEXT NOT NULL,\n                file TEXT NOT NULL,\n                function_name TEXT NOT NULL,\n                reason TEXT,\n                before_score REAL,\n                after_score REAL,\n                refactored_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # File health history table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS file_health_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                file TEXT NOT NULL,\n                health_score REAL NOT NULL,\n                metrics TEXT,  -- JSON\n                scanned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        # Create indexes\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_smells_file ON code_smells(file)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_smells_resolved ON code_smells(resolved)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_function_risk_file ON function_risk_scores(file)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_function_risk_id ON function_risk_scores(function_id)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_hotspots_file ON bug_hotspots(file)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_health_file ON file_health_history(file)')\n        \n        self.conn.commit()\n    \n    def save_code_smells(self, smells: List[Dict[str, Any]]):\n        \"\"\"Save code smells to database.\"\"\"\n        cursor = self.conn.cursor()\n        \n        for smell in smells:\n            cursor.execute('''\n                INSERT INTO code_smells (file, line, smell_type, severity, message, suggestion)\n                VALUES (?, ?, ?, ?, ?, ?)\n            ''', (\n                smell.get('file'),\n                smell.get('line'),\n                smell.get('type'),\n                smell.get('severity'),\n                smell.get('message'),\n                smell.get('suggestion')\n            ))\n        \n        self.conn.commit()\n        logger.info(f\"Saved {len(smells)} code smells\")\n    \n    def save_function_risk_score(self, score_data: Dict[str, Any]):\n        \"\"\"Save function risk score.\"\"\"\n        cursor = self.conn.cursor()\n        \n        function_id = f\"{score_data['file']}::{score_data['function_name']}\"\n        \n        cursor.execute('''\n            INSERT INTO function_risk_scores \n            (function_id, file, function_name, health_score, complexity, bug_count, churn_rate)\n            VALUES (?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            function_id,\n            score_data['file'],\n            score_data['function_name'],\n            score_data['health_score'],\n            score_data['complexity'],\n            score_data.get('bug_count', 0),\n            score_data.get('churn_rate', 0.0)\n        ))\n        \n        self.conn.commit()\n    \n    def save_bug_hotspot(self, hotspot: Dict[str, Any]):\n        \"\"\"Save or update bug hotspot.\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Check if hotspot exists\n        cursor.execute('''\n            SELECT id, bug_frequency FROM bug_hotspots\n            WHERE file = ? AND function_name = ?\n        ''', (hotspot['file'], hotspot.get('function_name', '')))\n        \n        existing = cursor.fetchone()\n        \n        if existing:\n            # Update existing\n            cursor.execute('''\n                UPDATE bug_hotspots\n                SET bug_frequency = bug_frequency + 1,\n                    last_bug_at = CURRENT_TIMESTAMP,\n                    severity = ?,\n                    notes = ?\n                WHERE id = ?\n            ''', (\n                hotspot.get('severity', 'medium'),\n                hotspot.get('notes', ''),\n                existing['id']\n            ))\n        else:\n            # Insert new\n            cursor.execute('''\n                INSERT INTO bug_hotspots\n                (file, function_name, bug_frequency, last_bug_at, severity, notes)\n                VALUES (?, ?, 1, CURRENT_TIMESTAMP, ?, ?)\n            ''', (\n                hotspot['file'],\n                hotspot.get('function_name', ''),\n                hotspot.get('severity', 'medium'),\n                hotspot.get('notes', '')\n            ))\n        \n        self.conn.commit()\n    \n    def save_file_health(self, file_path: str, health_score: float, metrics: Dict[str, Any]):\n        \"\"\"Save file health history.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            INSERT INTO file_health_history (file, health_score, metrics)\n            VALUES (?, ?, ?)\n        ''', (\n            file_path,\n            health_score,\n            json.dumps(metrics)\n        ))\n        \n        self.conn.commit()\n    \n    def get_file_health_trend(self, file_path: str, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get health trend for a file.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            SELECT health_score, scanned_at\n            FROM file_health_history\n            WHERE file = ?\n            ORDER BY scanned_at DESC\n            LIMIT ?\n        ''', (file_path, limit))\n        \n        rows = cursor.fetchall()\n        \n        return [\n            {\n                'health_score': row['health_score'],\n                'scanned_at': row['scanned_at']\n            }\n            for row in rows\n        ]\n    \n    def get_top_hotspots(self, limit: int = 20) -> List[Dict[str, Any]]:\n        \"\"\"Get top bug hotspots.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            SELECT file, function_name, bug_frequency, last_bug_at, severity, notes\n            FROM bug_hotspots\n            ORDER BY bug_frequency DESC, last_bug_at DESC\n            LIMIT ?\n        ''', (limit,))\n        \n        rows = cursor.fetchall()\n        \n        return [dict(row) for row in rows]\n    \n    def get_unresolved_smells(self, file_path: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Get unresolved code smells.\"\"\"\n        cursor = self.conn.cursor()\n        \n        if file_path:\n            cursor.execute('''\n                SELECT file, line, smell_type, severity, message, suggestion, detected_at\n                FROM code_smells\n                WHERE resolved = 0 AND file = ?\n                ORDER BY severity DESC, detected_at DESC\n            ''', (file_path,))\n        else:\n            cursor.execute('''\n                SELECT file, line, smell_type, severity, message, suggestion, detected_at\n                FROM code_smells\n                WHERE resolved = 0\n                ORDER BY severity DESC, detected_at DESC\n            ''')\n        \n        rows = cursor.fetchall()\n        \n        return [dict(row) for row in rows]\n    \n    def mark_smell_resolved(self, smell_id: int):\n        \"\"\"Mark a code smell as resolved.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            UPDATE code_smells\n            SET resolved = 1\n            WHERE id = ?\n        ''', (smell_id,))\n        \n        self.conn.commit()\n    \n    def get_function_history(self, function_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get health history for a function.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            SELECT health_score, complexity, bug_count, churn_rate, calculated_at\n            FROM function_risk_scores\n            WHERE function_id = ?\n            ORDER BY calculated_at DESC\n        ''', (function_id,))\n        \n        rows = cursor.fetchall()\n        \n        return [dict(row) for row in rows]\n    \n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.conn:\n            self.conn.close()\n    \n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n",
    "src\\intelligence\\health_scanner.py": "\"\"\"\nCode Health Scanner\n\nCalculates per-function health scores and identifies bug hotspots.\n\"\"\"\n\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nimport logging\n\nfrom static_analyzer import StaticAnalyzer, CodeSmell\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass HealthScore:\n    \"\"\"Health score for a code entity.\"\"\"\n    entity_type: str  # 'file' or 'function'\n    entity_name: str\n    file_path: str\n    score: float  # 0-100 (higher is better)\n    complexity: float\n    smell_count: int\n    bug_risk: str  # 'low', 'medium', 'high'\n    churn_rate: float  # placeholder for git history\n    recommendations: List[str]\n\n\nclass CodeHealthScanner:\n    \"\"\"\n    Scan code and calculate health scores.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize code health scanner.\"\"\"\n        self.analyzer = StaticAnalyzer()\n    \n    def scan_file(\n        self,\n        file_path: str,\n        content: str,\n        language: str,\n        functions: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Scan a file and calculate health metrics.\n        \n        Args:\n            file_path: Path to file\n            content: File content\n            language: Programming language\n            functions: List of function metadata\n        \n        Returns:\n            Dictionary of health metrics\n        \"\"\"\n        # Get code smells\n        smells = self.analyzer.analyze_file(file_path, content, language, functions)\n        \n        # Get file-level metrics\n        metrics = self.analyzer.calculate_metrics(functions, content)\n        \n        # Calculate file health score\n        file_score = self._calculate_file_health(metrics, smells)\n        \n        # Calculate function-level scores\n        function_scores = []\n        \n        for func in functions:\n            func_score = self._calculate_function_health(\n                file_path,\n                func,\n                content.splitlines(),\n                language,\n                smells\n            )\n            function_scores.append(func_score)\n        \n        return {\n            'file_path': file_path,\n            'file_health_score': file_score.score,\n            'file_bug_risk': file_score.bug_risk,\n            'metrics': metrics,\n            'smells': [\n                {\n                    'type': s.smell_type,\n                    'line': s.line,\n                    'severity': s.severity,\n                    'message': s.message,\n                    'suggestion': s.suggestion\n                }\n                for s in smells\n            ],\n            'function_health': [\n                {\n                    'name': fs.entity_name,\n                    'score': fs.score,\n                    'complexity': fs.complexity,\n                    'bug_risk': fs.bug_risk,\n                    'recommendations': fs.recommendations\n                }\n                for fs in function_scores\n            ]\n        }\n    \n    def _calculate_file_health(\n        self,\n        metrics: Dict[str, Any],\n        smells: List[CodeSmell]\n    ) -> HealthScore:\n        \"\"\"Calculate file-level health score.\"\"\"\n        score = 100.0\n        \n        # Penalize based on maintainability index\n        mi = metrics.get('maintainability_index', 50)\n        score -= (100 - mi) * 0.3\n        \n        # Penalize based on smells\n        high_severity = sum(1 for s in smells if s.severity == 'high')\n        medium_severity = sum(1 for s in smells if s.severity == 'medium')\n        low_severity = sum(1 for s in smells if s.severity == 'low')\n        \n        score -= high_severity * 10\n        score -= medium_severity * 5\n        score -= low_severity * 2\n        \n        score = max(0, min(100, score))\n        \n        # Determine bug risk\n        if score >= 70:\n            bug_risk = 'low'\n        elif score >= 40:\n            bug_risk = 'medium'\n        else:\n            bug_risk = 'high'\n        \n        # Generate recommendations\n        recommendations = []\n        \n        if high_severity > 0:\n            recommendations.append(f\"Address {high_severity} high-severity issues immediately\")\n        \n        if mi < 50:\n            recommendations.append(\"Improve maintainability through refactoring\")\n        \n        if metrics.get('avg_function_length', 0) > 50:\n            recommendations.append(\"Break down large functions into smaller units\")\n        \n        return HealthScore(\n            entity_type='file',\n            entity_name='',\n            file_path='',\n            score=score,\n            complexity=0,\n            smell_count=len(smells),\n            bug_risk=bug_risk,\n            churn_rate=0.0,\n            recommendations=recommendations\n        )\n    \n    def _calculate_function_health(\n        self,\n        file_path: str,\n        function: Dict[str, Any],\n        lines: List[str],\n        language: str,\n        all_smells: List[CodeSmell]\n    ) -> HealthScore:\n        \"\"\"Calculate function-level health score.\"\"\"\n        func_name = function.get('name', 'unknown')\n        start_line = function.get('start_line', 1)\n        end_line = function.get('end_line', 1)\n        \n        score = 100.0\n        \n        # Get function content\n        func_lines = lines[start_line-1:end_line]\n        func_content = '\\n'.join(func_lines)\n        \n        # Calculate complexity\n        complexity = self.analyzer._estimate_cyclomatic_complexity(func_content, language)\n        \n        # Penalize complexity\n        if complexity > 10:\n            score -= (complexity - 10) * 5\n        \n        # Count function-specific smells\n        func_smells = [\n            s for s in all_smells\n            if start_line <= s.line <= end_line\n        ]\n        \n        # Penalize smells\n        for smell in func_smells:\n            if smell.severity == 'high':\n                score -= 15\n            elif smell.severity == 'medium':\n                score -= 8\n            else:\n                score -= 3\n        \n        # Penalize length\n        func_length = end_line - start_line + 1\n        if func_length > 50:\n            score -= (func_length - 50) * 0.5\n        \n        score = max(0, min(100, score))\n        \n        # Determine bug risk\n        if score >= 70:\n            bug_risk = 'low'\n        elif score >= 40:\n            bug_risk = 'medium'\n        else:\n            bug_risk = 'high'\n        \n        # Generate recommendations\n        recommendations = []\n        \n        if complexity > 10:\n            recommendations.append(f\"Reduce complexity (current: {complexity})\")\n        \n        if func_length > 50:\n            recommendations.append(f\"Function too long ({func_length} lines), consider splitting\")\n        \n        if len(func_smells) > 0:\n            recommendations.append(f\"Fix {len(func_smells)} code smell(s)\")\n        \n        return HealthScore(\n            entity_type='function',\n            entity_name=func_name,\n            file_path=file_path,\n            score=score,\n            complexity=complexity,\n            smell_count=len(func_smells),\n            bug_risk=bug_risk,\n            churn_rate=0.0,\n            recommendations=recommendations\n        )\n    \n    def identify_bug_hotspots(\n        self,\n        health_results: List[Dict[str, Any]],\n        threshold: float = 50.0\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Identify bug hotspots (low health score functions).\n        \n        Args:\n            health_results: List of health scan results\n            threshold: Health score threshold (below = hotspot)\n        \n        Returns:\n            List of hotspots\n        \"\"\"\n        hotspots = []\n        \n        for result in health_results:\n            file_path = result['file_path']\n            \n            # Check file-level health\n            if result['file_health_score'] < threshold:\n                hotspots.append({\n                    'type': 'file',\n                    'file': file_path,\n                    'score': result['file_health_score'],\n                    'risk': result['file_bug_risk']\n                })\n            \n            # Check function-level health\n            for func_health in result.get('function_health', []):\n                if func_health['score'] < threshold:\n                    hotspots.append({\n                        'type': 'function',\n                        'file': file_path,\n                        'function': func_health['name'],\n                        'score': func_health['score'],\n                        'complexity': func_health['complexity'],\n                        'risk': func_health['bug_risk'],\n                        'recommendations': func_health['recommendations']\n                    })\n        \n        # Sort by score (lowest first)\n        hotspots.sort(key=lambda x: x['score'])\n        \n        return hotspots\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python health_scanner.py <file_path> <language>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    language = sys.argv[2] if len(sys.argv) > 2 else 'python'\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Mock function data\n    functions = [{'name': 'test', 'start_line': 1, 'end_line': 10}]\n    \n    scanner = CodeHealthScanner()\n    result = scanner.scan_file(file_path, content, language, functions)\n    \n    print(f\"\\n\ud83d\udcca Health Report for {file_path}\")\n    print(f\"  File Health Score: {result['file_health_score']:.1f}/100\")\n    print(f\"  Bug Risk: {result['file_bug_risk'].upper()}\")\n    print(f\"  Code Smells: {len(result['smells'])}\")\n    \n    if result['function_health']:\n        print(f\"\\n\ud83d\udce6 Function Health:\")\n        for fh in result['function_health']:\n            print(f\"  {fh['name']}: {fh['score']:.1f}/100 (complexity: {fh['complexity']})\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\intelligence\\import_resolver.py": "\"\"\"\nImport Resolver\n\nMaps import statements to actual file paths and detects circular dependencies.\n\"\"\"\n\nfrom typing import Dict, List, Set, Optional, Tuple\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Import:\n    \"\"\"Represents an import statement.\"\"\"\n    source_file: str\n    module: str\n    items: List[str]\n    is_relative: bool\n    line_number: int\n\n\n@dataclass\nclass CircularDependency:\n    \"\"\"Represents a circular dependency.\"\"\"\n    cycle: List[str]  # List of file paths in the cycle\n    \n    def __str__(self) -> str:\n        return \" -> \".join(self.cycle + [self.cycle[0]])\n\n\nclass ImportResolver:\n    \"\"\"\n    Resolve import statements and detect circular dependencies.\n    \"\"\"\n    \n    def __init__(self, root_path: str):\n        \"\"\"\n        Initialize import resolver.\n        \n        Args:\n            root_path: Root directory of the codebase\n        \"\"\"\n        self.root_path = Path(root_path).resolve()\n        self.import_graph: Dict[str, Set[str]] = {}  # file -> set of imported files\n        self.imports_by_file: Dict[str, List[Import]] = {}\n    \n    def extract_imports(self, file_path: str, content: str, language: str) -> List[Import]:\n        \"\"\"\n        Extract import statements from a file.\n        \n        Args:\n            file_path: Path to file\n            content: File content\n            language: Programming language\n        \n        Returns:\n            List of Import objects\n        \"\"\"\n        if language == 'python':\n            return self._extract_python_imports(file_path, content)\n        elif language in ['javascript', 'typescript']:\n            return self._extract_js_imports(file_path, content)\n        else:\n            return []\n    \n    def _extract_python_imports(self, file_path: str, content: str) -> List[Import]:\n        \"\"\"Extract Python imports.\"\"\"\n        imports: List[Import] = []\n        lines = content.splitlines()\n        \n        for i, line in enumerate(lines, 1):\n            line = line.strip()\n            \n            # Match: import module\n            match = re.match(r'^import\\s+([\\w.]+)', line)\n            if match:\n                imports.append(Import(\n                    source_file=file_path,\n                    module=match.group(1),\n                    items=[],\n                    is_relative=False,\n                    line_number=i\n                ))\n                continue\n            \n            # Match: from module import items\n            match = re.match(r'^from\\s+([\\w.]+)\\s+import\\s+(.+)', line)\n            if match:\n                module = match.group(1)\n                items_str = match.group(2)\n                \n                # Parse items\n                items = [item.strip() for item in items_str.split(',')]\n                \n                imports.append(Import(\n                    source_file=file_path,\n                    module=module,\n                    items=items,\n                    is_relative=module.startswith('.'),\n                    line_number=i\n                ))\n        \n        return imports\n    \n    def _extract_js_imports(self, file_path: str, content: str) -> List[Import]:\n        \"\"\"Extract JavaScript/TypeScript imports.\"\"\"\n        imports: List[Import] = []\n        lines = content.splitlines()\n        \n        for i, line in enumerate(lines, 1):\n            line = line.strip()\n            \n            # Match: import ... from 'module'\n            match = re.match(r'^import\\s+.*from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', line)\n            if match:\n                module = match.group(1)\n                \n                imports.append(Import(\n                    source_file=file_path,\n                    module=module,\n                    items=[],\n                    is_relative=module.startswith('.'),\n                    line_number=i\n                ))\n                continue\n            \n            # Match: import 'module'\n            match = re.match(r'^import\\s+[\\'\"]([^\\'\"]+)[\\'\"]', line)\n            if match:\n                module = match.group(1)\n                \n                imports.append(Import(\n                    source_file=file_path,\n                    module=module,\n                    items=[],\n                    is_relative=module.startswith('.'),\n                    line_number=i\n                ))\n        \n        return imports\n    \n    def resolve_import_path(\n        self,\n        import_stmt: Import,\n        language: str\n    ) -> Optional[str]:\n        \"\"\"\n        Resolve an import to an actual file path.\n        \n        Args:\n            import_stmt: Import statement\n            language: Programming language\n        \n        Returns:\n            Resolved file path or None if not found\n        \"\"\"\n        source_file = Path(import_stmt.source_file)\n        module = import_stmt.module\n        \n        if import_stmt.is_relative:\n            # Relative import\n            return self._resolve_relative_import(source_file, module, language)\n        else:\n            # Absolute import\n            return self._resolve_absolute_import(module, language)\n    \n    def _resolve_relative_import(\n        self,\n        source_file: Path,\n        module: str,\n        language: str\n    ) -> Optional[str]:\n        \"\"\"Resolve relative import.\"\"\"\n        # Count leading dots\n        level = len(module) - len(module.lstrip('.'))\n        module_name = module.lstrip('.')\n        \n        # Go up 'level' directories\n        current_dir = source_file.parent\n        \n        for _ in range(level - 1):\n            current_dir = current_dir.parent\n        \n        # Try different file extensions\n        if language == 'python':\n            extensions = ['.py']\n        else:\n            extensions = ['.ts', '.tsx', '.js', '.jsx']\n        \n        # Try as file\n        for ext in extensions:\n            target = current_dir / f\"{module_name}{ext}\"\n            if target.exists():\n                return str(target)\n        \n        # Try as directory with __init__ or index\n        if language == 'python':\n            target = current_dir / module_name / '__init__.py'\n        else:\n            target = current_dir / module_name / 'index.ts'\n            if not target.exists():\n                target = current_dir / module_name / 'index.js'\n        \n        if target.exists():\n            return str(target)\n        \n        return None\n    \n    def _resolve_absolute_import(\n        self,\n        module: str,\n        language: str\n    ) -> Optional[str]:\n        \"\"\"Resolve absolute import.\"\"\"\n        # Convert module path to file path\n        module_parts = module.split('.')\n        \n        if language == 'python':\n            # Try as file\n            file_path = self.root_path / '/'.join(module_parts) + '.py'\n            if file_path.exists():\n                return str(file_path)\n            \n            # Try as package\n            file_path = self.root_path / '/'.join(module_parts) / '__init__.py'\n            if file_path.exists():\n                return str(file_path)\n        \n        # For JS/TS, would need package.json resolution\n        # Simplified: just check local paths\n        \n        return None\n    \n    def build_dependency_graph(\n        self,\n        files: List[Dict[str, str]]  # List of {path, content, language}\n    ):\n        \"\"\"\n        Build dependency graph from files.\n        \n        Args:\n            files: List of file dictionaries\n        \"\"\"\n        # Extract imports from all files\n        for file_info in files:\n            file_path = file_info['path']\n            content = file_info['content']\n            language = file_info['language']\n            \n            imports = self.extract_imports(file_path, content, language)\n            self.imports_by_file[file_path] = imports\n            \n            # Resolve imports and build graph\n            if file_path not in self.import_graph:\n                self.import_graph[file_path] = set()\n            \n            for imp in imports:\n                resolved = self.resolve_import_path(imp, language)\n                if resolved:\n                    self.import_graph[file_path].add(resolved)\n    \n    def detect_circular_dependencies(self) -> List[CircularDependency]:\n        \"\"\"\n        Detect circular dependencies in the import graph.\n        \n        Returns:\n            List of circular dependencies\n        \"\"\"\n        cycles: List[CircularDependency] = []\n        visited: Set[str] = set()\n        rec_stack: List[str] = []\n        \n        def dfs(node: str) -> bool:\n            \"\"\"DFS to detect cycles.\"\"\"\n            visited.add(node)\n            rec_stack.append(node)\n            \n            for neighbor in self.import_graph.get(node, set()):\n                if neighbor not in visited:\n                    if dfs(neighbor):\n                        return True\n                elif neighbor in rec_stack:\n                    # Found cycle\n                    cycle_start = rec_stack.index(neighbor)\n                    cycle = rec_stack[cycle_start:]\n                    cycles.append(CircularDependency(cycle=cycle.copy()))\n                    return True\n            \n            rec_stack.pop()\n            return False\n        \n        # Run DFS from each node\n        for node in self.import_graph.keys():\n            if node not in visited:\n                dfs(node)\n        \n        return cycles\n    \n    def get_module_dependencies(self, file_path: str) -> Set[str]:\n        \"\"\"Get all modules that a file depends on.\"\"\"\n        return self.import_graph.get(file_path, set())\n    \n    def get_module_dependents(self, file_path: str) -> Set[str]:\n        \"\"\"Get all modules that depend on a file.\"\"\"\n        dependents: Set[str] = set()\n        \n        for source, targets in self.import_graph.items():\n            if file_path in targets:\n                dependents.add(source)\n        \n        return dependents\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python import_resolver.py <file_path> <language>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    language = sys.argv[2] if len(sys.argv) > 2 else 'python'\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    resolver = ImportResolver(str(Path(file_path).parent))\n    imports = resolver.extract_imports(file_path, content, language)\n    \n    print(f\"\\n\ud83d\udce6 Found {len(imports)} imports:\")\n    for imp in imports:\n        rel = \"(relative)\" if imp.is_relative else \"(absolute)\"\n        print(f\"  Line {imp.line_number}: {imp.module} {rel}\")\n        if imp.items:\n            print(f\"    Items: {', '.join(imp.items)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\intelligence\\static_analyzer.py": "\"\"\"\nStatic Code Analyzer\n\nDetects code smells, complexity issues, and quality metrics.\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass\nimport re\nfrom pathlib import Path\n\n\n@dataclass\nclass CodeSmell:\n    \"\"\"Represents a detected code smell.\"\"\"\n    file: str\n    line: int\n    smell_type: str\n    severity: str  # 'low', 'medium', 'high'\n    message: str\n    suggestion: Optional[str] = None\n\n\nclass StaticAnalyzer:\n    \"\"\"\n    Analyze code for quality issues and complexity metrics.\n    \"\"\"\n    \n    # Thresholds\n    MAX_FUNCTION_LINES = 50\n    MAX_LINE_LENGTH = 120\n    MAX_PARAMETERS = 5\n    MAX_COMPLEXITY = 10\n    \n    def __init__(self):\n        \"\"\"Initialize static analyzer.\"\"\"\n        pass\n    \n    def analyze_file(\n        self,\n        file_path: str,\n        content: str,\n        language: str,\n        functions: List[Dict[str, Any]]\n    ) -> List[CodeSmell]:\n        \"\"\"\n        Analyze a file for code smells.\n        \n        Args:\n            file_path: Path to file\n            content: File content\n            language: Programming language\n            functions: List of function metadata\n        \n        Returns:\n            List of detected code smells\n        \"\"\"\n        smells: List[CodeSmell] = []\n        lines = content.splitlines()\n        \n        # Analyze functions\n        for func in functions:\n            func_smells = self._analyze_function(file_path, func, lines, language)\n            smells.extend(func_smells)\n        \n        # Analyze overall file\n        file_smells = self._analyze_file_level(file_path, lines, language)\n        smells.extend(file_smells)\n        \n        return smells\n    \n    def _analyze_function(\n        self,\n        file_path: str,\n        function: Dict[str, Any],\n        lines: List[str],\n        language: str\n    ) -> List[CodeSmell]:\n        \"\"\"Analyze a single function.\"\"\"\n        smells: List[CodeSmell] = []\n        \n        func_name = function.get('name', 'unknown')\n        start_line = function.get('start_line', 1)\n        end_line = function.get('end_line', 1)\n        \n        # Calculate function length\n        func_length = end_line - start_line + 1\n        \n        if func_length > self.MAX_FUNCTION_LINES:\n            smells.append(CodeSmell(\n                file=file_path,\n                line=start_line,\n                smell_type='long_function',\n                severity='medium',\n                message=f\"Function '{func_name}' is {func_length} lines (max: {self.MAX_FUNCTION_LINES})\",\n                suggestion=\"Consider breaking into smaller functions\"\n            ))\n        \n        # Get function content\n        func_lines = lines[start_line-1:end_line]\n        func_content = '\\n'.join(func_lines)\n        \n        # Check for complexity indicators\n        complexity = self._estimate_cyclomatic_complexity(func_content, language)\n        \n        if complexity > self.MAX_COMPLEXITY:\n            smells.append(CodeSmell(\n                file=file_path,\n                line=start_line,\n                smell_type='high_complexity',\n                severity='high',\n                message=f\"Function '{func_name}' has complexity {complexity} (max: {self.MAX_COMPLEXITY})\",\n                suggestion=\"Simplify logic or extract helper functions\"\n            ))\n        \n        # Check parameter count\n        param_count = self._count_parameters(func_lines[0] if func_lines else '', language)\n        \n        if param_count > self.MAX_PARAMETERS:\n            smells.append(CodeSmell(\n                file=file_path,\n                line=start_line,\n                smell_type='too_many_parameters',\n                severity='medium',\n                message=f\"Function '{func_name}' has {param_count} parameters (max: {self.MAX_PARAMETERS})\",\n                suggestion=\"Consider using parameter objects or configuration\"\n            ))\n        \n        # Check for code duplication patterns\n        if self._has_code_duplication(func_content):\n            smells.append(CodeSmell(\n                file=file_path,\n                line=start_line,\n                smell_type='code_duplication',\n                severity='low',\n                message=f\"Function '{func_name}' may contain duplicated code\",\n                suggestion=\"Extract common patterns into helper functions\"\n            ))\n        \n        return smells\n    \n    def _analyze_file_level(\n        self,\n        file_path: str,\n        lines: List[str],\n        language: str\n    ) -> List[CodeSmell]:\n        \"\"\"Analyze file-level issues.\"\"\"\n        smells: List[CodeSmell] = []\n        \n        # Check line lengths\n        for i, line in enumerate(lines, 1):\n            if len(line) > self.MAX_LINE_LENGTH:\n                smells.append(CodeSmell(\n                    file=file_path,\n                    line=i,\n                    smell_type='long_line',\n                    severity='low',\n                    message=f\"Line exceeds {self.MAX_LINE_LENGTH} characters ({len(line)})\",\n                    suggestion=\"Break into multiple lines\"\n                ))\n        \n        # Check for commented-out code\n        commented_lines = self._find_commented_code(lines, language)\n        \n        if len(commented_lines) > 10:\n            smells.append(CodeSmell(\n                file=file_path,\n                line=commented_lines[0],\n                smell_type='commented_code',\n                severity='low',\n                message=f\"File contains {len(commented_lines)} lines of commented code\",\n                suggestion=\"Remove dead code or use version control\"\n            ))\n        \n        # Check for TODO/FIXME\n        todos = self._find_todos(lines)\n        \n        for line_num in todos:\n            smells.append(CodeSmell(\n                file=file_path,\n                line=line_num,\n                smell_type='todo',\n                severity='low',\n                message=\"TODO/FIXME comment found\",\n                suggestion=\"Track in issue tracker instead\"\n            ))\n        \n        return smells\n    \n    def _estimate_cyclomatic_complexity(self, content: str, language: str) -> int:\n        \"\"\"\n        Estimate cyclomatic complexity (rough approximation).\n        \n        Real complexity = 1 + number of decision points\n        \"\"\"\n        complexity = 1\n        \n        # Count decision points\n        decision_keywords = [\n            r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n            r'\\bfor\\b', r'\\bwhile\\b',\n            r'\\bcase\\b', r'\\bswitch\\b',\n            r'\\band\\b', r'\\bor\\b',\n            r'\\?', r'\\&\\&', r'\\|\\|'\n        ]\n        \n        for keyword in decision_keywords:\n            complexity += len(re.findall(keyword, content, re.IGNORECASE))\n        \n        return complexity\n    \n    def _count_parameters(self, signature: str, language: str) -> int:\n        \"\"\"Count function parameters from signature.\"\"\"\n        # Simple approach: count commas in parameter list\n        # Extract content between parentheses\n        match = re.search(r'\\(([^)]*)\\)', signature)\n        \n        if not match:\n            return 0\n        \n        params = match.group(1).strip()\n        \n        if not params:\n            return 0\n        \n        # Count commas + 1\n        return params.count(',') + 1\n    \n    def _has_code_duplication(self, content: str) -> bool:\n        \"\"\"Detect potential code duplication (simplified).\"\"\"\n        lines = [l.strip() for l in content.splitlines() if l.strip() and not l.strip().startswith('#')]\n        \n        if len(lines) < 10:\n            return False\n        \n        # Look for repeated line patterns\n        line_counts: Dict[str, int] = {}\n        \n        for line in lines:\n            if len(line) > 10:  # Ignore short lines\n                line_counts[line] = line_counts.get(line, 0) + 1\n        \n        # If same line appears 3+ times, likely duplication\n        for count in line_counts.values():\n            if count >= 3:\n                return True\n        \n        return False\n    \n    def _find_commented_code(self, lines: List[str], language: str) -> List[int]:\n        \"\"\"Find lines that appear to be commented-out code.\"\"\"\n        commented = []\n        \n        comment_prefix = '#' if language == 'python' else '//'\n        \n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n            \n            if stripped.startswith(comment_prefix):\n                # Check if it looks like code (has operators, keywords, etc.)\n                uncommented = stripped[len(comment_prefix):].strip()\n                \n                if self._looks_like_code(uncommented, language):\n                    commented.append(i)\n        \n        return commented\n    \n    def _looks_like_code(self, text: str, language: str) -> bool:\n        \"\"\"Check if text looks like commented-out code.\"\"\"\n        code_indicators = ['=', '(', ')', '{', '}', '[', ']', ';', 'def ', 'class ', 'import ', 'const ', 'let ', 'var ']\n        \n        for indicator in code_indicators:\n            if indicator in text:\n                return True\n        \n        return False\n    \n    def _find_todos(self, lines: List[str]) -> List[int]:\n        \"\"\"Find TODO/FIXME comments.\"\"\"\n        todos = []\n        \n        for i, line in enumerate(lines, 1):\n            if re.search(r'\\b(TODO|FIXME|HACK|XXX)\\b', line, re.IGNORECASE):\n                todos.append(i)\n        \n        return todos\n    \n    def calculate_metrics(self, functions: List[Dict[str, Any]], content: str) -> Dict[str, Any]:\n        \"\"\"\n        Calculate code quality metrics.\n        \n        Returns:\n            Dictionary of metrics\n        \"\"\"\n        lines = content.splitlines()\n        code_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]\n        \n        return {\n            'total_lines': len(lines),\n            'code_lines': len(code_lines),\n            'comment_lines': len(lines) - len(code_lines),\n            'function_count': len(functions),\n            'avg_function_length': sum(f['end_line'] - f['start_line'] + 1 for f in functions) / len(functions) if functions else 0,\n            'maintainability_index': self._calculate_maintainability_index(content, functions)\n        }\n    \n    def _calculate_maintainability_index(\n        self,\n        content: str,\n        functions: List[Dict[str, Any]]\n    ) -> float:\n        \"\"\"\n        Calculate maintainability index (simplified).\n        \n        Real MI formula is complex; this is a simplified approximation.\n        Score: 0-100 (higher is better)\n        \"\"\"\n        lines = content.splitlines()\n        code_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]\n        \n        # Volume (lines of code)\n        volume = len(code_lines)\n        \n        # Complexity (average across functions)\n        avg_complexity = sum(\n            self._estimate_cyclomatic_complexity('\\n'.join(lines[f['start_line']-1:f['end_line']]), 'python')\n            for f in functions\n        ) / len(functions) if functions else 1\n        \n        # Simple formula: 100 - penalties\n        score = 100\n        score -= min(volume / 10, 30)  # Penalize large files\n        score -= min(avg_complexity * 3, 40)  # Penalize complexity\n        \n        return max(0, score)\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python static_analyzer.py <file_path> <language>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    language = sys.argv[2] if len(sys.argv) > 2 else 'python'\n    \n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Mock function data for demo\n    functions = [{'name': 'test', 'start_line': 1, 'end_line': 10}]\n    \n    analyzer = StaticAnalyzer()\n    smells = analyzer.analyze_file(file_path, content, language, functions)\n    \n    print(f\"\\n\ud83d\udd0d Found {len(smells)} code smells:\")\n    for smell in smells:\n        print(f\"\\n  [{smell.severity.upper()}] {smell.smell_type}\")\n        print(f\"  Line {smell.line}: {smell.message}\")\n        if smell.suggestion:\n            print(f\"  \ud83d\udca1 {smell.suggestion}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\intelligence\\symbol_tracer.py": "\"\"\"\nSymbol Lineage Tracer\n\nTraces symbols (variables, functions, classes) across files to understand:\n- Where a symbol is defined\n- Where it's imported\n- Where it's used\n- How it's renamed/refactored\n\"\"\"\n\nfrom typing import Dict, List, Any, Set, Optional\nfrom collections import defaultdict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass SymbolLineageTracer:\n    \"\"\"\n    Trace symbol usage across codebase.\n    \n    Builds a lineage graph showing:\n    - Definition location\n    - Import chain\n    - All usage locations\n    - Type evolution\n    \"\"\"\n    \n    def __init__(self, neo4j_client=None):\n        \"\"\"\n        Initialize tracer.\n        \n        Args:\n            neo4j_client: Neo4j client for graph queries\n        \"\"\"\n        self.neo4j_client = neo4j_client\n        self.symbol_cache: Dict[str, Dict[str, Any]] = {}\n    \n    def trace_symbol(self, symbol_name: str, context_file: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Trace a symbol's lineage.\n        \n        Args:\n            symbol_name: Name of symbol to trace\n            context_file: File where symbol is being referenced\n        \n        Returns:\n            Symbol lineage data\n        \"\"\"\n        logger.info(f\"Tracing symbol: {symbol_name}\")\n        \n        lineage = {\n            'symbol': symbol_name,\n            'definitions': [],\n            'imports': [],\n            'usages': [],\n            'type_info': {},\n            'lineage_chain': [],\n        }\n        \n        # Find all definitions\n        definitions = self._find_definitions(symbol_name)\n        lineage['definitions'] = definitions\n        \n        # Find imports\n        imports = self._find_imports(symbol_name, context_file)\n        lineage['imports'] = imports\n        \n        # Find all usages\n        usages = self._find_usages(symbol_name)\n        lineage['usages'] = usages\n        \n        # Build lineage chain\n        lineage['lineage_chain'] = self._build_lineage_chain(\n            symbol_name,\n            definitions,\n            imports\n        )\n        \n        return lineage\n    \n    def _find_definitions(self, symbol_name: str) -> List[Dict[str, Any]]:\n        \"\"\"Find all places where symbol is defined.\"\"\"\n        definitions = []\n        \n        if self.neo4j_client:\n            # Query Neo4j for function/class nodes\n            query = \"\"\"\n            MATCH (s)\n            WHERE s.name = $symbol_name\n            AND (s:Function OR s:Class)\n            RETURN s.file as file, s.line as line, \n                   s.type as type, s.signature as signature\n            \"\"\"\n            \n            results = self.neo4j_client.execute_query(\n                query,\n                {'symbol_name': symbol_name}\n            )\n            \n            for record in results:\n                definitions.append({\n                    'file': record['file'],\n                    'line': record['line'],\n                    'type': record['type'],\n                    'signature': record.get('signature', ''),\n                })\n        \n        return definitions\n    \n    def _find_imports(\n        self,\n        symbol_name: str,\n        context_file: Optional[str] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find all import statements for this symbol.\"\"\"\n        imports = []\n        \n        if self.neo4j_client:\n            query = \"\"\"\n            MATCH (f:File)-[imp:IMPORTS]->(target)\n            WHERE target.name = $symbol_name\n            \"\"\"\n            \n            if context_file:\n                query += \" AND f.path = $context_file\"\n            \n            query += \"\"\"\n            RETURN f.path as file, imp.line as line,\n                   imp.import_type as import_type,\n                   imp.alias as alias\n            \"\"\"\n            \n            params = {'symbol_name': symbol_name}\n            if context_file:\n                params['context_file'] = context_file\n            \n            results = self.neo4j_client.execute_query(query, params)\n            \n            for record in results:\n                imports.append({\n                    'file': record['file'],\n                    'line': record['line'],\n                    'import_type': record['import_type'],\n                    'alias': record.get('alias'),\n                })\n        \n        return imports\n    \n    def _find_usages(self, symbol_name: str) -> List[Dict[str, Any]]:\n        \"\"\"Find all places where symbol is used.\"\"\"\n        usages = []\n        \n        # This would typically involve:\n        # 1. Full-text search in code\n        # 2. AST analysis to find references\n        # 3. Graph traversal for call sites\n        \n        if self.neo4j_client:\n            # Find functions that call this symbol\n            query = \"\"\"\n            MATCH (caller:Function)-[:CALLS]->(callee)\n            WHERE callee.name = $symbol_name\n            RETURN caller.file as file, caller.name as caller_name,\n                   caller.line as line\n            \"\"\"\n            \n            results = self.neo4j_client.execute_query(\n                query,\n                {'symbol_name': symbol_name}\n            )\n            \n            for record in results:\n                usages.append({\n                    'file': record['file'],\n                    'line': record['line'],\n                    'caller': record['caller_name'],\n                    'usage_type': 'function_call',\n                })\n        \n        return usages\n    \n    def _build_lineage_chain(\n        self,\n        symbol_name: str,\n        definitions: List[Dict[str, Any]],\n        imports: List[Dict[str, Any]]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Build the lineage chain from definition to usage.\n        \n        Returns chain: definition -> imports -> usages\n        \"\"\"\n        chain = []\n        \n        # Start with definition\n        if definitions:\n            primary_def = definitions[0]\n            chain.append({\n                'step': 'definition',\n                'file': primary_def['file'],\n                'line': primary_def['line'],\n                'description': f\"Defined in {primary_def['file']}\",\n            })\n        \n        # Add import steps\n        for imp in imports:\n            chain.append({\n                'step': 'import',\n                'file': imp['file'],\n                'line': imp['line'],\n                'description': f\"Imported in {imp['file']}\" + \n                              (f\" as {imp['alias']}\" if imp.get('alias') else \"\"),\n            })\n        \n        return chain\n    \n    def find_rename_candidates(\n        self,\n        old_name: str,\n        new_name: str\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Find all locations that would need to change for a rename.\n        \n        Args:\n            old_name: Current symbol name\n            new_name: Proposed new name\n        \n        Returns:\n            List of locations to update\n        \"\"\"\n        lineage = self.trace_symbol(old_name)\n        \n        candidates = []\n        \n        # Add definition locations\n        for defn in lineage['definitions']:\n            candidates.append({\n                'file': defn['file'],\n                'line': defn['line'],\n                'change_type': 'definition',\n                'old': old_name,\n                'new': new_name,\n            })\n        \n        # Add import locations\n        for imp in lineage['imports']:\n            candidates.append({\n                'file': imp['file'],\n                'line': imp['line'],\n                'change_type': 'import',\n                'old': old_name,\n                'new': new_name,\n            })\n        \n        # Add usage locations\n        for usage in lineage['usages']:\n            candidates.append({\n                'file': usage['file'],\n                'line': usage['line'],\n                'change_type': 'usage',\n                'old': old_name,\n                'new': new_name,\n            })\n        \n        return candidates\n    \n    def analyze_symbol_impact(self, symbol_name: str) -> Dict[str, Any]:\n        \"\"\"\n        Analyze the impact of changing/removing a symbol.\n        \n        Returns:\n            Impact analysis\n        \"\"\"\n        lineage = self.trace_symbol(symbol_name)\n        \n        return {\n            'symbol': symbol_name,\n            'total_usages': len(lineage['usages']),\n            'files_affected': len(set(u['file'] for u in lineage['usages'])),\n            'import_count': len(lineage['imports']),\n            'definition_count': len(lineage['definitions']),\n            'impact_score': self._calculate_impact_score(lineage),\n            'high_risk': len(lineage['usages']) > 10,\n        }\n    \n    def _calculate_impact_score(self, lineage: Dict[str, Any]) -> float:\n        \"\"\"Calculate impact score (0-100).\"\"\"\n        # More usages = higher impact\n        usage_score = min(len(lineage['usages']) * 10, 60)\n        \n        # More files = higher impact\n        files = set(u['file'] for u in lineage['usages'])\n        file_score = min(len(files) * 5, 30)\n        \n        # Imports add impact\n        import_score = min(len(lineage['imports']) * 2, 10)\n        \n        return usage_score + file_score + import_score\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    tracer = SymbolLineageTracer()\n    \n    print(\"\\n\ud83d\udd0d Symbol Lineage Tracer\")\n    print(\"=\" * 50)\n    \n    # Example trace\n    symbol = \"UserAuth\"\n    lineage = tracer.trace_symbol(symbol)\n    \n    print(f\"\\nTracing symbol: {symbol}\")\n    print(f\"  Definitions: {len(lineage['definitions'])}\")\n    print(f\"  Imports: {len(lineage['imports'])}\")\n    print(f\"  Usages: {len(lineage['usages'])}\")\n    \n    # Example impact analysis\n    impact = tracer.analyze_symbol_impact(symbol)\n    print(f\"\\n\ud83d\udcca Impact Analysis:\")\n    print(f\"  Total usages: {impact['total_usages']}\")\n    print(f\"  Files affected: {impact['files_affected']}\")\n    print(f\"  Impact score: {impact['impact_score']:.1f}/100\")\n    print(f\"  High risk: {'Yes' if impact['high_risk'] else 'No'}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\intelligence\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\memory\\error_memory.py": "\"\"\"\nError Memory Database\n\nStores error snapshots and debugging history.\n\"\"\"\n\nimport sqlite3\nfrom typing import Dict, List, Any, Optional\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ErrorMemoryDB:\n    \"\"\"\n    SQLite database for error tracking and debugging history.\n    \"\"\"\n    \n    def __init__(self, db_path: str):\n        \"\"\"\n        Initialize error memory database.\n        \n        Args:\n            db_path: Path to SQLite database\n        \"\"\"\n        self.db_path = Path(db_path)\n        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        self.conn = sqlite3.connect(str(self.db_path))\n        self.conn.row_factory = sqlite3.Row\n        \n        self._create_tables()\n    \n    def _create_tables(self):\n        \"\"\"Create database tables.\"\"\"\n        cursor = self.conn.cursor()\n        \n        # Error snapshots table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS error_snapshots (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                error_hash TEXT NOT NULL UNIQUE,\n                error_type TEXT NOT NULL,\n                error_message TEXT NOT NULL,\n                file_path TEXT,\n                line_number INTEGER,\n                function_name TEXT,\n                stack_trace TEXT,\n                context_code TEXT,\n                first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                occurrence_count INTEGER DEFAULT 1,\n                resolved BOOLEAN DEFAULT 0\n            )\n        ''')\n        \n        # Error resolutions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS error_resolutions (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                error_hash TEXT NOT NULL,\n                resolution_type TEXT NOT NULL,\n                patch_applied TEXT,\n                llm_model TEXT,\n                success BOOLEAN NOT NULL,\n                resolution_time_ms INTEGER,\n                resolved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                notes TEXT,\n                FOREIGN KEY (error_hash) REFERENCES error_snapshots(error_hash)\n            )\n        ''')\n        \n        # Debugging sessions table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS debug_sessions (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL UNIQUE,\n                error_hash TEXT,\n                query TEXT NOT NULL,\n                retrieved_chunks TEXT,\n                reasoning_tier TEXT,\n                llm_calls INTEGER DEFAULT 0,\n                total_tokens INTEGER DEFAULT 0,\n                success BOOLEAN,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                completed_at TIMESTAMP,\n                FOREIGN KEY (error_hash) REFERENCES error_snapshots(error_hash)\n            )\n        ''')\n        \n        # Conversation history table\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS conversation_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT NOT NULL,\n                role TEXT NOT NULL,\n                content TEXT NOT NULL,\n                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                FOREIGN KEY (session_id) REFERENCES debug_sessions(session_id)\n            )\n        ''')\n        \n        # Create indexes\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_hash ON error_snapshots(error_hash)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_file ON error_snapshots(file_path)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_session ON debug_sessions(session_id)')\n        cursor.execute('CREATE INDEX IF NOT EXISTS idx_conversation_session ON conversation_history(session_id)')\n        \n        self.conn.commit()\n    \n    def save_error_snapshot(self, error_data: Dict[str, Any]) -> str:\n        \"\"\"\n        Save or update error snapshot.\n        \n        Args:\n            error_data: Error information\n        \n        Returns:\n            Error hash\n        \"\"\"\n        cursor = self.conn.cursor()\n        \n        error_hash = error_data['error_hash']\n        \n        # Check if error exists\n        cursor.execute('SELECT id, occurrence_count FROM error_snapshots WHERE error_hash = ?', (error_hash,))\n        existing = cursor.fetchone()\n        \n        if existing:\n            # Update existing\n            cursor.execute('''\n                UPDATE error_snapshots\n                SET last_seen = CURRENT_TIMESTAMP,\n                    occurrence_count = occurrence_count + 1\n                WHERE error_hash = ?\n            ''', (error_hash,))\n        else:\n            # Insert new\n            cursor.execute('''\n                INSERT INTO error_snapshots\n                (error_hash, error_type, error_message, file_path, line_number, \n                 function_name, stack_trace, context_code)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                error_hash,\n                error_data.get('error_type'),\n                error_data.get('error_message'),\n                error_data.get('file_path'),\n                error_data.get('line_number'),\n                error_data.get('function_name'),\n                error_data.get('stack_trace'),\n                error_data.get('context_code')\n            ))\n        \n        self.conn.commit()\n        logger.info(f\"Saved error snapshot: {error_hash}\")\n        \n        return error_hash\n    \n    def save_error_resolution(self, resolution_data: Dict[str, Any]):\n        \"\"\"Save error resolution attempt.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            INSERT INTO error_resolutions\n            (error_hash, resolution_type, patch_applied, llm_model, success, resolution_time_ms, notes)\n            VALUES (?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            resolution_data['error_hash'],\n            resolution_data.get('resolution_type', 'patch'),\n            resolution_data.get('patch_applied'),\n            resolution_data.get('llm_model'),\n            resolution_data['success'],\n            resolution_data.get('resolution_time_ms'),\n            resolution_data.get('notes')\n        ))\n        \n        # Mark error as resolved if successful\n        if resolution_data['success']:\n            cursor.execute('''\n                UPDATE error_snapshots\n                SET resolved = 1\n                WHERE error_hash = ?\n            ''', (resolution_data['error_hash'],))\n        \n        self.conn.commit()\n    \n    def create_debug_session(self, session_id: str, query: str, error_hash: Optional[str] = None) -> str:\n        \"\"\"Create a new debugging session.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            INSERT INTO debug_sessions (session_id, error_hash, query)\n            VALUES (?, ?, ?)\n        ''', (session_id, error_hash, query))\n        \n        self.conn.commit()\n        \n        return session_id\n    \n    def update_debug_session(self, session_id: str, updates: Dict[str, Any]):\n        \"\"\"Update debugging session.\"\"\"\n        cursor = self.conn.cursor()\n        \n        set_clauses = []\n        values = []\n        \n        for key, value in updates.items():\n            if key == 'retrieved_chunks' and isinstance(value, (list, dict)):\n                value = json.dumps(value)\n            \n            set_clauses.append(f\"{key} = ?\")\n            values.append(value)\n        \n        values.append(session_id)\n        \n        query = f\"UPDATE debug_sessions SET {', '.join(set_clauses)} WHERE session_id = ?\"\n        cursor.execute(query, values)\n        \n        self.conn.commit()\n    \n    def save_conversation_turn(self, session_id: str, role: str, content: str):\n        \"\"\"Save a conversation turn.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            INSERT INTO conversation_history (session_id, role, content)\n            VALUES (?, ?, ?)\n        ''', (session_id, role, content))\n        \n        self.conn.commit()\n    \n    def get_conversation_history(self, session_id: str, limit: int = 50) -> List[Dict[str, Any]]:\n        \"\"\"Get conversation history for a session.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            SELECT role, content, timestamp\n            FROM conversation_history\n            WHERE session_id = ?\n            ORDER BY timestamp ASC\n            LIMIT ?\n        ''', (session_id, limit))\n        \n        return [dict(row) for row in cursor.fetchall()]\n    \n    def get_similar_errors(self, error_type: str, limit: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Find similar errors that were resolved.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('''\n            SELECT e.*, r.resolution_type, r.patch_applied\n            FROM error_snapshots e\n            JOIN error_resolutions r ON e.error_hash = r.error_hash\n            WHERE e.error_type = ? AND e.resolved = 1 AND r.success = 1\n            ORDER BY e.last_seen DESC\n            LIMIT ?\n        ''', (error_type, limit))\n        \n        return [dict(row) for row in cursor.fetchall()]\n    \n    def get_error_stats(self) -> Dict[str, Any]:\n        \"\"\"Get error statistics.\"\"\"\n        cursor = self.conn.cursor()\n        \n        cursor.execute('SELECT COUNT(*) as total FROM error_snapshots')\n        total = cursor.fetchone()['total']\n        \n        cursor.execute('SELECT COUNT(*) as resolved FROM error_snapshots WHERE resolved = 1')\n        resolved = cursor.fetchone()['resolved']\n        \n        cursor.execute('SELECT error_type, COUNT(*) as count FROM error_snapshots GROUP BY error_type ORDER BY count DESC LIMIT 5')\n        top_types = [dict(row) for row in cursor.fetchall()]\n        \n        return {\n            'total_errors': total,\n            'resolved_errors': resolved,\n            'unresolved_errors': total - resolved,\n            'top_error_types': top_types\n        }\n    \n    def close(self):\n        \"\"\"Close database connection.\"\"\"\n        if self.conn:\n            self.conn.close()\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    from ..app.config import get_settings\n    \n    settings = get_settings()\n    \n    with ErrorMemoryDB(settings.memory_db_path) as db:\n        # Test error snapshot\n        error_data = {\n            'error_hash': 'test_error_123',\n            'error_type': 'TypeError',\n            'error_message': 'Cannot read property of undefined',\n            'file_path': 'test.py',\n            'line_number': 42,\n            'function_name': 'process_data'\n        }\n        \n        db.save_error_snapshot(error_data)\n        \n        # Get stats\n        stats = db.get_error_stats()\n        \n        print(\"\\n\ud83d\udcca Error Memory Stats:\")\n        for key, value in stats.items():\n            print(f\"  {key}: {value}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\memory\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\patching\\diff_generator.py": "\"\"\"\nDiff Generator\n\nGenerates unified diffs for code patches.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport difflib\nfrom pathlib import Path\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DiffGenerator:\n    \"\"\"\n    Generate unified diff patches for code changes.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize diff generator.\"\"\"\n        pass\n    \n    def generate_diff(\n        self,\n        original_content: str,\n        modified_content: str,\n        file_path: str\n    ) -> str:\n        \"\"\"\n        Generate unified diff.\n        \n        Args:\n            original_content: Original file content\n            modified_content: Modified file content\n            file_path: File path for context\n        \n        Returns:\n            Unified diff string\n        \"\"\"\n        original_lines = original_content.splitlines(keepends=True)\n        modified_lines = modified_content.splitlines(keepends=True)\n        \n        diff = difflib.unified_diff(\n            original_lines,\n            modified_lines,\n            fromfile=f\"a/{file_path}\",\n            tofile=f\"b/{file_path}\",\n            lineterm=''\n        )\n        \n        return ''.join(diff)\n    \n    def generate_patch(\n        self,\n        changes: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate a complete patch for multiple file changes.\n        \n        Args:\n            changes: List of change dicts with file_path, original, modified\n        \n        Returns:\n            Patch dictionary with diffs and metadata\n        \"\"\"\n        patches = []\n        \n        for change in changes:\n            file_path = change['file_path']\n            original = change.get('original_content', '')\n            modified = change.get('modified_content', '')\n            \n            diff = self.generate_diff(original, modified, file_path)\n            \n            if diff:\n                patches.append({\n                    'file': file_path,\n                    'diff': diff,\n                    'additions': self._count_additions(diff),\n                    'deletions': self._count_deletions(diff)\n                })\n        \n        return {\n            'patches': patches,\n            'total_files': len(patches),\n            'total_additions': sum(p['additions'] for p in patches),\n            'total_deletions': sum(p['deletions'] for p in patches)\n        }\n    \n    def _count_additions(self, diff: str) -> int:\n        \"\"\"Count added lines in diff.\"\"\"\n        return sum(1 for line in diff.splitlines() if line.startswith('+') and not line.startswith('+++'))\n    \n    def _count_deletions(self, diff: str) -> int:\n        \"\"\"Count deleted lines in diff.\"\"\"\n        return sum(1 for line in diff.splitlines() if line.startswith('-') and not line.startswith('---'))\n    \n    def apply_patch(\n        self,\n        file_path: str,\n        patch_content: str,\n        dry_run: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Apply a patch to a file.\n        \n        Args:\n            file_path: Target file path\n            patch_content: Patch content\n            dry_run: If True, don't actually modify file\n        \n        Returns:\n            Result dictionary\n        \"\"\"\n        try:\n            # Read original file\n            with open(file_path, 'r', encoding='utf-8') as f:\n                original_content = f.read()\n            \n            # Parse patch and apply\n            # This is simplified - production would use proper patch library\n            modified_content = self._apply_unified_diff(original_content, patch_content)\n            \n            if not dry_run:\n                # Write modified content\n                with open(file_path, 'w', encoding='utf-8') as f:\n                    f.write(modified_content)\n                \n                logger.info(f\"Applied patch to {file_path}\")\n            \n            return {\n                'success': True,\n                'file': file_path,\n                'dry_run': dry_run\n            }\n        \n        except Exception as e:\n            logger.error(f\"Failed to apply patch: {e}\")\n            \n            return {\n                'success': False,\n                'file': file_path,\n                'error': str(e)\n            }\n    \n    def _apply_unified_diff(self, original: str, diff: str) -> str:\n        \"\"\"Apply unified diff to content (simplified).\"\"\"\n        # In production, use a proper patch library\n        # This is a placeholder\n        return original\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    # Test diff generation\n    original = \"\"\"def hello():\n    print(\"Hello\")\n    return True\n\"\"\"\n    \n    modified = \"\"\"def hello(name):\n    print(f\"Hello {name}\")\n    return True\n\"\"\"\n    \n    generator = DiffGenerator()\n    diff = generator.generate_diff(original, modified, \"test.py\")\n    \n    print(\"\\n\ud83d\udcdd Generated Diff:\\n\")\n    print(diff)\n    \n    patch = generator.generate_patch([\n        {\n            'file_path': 'test.py',\n            'original_content': original,\n            'modified_content': modified\n        }\n    ])\n    \n    print(f\"\\n\ud83d\udcca Patch Stats:\")\n    print(f\"  Files: {patch['total_files']}\")\n    print(f\"  Additions: +{patch['total_additions']}\")\n    print(f\"  Deletions: -{patch['total_deletions']}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\patching\\multi_file_planner.py": "\"\"\"\nMulti-file Patch Planner\n\nPlans atomic multi-file changes considering dependencies and conflicts.\n\"\"\"\n\nfrom typing import List, Dict, Any, Set, Optional, Tuple\nimport logging\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass FileChange:\n    \"\"\"Represents a change to a single file.\"\"\"\n    file_path: str\n    original_content: str\n    patched_content: str\n    change_type: str  # 'modify', 'create', 'delete'\n    dependencies: List[str]  # Other files this change depends on\n\n\n@dataclass\nclass MultiFilePatch:\n    \"\"\"Represents a multi-file patch with dependencies.\"\"\"\n    patch_id: str\n    description: str\n    changes: List[FileChange]\n    dependency_order: List[str]  # Order to apply changes\n    conflicts: List[Dict[str, Any]]\n    impact_analysis: Dict[str, Any]\n\n\nclass MultiFilePatchPlanner:\n    \"\"\"\n    Plan atomic multi-file changes.\n    \n    Features:\n    - Dependency resolution\n    - Conflict detection\n    - Impact prediction\n    - Atomic application (all or nothing)\n    \"\"\"\n    \n    def __init__(self, neo4j_client=None, symbol_tracer=None):\n        \"\"\"\n        Initialize planner.\n        \n        Args:\n            neo4j_client: Neo4j client for dependency queries\n            symbol_tracer: Symbol lineage tracer\n        \"\"\"\n        self.neo4j_client = neo4j_client\n        self.symbol_tracer = symbol_tracer\n    \n    def create_multi_file_patch(\n        self,\n        changes: List[Dict[str, Any]],\n        description: str = \"\"\n    ) -> MultiFilePatch:\n        \"\"\"\n        Create a multi-file patch plan.\n        \n        Args:\n            changes: List of file changes\n            description: Patch description\n        \n        Returns:\n            Multi-file patch plan\n        \"\"\"\n        logger.info(f\"Planning multi-file patch with {len(changes)} changes\")\n        \n        # Convert to FileChange objects\n        file_changes = [\n            FileChange(\n                file_path=c['file_path'],\n                original_content=c.get('original_content', ''),\n                patched_content=c['patched_content'],\n                change_type=c.get('change_type', 'modify'),\n                dependencies=c.get('dependencies', [])\n            )\n            for c in changes\n        ]\n        \n        # Resolve dependency order\n        dependency_order = self._resolve_dependencies(file_changes)\n        \n        # Detect conflicts\n        conflicts = self._detect_conflicts(file_changes)\n        \n        # Analyze impact\n        impact = self._analyze_impact(file_changes)\n        \n        patch = MultiFilePatch(\n            patch_id=f\"mfp_{hash(description)}\",\n            description=description,\n            changes=file_changes,\n            dependency_order=dependency_order,\n            conflicts=conflicts,\n            impact_analysis=impact\n        )\n        \n        return patch\n    \n    def _resolve_dependencies(\n        self,\n        changes: List[FileChange]\n    ) -> List[str]:\n        \"\"\"\n        Resolve dependency order for applying changes.\n        \n        Uses topological sort.\n        \"\"\"\n        # Build dependency graph\n        graph: Dict[str, Set[str]] = {}\n        in_degree: Dict[str, int] = {}\n        \n        for change in changes:\n            graph[change.file_path] = set(change.dependencies)\n            in_degree[change.file_path] = 0\n        \n        # Calculate in-degrees\n        for file_path in graph:\n            for dep in graph[file_path]:\n                if dep in in_degree:\n                    in_degree[dep] += 1\n        \n        # Topological sort (Kahn's algorithm)\n        queue = [f for f in graph if in_degree[f] == 0]\n        order = []\n        \n        while queue:\n            current = queue.pop(0)\n            order.append(current)\n            \n            for neighbor in graph.get(current, set()):\n                if neighbor in in_degree:\n                    in_degree[neighbor] -= 1\n                    if in_degree[neighbor] == 0:\n                        queue.append(neighbor)\n        \n        # Check for cycles\n        if len(order) != len(changes):\n            logger.warning(\"Circular dependencies detected in patch\")\n        \n        return order\n    \n    def _detect_conflicts(\n        self,\n        changes: List[FileChange]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Detect conflicts between changes.\n        \n        Conflicts occur when:\n        - Multiple changes to same file/lines\n        - Symbol renames that clash\n        - Incompatible type changes\n        \"\"\"\n        conflicts = []\n        \n        # Group changes by file\n        by_file: Dict[str, List[FileChange]] = {}\n        for change in changes:\n            if change.file_path not in by_file:\n                by_file[change.file_path] = []\n            by_file[change.file_path].append(change)\n        \n        # Check for multiple changes to same file\n        for file_path, file_changes in by_file.items():\n            if len(file_changes) > 1:\n                conflicts.append({\n                    'type': 'multiple_changes',\n                    'file': file_path,\n                    'count': len(file_changes),\n                    'severity': 'high',\n                    'message': f\"Multiple changes to {file_path}\",\n                })\n        \n        # Check for symbol conflicts using symbol tracer\n        if self.symbol_tracer:\n            # Would check for rename conflicts, etc.\n            pass\n        \n        return conflicts\n    \n    def _analyze_impact(\n        self,\n        changes: List[FileChange]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Analyze impact of multi-file patch.\n        \n        Returns:\n            Impact analysis data\n        \"\"\"\n        impact = {\n            'total_files': len(changes),\n            'total_additions': 0,\n            'total_deletions': 0,\n            'files_modified': 0,\n            'files_created': 0,\n            'files_deleted': 0,\n            'risk_score': 0.0,\n            'affected_modules': set(),\n        }\n        \n        for change in changes:\n            if change.change_type == 'modify':\n                impact['files_modified'] += 1\n            elif change.change_type == 'create':\n                impact['files_created'] += 1\n            elif change.change_type == 'delete':\n                impact['files_deleted'] += 1\n            \n            # Calculate line changes\n            original_lines = change.original_content.count('\\n')\n            patched_lines = change.patched_content.count('\\n')\n            \n            if patched_lines > original_lines:\n                impact['total_additions'] += patched_lines - original_lines\n            else:\n                impact['total_deletions'] += original_lines - patched_lines\n            \n            # Track modules\n            module = str(Path(change.file_path).parent)\n            impact['affected_modules'].add(module)\n        \n        # Calculate risk score (0-100)\n        base_risk = len(changes) * 10  # More files = more risk\n        conflict_risk = len(self._detect_conflicts(changes)) * 20\n        impact['risk_score'] = min(100, base_risk + conflict_risk)\n        \n        # Convert set to list for JSON serialization\n        impact['affected_modules'] = list(impact['affected_modules'])\n        \n        return impact\n    \n    def validate_patch(self, patch: MultiFilePatch) -> Dict[str, Any]:\n        \"\"\"\n        Validate multi-file patch before application.\n        \n        Returns:\n            Validation result\n        \"\"\"\n        result = {\n            'valid': True,\n            'errors': [],\n            'warnings': [],\n            'can_apply': True,\n        }\n        \n        # Check for conflicts\n        if patch.conflicts:\n            result['warnings'].append(\n                f\"Found {len(patch.conflicts)} conflicts\"\n            )\n            \n            high_severity = [c for c in patch.conflicts if c['severity'] == 'high']\n            if high_severity:\n                result['valid'] = False\n                result['can_apply'] = False\n                result['errors'].append(\n                    f\"{len(high_severity)} high-severity conflicts must be resolved\"\n                )\n        \n        # Check dependency order\n        if not patch.dependency_order:\n            result['errors'].append(\"Could not resolve dependency order\")\n            result['valid'] = False\n        \n        # Check risk score\n        if patch.impact_analysis['risk_score'] > 80:\n            result['warnings'].append(\n                f\"High risk score: {patch.impact_analysis['risk_score']}\"\n            )\n        \n        return result\n    \n    def generate_preview(self, patch: MultiFilePatch) -> str:\n        \"\"\"\n        Generate human-readable preview of multi-file patch.\n        \n        Returns:\n            Preview text\n        \"\"\"\n        lines = []\n        lines.append(f\"=== Multi-File Patch: {patch.description} ===\\n\")\n        lines.append(f\"Patch ID: {patch.patch_id}\")\n        lines.append(f\"Total files: {patch.impact_analysis['total_files']}\")\n        lines.append(f\"Risk score: {patch.impact_analysis['risk_score']}/100\\n\")\n        \n        lines.append(\"Changes:\")\n        for idx, change in enumerate(patch.changes, 1):\n            lines.append(f\"  {idx}. {change.change_type.upper()}: {change.file_path}\")\n            if change.dependencies:\n                lines.append(f\"     Dependencies: {', '.join(change.dependencies)}\")\n        \n        if patch.conflicts:\n            lines.append(f\"\\n\u26a0\ufe0f  {len(patch.conflicts)} conflicts detected:\")\n            for conflict in patch.conflicts:\n                lines.append(f\"  - {conflict['message']}\")\n        \n        lines.append(f\"\\nApplication order:\")\n        for idx, file_path in enumerate(patch.dependency_order, 1):\n            lines.append(f\"  {idx}. {file_path}\")\n        \n        return '\\n'.join(lines)\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    planner = MultiFilePatchPlanner()\n    \n    # Example multi-file patch\n    changes = [\n        {\n            'file_path': 'models/user.py',\n            'patched_content': '# Updated user model',\n            'change_type': 'modify',\n            'dependencies': [],\n        },\n        {\n            'file_path': 'api/users.py',\n            'patched_content': '# Updated API',\n            'change_type': 'modify',\n            'dependencies': ['models/user.py'],\n        },\n        {\n            'file_path': 'tests/test_users.py',\n            'patched_content': '# Updated tests',\n            'change_type': 'modify',\n            'dependencies': ['models/user.py', 'api/users.py'],\n        },\n    ]\n    \n    patch = planner.create_multi_file_patch(\n        changes,\n        description=\"Update user model and API\"\n    )\n    \n    print(planner.generate_preview(patch))\n    \n    validation = planner.validate_patch(patch)\n    print(f\"\\nValidation: {'\u2705 PASS' if validation['valid'] else '\u274c FAIL'}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\patching\\validator.py": "\"\"\"\nPatch Validator\n\nValidates code patches before applying them:\n- Syntax checking\n- Type checking (mypy, tsc)\n- Test execution\n- Rollback support\n\"\"\"\n\nimport subprocess\nimport tempfile\nimport os\nfrom typing import Dict, Any, List, Optional\nfrom pathlib import Path\nimport logging\nimport shutil\n\nlogger = logging.getLogger(__name__)\n\n\nclass PatchValidator:\n    \"\"\"\n    Validate code patches in isolated environment.\n    \n    Validation steps:\n    1. Apply patch to temporary copy\n    2. Run syntax checks\n    3. Run type checkers\n    4. Run tests (if available)\n    5. Report results\n    \"\"\"\n    \n    def __init__(self, use_docker: bool = False):\n        \"\"\"\n        Initialize validator.\n        \n        Args:\n            use_docker: Use Docker for isolation (recommended for production)\n        \"\"\"\n        self.use_docker = use_docker\n    \n    def validate_patch(\n        self,\n        file_path: str,\n        original_content: str,\n        patched_content: str,\n        language: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Validate a patch.\n        \n        Args:\n            file_path: Original file path\n            original_content: Original file content\n            patched_content: Patched file content\n            language: Programming language\n        \n        Returns:\n            Validation result\n        \"\"\"\n        logger.info(f\"Validating patch for {file_path}\")\n        \n        results = {\n            'valid': True,\n            'checks': {},\n            'errors': [],\n            'warnings': [],\n        }\n        \n        try:\n            # Create temporary directory\n            with tempfile.TemporaryDirectory() as tmpdir:\n                tmp_file = Path(tmpdir) / Path(file_path).name\n                \n                # Write patched content\n                tmp_file.write_text(patched_content, encoding='utf-8')\n                \n                # Run syntax check\n                syntax_result = self._check_syntax(str(tmp_file), language)\n                results['checks']['syntax'] = syntax_result\n                \n                if not syntax_result['passed']:\n                    results['valid'] = False\n                    results['errors'].extend(syntax_result.get('errors', []))\n                \n                # Run type checker\n                if language in ['python', 'typescript']:\n                    type_result = self._check_types(str(tmp_file), language)\n                    results['checks']['types'] = type_result\n                    \n                    if not type_result['passed']:\n                        # Type errors are warnings, not failures\n                        results['warnings'].extend(type_result.get('errors', []))\n                \n                # Run linter\n                lint_result = self._run_linter(str(tmp_file), language)\n                results['checks']['lint'] = lint_result\n                \n                if not lint_result['passed']:\n                    results['warnings'].extend(lint_result.get('warnings', []))\n        \n        except Exception as e:\n            logger.error(f\"Validation error: {e}\")\n            results['valid'] = False\n            results['errors'].append(str(e))\n        \n        return results\n    \n    def _check_syntax(self, file_path: str, language: str) -> Dict[str, Any]:\n        \"\"\"Check syntax validity.\"\"\"\n        result = {'passed': True, 'errors': []}\n        \n        try:\n            if language == 'python':\n                # Use Python's compile to check syntax\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    code = f.read()\n                \n                try:\n                    compile(code, file_path, 'exec')\n                except SyntaxError as e:\n                    result['passed'] = False\n                    result['errors'].append(f\"Syntax error at line {e.lineno}: {e.msg}\")\n            \n            elif language in ['typescript', 'javascript']:\n                # Use tsc or node to check syntax\n                cmd = ['node', '--check', file_path]\n                proc = subprocess.run(\n                    cmd,\n                    capture_output=True,\n                    text=True,\n                    timeout=10\n                )\n                \n                if proc.returncode != 0:\n                    result['passed'] = False\n                    result['errors'].append(proc.stderr)\n        \n        except Exception as e:\n            result['passed'] = False\n            result['errors'].append(str(e))\n        \n        return result\n    \n    def _check_types(self, file_path: str, language: str) -> Dict[str, Any]:\n        \"\"\"Run type checker.\"\"\"\n        result = {'passed': True, 'errors': []}\n        \n        try:\n            if language == 'python':\n                # Run mypy\n                cmd = ['mypy', '--no-error-summary', file_path]\n                proc = subprocess.run(\n                    cmd,\n                    capture_output=True,\n                    text=True,\n                    timeout=30\n                )\n                \n                if proc.returncode != 0:\n                    result['passed'] = False\n                    result['errors'] = proc.stdout.split('\\n')\n            \n            elif language == 'typescript':\n                # Run tsc\n                cmd = ['tsc', '--noEmit', file_path]\n                proc = subprocess.run(\n                    cmd,\n                    capture_output=True,\n                    text=True,\n                    timeout=30\n                )\n                \n                if proc.returncode != 0:\n                    result['passed'] = False\n                    result['errors'] = proc.stdout.split('\\n')\n        \n        except FileNotFoundError:\n            # Type checker not installed\n            result['passed'] = True\n            result['errors'] = ['Type checker not available']\n        except Exception as e:\n            result['passed'] = False\n            result['errors'].append(str(e))\n        \n        return result\n    \n    def _run_linter(self, file_path: str, language: str) -> Dict[str, Any]:\n        \"\"\"Run linter.\"\"\"\n        result = {'passed': True, 'warnings': []}\n        \n        try:\n            if language == 'python':\n                # Run flake8\n                cmd = ['flake8', '--select=E,W', file_path]\n                proc = subprocess.run(\n                    cmd,\n                    capture_output=True,\n                    text=True,\n                    timeout=30\n                )\n                \n                if proc.returncode != 0:\n                    result['passed'] = False\n                    result['warnings'] = proc.stdout.split('\\n')\n            \n            elif language in ['typescript', 'javascript']:\n                # Run eslint\n                cmd = ['eslint', file_path]\n                proc = subprocess.run(\n                    cmd,\n                    capture_output=True,\n                    text=True,\n                    timeout=30\n                )\n                \n                if proc.returncode != 0:\n                    result['passed'] = False\n                    result['warnings'] = proc.stdout.split('\\n')\n        \n        except FileNotFoundError:\n            # Linter not installed\n            result['passed'] = True\n        except Exception as e:\n            result['warnings'].append(str(e))\n        \n        return result\n    \n    def validate_in_docker(\n        self,\n        file_path: str,\n        patched_content: str,\n        language: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Validate patch in Docker container (safer isolation).\n        \n        This is more secure for untrusted patches.\n        \"\"\"\n        # TODO: Implement Docker-based validation\n        # Would create container, mount code, run checks, cleanup\n        \n        logger.warning(\"Docker validation not yet implemented, using direct validation\")\n        \n        return self.validate_patch(file_path, \"\", patched_content, language)\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    validator = PatchValidator()\n    \n    # Example validation\n    original = \"def hello():\\n    print('Hello')\\n\"\n    patched = \"def hello():\\n    print('Hello World')\\n\"\n    \n    result = validator.validate_patch(\n        \"test.py\",\n        original,\n        patched,\n        \"python\"\n    )\n    \n    print(f\"\\n\u2705 Validation Result:\")\n    print(f\"  Valid: {result['valid']}\")\n    print(f\"  Checks: {list(result['checks'].keys())}\")\n    \n    if result['errors']:\n        print(f\"\\n\u274c Errors:\")\n        for error in result['errors']:\n            print(f\"  - {error}\")\n    \n    if result['warnings']:\n        print(f\"\\n\u26a0\ufe0f  Warnings:\")\n        for warning in result['warnings']:\n            print(f\"  - {warning}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\patching\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\reasoning\\llm_client.py": "\"\"\"\nLLM Client\n\nHandles all interactions with Google Gemini API.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Generator\nimport logging\nimport time\n\ntry:\n    import google.generativeai as genai\n    HAS_GENAI = True\nexcept ImportError:\n    HAS_GENAI = False\n    print(\"Warning: google-generativeai not installed\")\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMClient:\n    \"\"\"\n    Client for Google Gemini API with rate limiting and error handling.\n    \"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model_name: str = \"gemini-2.0-flash-exp\",\n        fallback_model: str = \"gemini-1.5-pro\",\n        max_output_tokens: int = 8192\n    ):\n        \"\"\"\n        Initialize LLM client.\n        \n        Args:\n            api_key: Gemini API key\n            model_name: Primary model name\n            fallback_model: Fallback model for tier escalation\n            max_output_tokens: Maximum tokens in response\n        \"\"\"\n        if not HAS_GENAI:\n            raise RuntimeError(\"google-generativeai not installed\")\n        \n        genai.configure(api_key=api_key)\n        \n        self.model_name = model_name\n        self.fallback_model = fallback_model\n        self.max_output_tokens = max_output_tokens\n        \n        # Initialize models\n        self.model = genai.GenerativeModel(model_name)\n        self.fallback = genai.GenerativeModel(fallback_model)\n        \n        # Rate limiting\n        self.last_request_time = 0\n        self.min_request_interval = 0.1  # 100ms between requests\n        \n        logger.info(f\"Initialized LLM client with model: {model_name}\")\n    \n    def _rate_limit(self):\n        \"\"\"Enforce rate limiting between requests.\"\"\"\n        elapsed = time.time() - self.last_request_time\n        \n        if elapsed < self.min_request_interval:\n            time.sleep(self.min_request_interval - elapsed)\n        \n        self.last_request_time = time.time()\n    \n    def generate(\n        self,\n        prompt: str,\n        system_instruction: Optional[str] = None,\n        temperature: float = 0.7,\n        use_fallback: bool = False,\n        max_retries: int = 3\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate a response from the LLM.\n        \n        Args:\n            prompt: User prompt\n            system_instruction: Optional system instruction\n            temperature: Sampling temperature\n            use_fallback: Whether to use fallback model\n            max_retries: Maximum retry attempts\n        \n        Returns:\n            Response dictionary with text and metadata\n        \"\"\"\n        self._rate_limit()\n        \n        model = self.fallback if use_fallback else self.model\n        model_name = self.fallback_model if use_fallback else self.model_name\n        \n        generation_config = {\n            'temperature': temperature,\n            'max_output_tokens': self.max_output_tokens,\n        }\n        \n        # Build messages\n        messages = []\n        \n        if system_instruction:\n            messages.append({\n                'role': 'system',\n                'parts': [system_instruction]\n            })\n        \n        messages.append({\n            'role': 'user',\n            'parts': [prompt]\n        })\n        \n        # Try generation with retries\n        last_error = None\n        \n        for attempt in range(max_retries):\n            try:\n                logger.info(f\"Generating with {model_name} (attempt {attempt + 1}/{max_retries})\")\n                \n                start_time = time.time()\n                \n                response = model.generate_content(\n                    prompt,\n                    generation_config=generation_config\n                )\n                \n                elapsed = time.time() - start_time\n                \n                # Extract response\n                text = response.text\n                \n                # Get token counts if available\n                try:\n                    usage = {\n                        'prompt_tokens': response.usage_metadata.prompt_token_count,\n                        'completion_tokens': response.usage_metadata.candidates_token_count,\n                        'total_tokens': response.usage_metadata.total_token_count\n                    }\n                except AttributeError:\n                    usage = {'total_tokens': 0}\n                \n                logger.info(f\"Generated {usage.get('completion_tokens', 0)} tokens in {elapsed:.2f}s\")\n                \n                return {\n                    'text': text,\n                    'model': model_name,\n                    'usage': usage,\n                    'elapsed_ms': int(elapsed * 1000),\n                    'success': True\n                }\n            \n            except Exception as e:\n                last_error = e\n                logger.warning(f\"Generation attempt {attempt + 1} failed: {e}\")\n                \n                if attempt < max_retries - 1:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n        \n        # All retries failed\n        logger.error(f\"Generation failed after {max_retries} attempts: {last_error}\")\n        \n        return {\n            'text': '',\n            'model': model_name,\n            'usage': {'total_tokens': 0},\n            'elapsed_ms': 0,\n            'success': False,\n            'error': str(last_error)\n        }\n    \n    def generate_streaming(\n        self,\n        prompt: str,\n        system_instruction: Optional[str] = None,\n        temperature: float = 0.7\n    ) -> Generator[str, None, None]:\n        \"\"\"\n        Generate a streaming response.\n        \n        Args:\n            prompt: User prompt\n            system_instruction: Optional system instruction\n            temperature: Sampling temperature\n        \n        Yields:\n            Chunks of generated text\n        \"\"\"\n        self._rate_limit()\n        \n        generation_config = {\n            'temperature': temperature,\n            'max_output_tokens': self.max_output_tokens,\n        }\n        \n        try:\n            response = self.model.generate_content(\n                prompt,\n                generation_config=generation_config,\n                stream=True\n            )\n            \n            for chunk in response:\n                if chunk.text:\n                    yield chunk.text\n        \n        except Exception as e:\n            logger.error(f\"Streaming generation failed: {e}\")\n            yield f\"Error: {str(e)}\"\n    \n    def count_tokens(self, text: str) -> int:\n        \"\"\"\n        Count tokens in text.\n        \n        Args:\n            text: Text to count\n        \n        Returns:\n            Token count\n        \"\"\"\n        try:\n            result = self.model.count_tokens(text)\n            return result.total_tokens\n        except Exception as e:\n            logger.warning(f\"Token counting failed: {e}\")\n            # Fallback: rough approximation (4 chars per token)\n            return len(text) // 4\n    \n    def chat(\n        self,\n        messages: List[Dict[str, str]],\n        temperature: float = 0.7\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Multi-turn chat conversation.\n        \n        Args:\n            messages: List of message dicts with 'role' and 'content'\n            temperature: Sampling temperature\n        \n        Returns:\n            Response dictionary\n        \"\"\"\n        # Convert messages to Gemini format\n        history = []\n        \n        for msg in messages[:-1]:  # All but last\n            role = 'model' if msg['role'] == 'assistant' else 'user'\n            history.append({\n                'role': role,\n                'parts': [msg['content']]\n            })\n        \n        # Start chat with history\n        chat = self.model.start_chat(history=history)\n        \n        # Send last message\n        last_msg = messages[-1]\n        \n        try:\n            response = chat.send_message(\n                last_msg['content'],\n                generation_config={'temperature': temperature}\n            )\n            \n            return {\n                'text': response.text,\n                'model': self.model_name,\n                'success': True\n            }\n        \n        except Exception as e:\n            logger.error(f\"Chat failed: {e}\")\n            \n            return {\n                'text': '',\n                'model': self.model_name,\n                'success': False,\n                'error': str(e)\n            }\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    from ..app.config import get_settings\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python llm_client.py '<prompt>'\")\n        sys.exit(1)\n    \n    prompt = sys.argv[1]\n    \n    settings = get_settings()\n    \n    client = LLMClient(\n        api_key=settings.gemini_api_key,\n        model_name=settings.llm_model_type\n    )\n    \n    print(f\"\\n\ud83e\udd16 Generating response with {client.model_name}...\")\n    \n    response = client.generate(prompt)\n    \n    if response['success']:\n        print(f\"\\n\u2705 Response:\\n{response['text']}\\n\")\n        print(f\"\ud83d\udcca Usage: {response['usage']}\")\n        print(f\"\u23f1\ufe0f  Time: {response['elapsed_ms']}ms\")\n    else:\n        print(f\"\\n\u274c Generation failed: {response.get('error')}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\reasoning\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "src\\retrieval\\context_packer.py": "\"\"\"\nContext Packer\n\nPacks retrieved chunks into LLM context with smart truncation.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextPacker:\n    \"\"\"\n    Pack code chunks into LLM context within token limits.\n    \"\"\"\n    \n    def __init__(\n        self,\n        max_tokens: int = 70000,\n        system_reserved: int = 3000,\n        output_reserved: int = 8192\n    ):\n        \"\"\"\n        Initialize context packer.\n        \n        Args:\n            max_tokens: Maximum total tokens\n            system_reserved: Tokens reserved for system prompt\n            output_reserved: Tokens reserved for output\n        \"\"\"\n        self.max_tokens = max_tokens\n        self.system_reserved = system_reserved\n        self.output_reserved = output_reserved\n        \n        self.available_tokens = max_tokens - system_reserved - output_reserved\n        \n        logger.info(f\"Context packer initialized: {self.available_tokens} tokens available\")\n    \n    def pack(\n        self,\n        chunks: List[Dict[str, Any]],\n        query: str,\n        prioritize_by_score: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Pack chunks into context.\n        \n        Args:\n            chunks: Retrieved chunks with metadata\n            query: User query\n            prioritize_by_score: Whether to prioritize by relevance score\n        \n        Returns:\n            Packed context dictionary\n        \"\"\"\n        # Sort chunks by score if requested\n        if prioritize_by_score:\n            chunks = sorted(chunks, key=lambda x: x.get('hybrid_score', x.get('score', 0)), reverse=True)\n        \n        packed_chunks = []\n        total_tokens = 0\n        \n        # Reserve tokens for query\n        query_tokens = self._estimate_tokens(query)\n        total_tokens += query_tokens\n        \n        # Pack chunks until we hit limit\n        for chunk in chunks:\n            content = chunk.get('content', '')\n            chunk_tokens = chunk.get('token_count', self._estimate_tokens(content))\n            \n            if total_tokens + chunk_tokens > self.available_tokens:\n                # Try to fit truncated version\n                remaining = self.available_tokens - total_tokens\n                \n                if remaining > 50:  # Only if we have reasonable space\n                    truncated = self._truncate_to_tokens(content, remaining)\n                    packed_chunks.append({\n                        **chunk,\n                        'content': truncated,\n                        'truncated': True,\n                        'original_tokens': chunk_tokens,\n                        'packed_tokens': remaining\n                    })\n                    total_tokens += remaining\n                \n                break\n            \n            packed_chunks.append({\n                **chunk,\n                'truncated': False,\n                'packed_tokens': chunk_tokens\n            })\n            \n            total_tokens += chunk_tokens\n        \n        logger.info(f\"Packed {len(packed_chunks)}/{len(chunks)} chunks ({total_tokens} tokens)\")\n        \n        return {\n            'chunks': packed_chunks,\n            'total_chunks': len(packed_chunks),\n            'total_tokens': total_tokens,\n            'truncated_count': sum(1 for c in packed_chunks if c.get('truncated', False))\n        }\n    \n    def build_prompt(\n        self,\n        packed_context: Dict[str, Any],\n        query: str,\n        system_instruction: Optional[str] = None\n    ) -> str:\n        \"\"\"\n        Build final prompt with packed context.\n        \n        Args:\n            packed_context: Packed context from pack()\n            query: User query\n            system_instruction: Optional system instruction\n        \n        Returns:\n            Complete prompt string\n        \"\"\"\n        prompt_parts = []\n        \n        # System instruction\n        if system_instruction:\n            prompt_parts.append(system_instruction)\n            prompt_parts.append(\"\\n\\n\")\n        \n        # Context\n        prompt_parts.append(\"# Relevant Code Context\\n\\n\")\n        \n        for i, chunk in enumerate(packed_context['chunks'], 1):\n            file_path = chunk.get('file_path', 'unknown')\n            start_line = chunk.get('start_line', 0)\n            end_line = chunk.get('end_line', 0)\n            content = chunk.get('content', '')\n            \n            prompt_parts.append(f\"## Chunk {i}: {file_path} (lines {start_line}-{end_line})\\n\\n\")\n            prompt_parts.append(f\"```{chunk.get('language', '')}\\\\n{content}\\\\n```\\n\\n\")\n        \n        # Query\n        prompt_parts.append(f\"# User Query\\n\\n{query}\\n\\n\")\n        \n        # Instructions\n        prompt_parts.append(\"# Instructions\\n\\n\")\n        prompt_parts.append(\"Based on the code context above, please provide a detailed response to the user's query.\\n\")\n        \n        return ''.join(prompt_parts)\n    \n    def _estimate_tokens(self, text: str) -> int:\n        \"\"\"Estimate token count (rough approximation).\"\"\"\n        # Rough: 1 token \u2248 4 characters for code\n        return len(text) // 4\n    \n    def _truncate_to_tokens(self, text: str, max_tokens: int) -> str:\n        \"\"\"Truncate text to approximately max_tokens.\"\"\"\n        # Rough: keep max_tokens * 4 characters\n        max_chars = max_tokens * 4\n        \n        if len(text) <= max_chars:\n            return text\n        \n        return text[:max_chars] + \"\\n... [truncated]\"\n    \n    def pack_with_health(\n        self,\n        chunks: List[Dict[str, Any]],\n        query: str,\n        health_scores: Dict[str, float]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Pack chunks considering code health scores.\n        \n        Higher health = higher priority for inclusion.\n        \"\"\"\n        # Boost scores based on health\n        for chunk in chunks:\n            file_path = chunk.get('file_path', '')\n            health = health_scores.get(file_path, 50.0) / 100.0\n            \n            base_score = chunk.get('hybrid_score', chunk.get('score', 0))\n            chunk['adjusted_score'] = base_score * (0.8 + 0.2 * health)\n        \n        # Sort by adjusted score\n        chunks_sorted = sorted(chunks, key=lambda x: x.get('adjusted_score', 0), reverse=True)\n        \n        return self.pack(chunks_sorted, query, prioritize_by_score=False)\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    # Test data\n    chunks = [\n        {\n            'file_path': 'test.py',\n            'start_line': 1,\n            'end_line': 20,\n            'content': 'def test():\\n    pass\\n' * 10,\n            'language': 'python',\n            'score': 0.9\n        },\n        {\n            'file_path': 'test2.py',\n            'start_line': 30,\n            'end_line': 50,\n            'content': 'class Test:\\n    pass\\n' * 10,\n            'language': 'python',\n            'score': 0.8\n        }\n    ]\n    \n    packer = ContextPacker(max_tokens=1000)\n    \n    packed = packer.pack(chunks, \"What does this code do?\")\n    \n    print(f\"\\n\ud83d\udce6 Context Packing Results:\")\n    print(f\"  Total chunks: {packed['total_chunks']}\")\n    print(f\"  Total tokens: {packed['total_tokens']}\")\n    print(f\"  Truncated: {packed['truncated_count']}\")\n    \n    prompt = packer.build_prompt(packed, \"What does this code do?\")\n    \n    print(f\"\\n\ud83d\udcdd Generated Prompt ({len(prompt)} chars):\")\n    print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\retrieval\\error_path_retrieval.py": "\"\"\"\nAdvanced Error-Path Retrieval\n\nTraces error propagation through call graph using weighted traversal.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional, Set, Tuple\nimport logging\nfrom collections import deque\n\nlogger = logging.getLogger(__name__)\n\n\nclass ErrorPathRetriever:\n    \"\"\"\n    Advanced error-path retrieval using graph traversal.\n    \n    Finds execution paths that might lead to an error by:\n    1. Starting from error location\n    2. Traversing call graph backward (who calls this?)\n    3. Weighting paths by relevance\n    4. Ranking by likelihood\n    \"\"\"\n    \n    def __init__(self, neo4j_client, semantic_search):\n        \"\"\"\n        Initialize error-path retriever.\n        \n        Args:\n            neo4j_client: Neo4j client for graph queries\n            semantic_search: Semantic search for code retrieval\n        \"\"\"\n        self.neo4j_client = neo4j_client\n        self.semantic_search = semantic_search\n    \n    def find_error_paths(\n        self,\n        error_function: str,\n        error_file: str,\n        max_depth: int = 3,\n        max_paths: int = 5\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Find possible execution paths leading to error.\n        \n        Args:\n            error_function: Function where error occurred\n            error_file: File containing error\n            max_depth: Maximum traversal depth\n            max_paths: Maximum paths to return\n        \n        Returns:\n            List of execution paths\n        \"\"\"\n        logger.info(f\"Finding error paths for {error_function} in {error_file}\")\n        \n        paths = []\n        \n        # Start from error location\n        start_node = {\n            'function': error_function,\n            'file': error_file,\n            'depth': 0,\n        }\n        \n        # BFS traversal backward through call graph\n        paths = self._backward_traversal(start_node, max_depth)\n        \n        # Weight and rank paths\n        ranked_paths = self._rank_paths(paths)\n        \n        return ranked_paths[:max_paths]\n    \n    def _backward_traversal(\n        self,\n        start_node: Dict[str, Any],\n        max_depth: int\n    ) -> List[List[Dict[str, Any]]]:\n        \"\"\"\n        Traverse backward through call graph (BFS).\n        \n        Returns all paths from root callers to error location.\n        \"\"\"\n        all_paths = []\n        queue = deque([[start_node]])\n        visited = set()\n        \n        while queue:\n            current_path = queue.popleft()\n            current_node = current_path[-1]\n            \n            # Check depth\n            if current_node['depth'] >= max_depth:\n                all_paths.append(current_path)\n                continue\n            \n            # Get callers of current function\n            callers = self._get_callers(\n                current_node['function'],\n                current_node['file']\n            )\n            \n            if not callers:\n                # No more callers - this is a root\n                all_paths.append(current_path)\n                continue\n            \n            # Expand path with each caller\n            for caller in callers:\n                caller_key = f\"{caller['function']}:{caller['file']}\"\n                \n                # Avoid cycles\n                if caller_key in visited:\n                    continue\n                \n                visited.add(caller_key)\n                \n                new_path = current_path + [{\n                    'function': caller['function'],\n                    'file': caller['file'],\n                    'depth': current_node['depth'] + 1,\n                    'call_type': caller.get('call_type', 'direct'),\n                }]\n                \n                queue.append(new_path)\n        \n        return all_paths\n    \n    def _get_callers(self, function_name: str, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all functions that call this function.\"\"\"\n        if not self.neo4j_client:\n            return []\n        \n        query = \"\"\"\n        MATCH (caller:Function)-[c:CALLS]->(callee:Function)\n        WHERE callee.name = $function_name \n        AND callee.file = $file_path\n        RETURN caller.name as function, caller.file as file,\n               c.call_type as call_type\n        \"\"\"\n        \n        results = self.neo4j_client.execute_query(query, {\n            'function_name': function_name,\n            'file_path': file_path,\n        })\n        \n        return [\n            {\n                'function': r['function'],\n                'file': r['file'],\n                'call_type': r.get('call_type', 'direct'),\n            }\n            for r in results\n        ]\n    \n    def _rank_paths(self, paths: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rank paths by likelihood of causing error.\n        \n        Scoring factors:\n        - Path length (shorter = more direct)\n        - Call types (direct > indirect)\n        - Code health (unhealthy code = more likely)\n        - Semantic similarity to error\n        \"\"\"\n        ranked = []\n        \n        for path in paths:\n            score = self._calculate_path_score(path)\n            \n            ranked.append({\n                'path': path,\n                'score': score,\n                'length': len(path),\n                'root_function': path[-1]['function'] if path else None,\n                'root_file': path[-1]['file'] if path else None,\n            })\n        \n        # Sort by score descending\n        ranked.sort(key=lambda x: x['score'], reverse=True)\n        \n        return ranked\n    \n    def _calculate_path_score(self, path: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate relevance score for a path.\"\"\"\n        if not path:\n            return 0.0\n        \n        # Base score\n        score = 100.0\n        \n        # Penalty for length (prefer shorter paths)\n        length_penalty = len(path) * 5\n        score -= length_penalty\n        \n        # Bonus for direct calls\n        direct_calls = sum(1 for node in path if node.get('call_type') == 'direct')\n        score += direct_calls * 10\n        \n        # Normalize to 0-100\n        score = max(0, min(100, score))\n        \n        return score\n    \n    def get_error_propagation_context(\n        self,\n        paths: List[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Get code context for error propagation paths.\n        \n        Retrieves relevant code snippets for each function in paths.\n        \"\"\"\n        context = {\n            'paths': paths,\n            'code_snippets': {},\n            'total_functions': 0,\n        }\n        \n        # Collect unique functions\n        functions = set()\n        for path_data in paths:\n            for node in path_data['path']:\n                func_key = f\"{node['function']}:{node['file']}\"\n                functions.add((node['function'], node['file']))\n        \n        context['total_functions'] = len(functions)\n        \n        # Retrieve code for each function\n        for func_name, file_path in functions:\n            # This would use semantic search or direct file reading\n            context['code_snippets'][f\"{func_name}:{file_path}\"] = {\n                'function': func_name,\n                'file': file_path,\n                'code': '# Code would be retrieved here',\n            }\n        \n        return context\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    retriever = ErrorPathRetriever(None, None)\n    \n    print(\"\\n\ud83d\udd0d Error-Path Retrieval Example\")\n    print(\"=\" * 50)\n    \n    paths = retriever.find_error_paths(\n        error_function=\"process_payment\",\n        error_file=\"payment.py\",\n        max_depth=3\n    )\n    \n    print(f\"\\nFound {len(paths)} execution paths\")\n    \n    for idx, path_data in enumerate(paths, 1):\n        print(f\"\\nPath {idx} (score: {path_data['score']:.1f}):\")\n        for node in reversed(path_data['path']):\n            print(f\"  \u2192 {node['function']} ({node['file']})\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\retrieval\\ranking.py": "\"\"\"\nHybrid Ranking\n\nCombines semantic similarity, graph relevance, code health, and recency.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\n\nclass HybridRanker:\n    \"\"\"\n    Rank code chunks using multiple signals.\n    \n    Ranking formula:\n    score = 0.60 * semantic_similarity\n          + 0.25 * graph_relevance\n          + 0.10 * code_health_score\n          + 0.05 * recency\n    \"\"\"\n    \n    # Weights for different signals\n    SEMANTIC_WEIGHT = 0.60\n    GRAPH_WEIGHT = 0.25\n    HEALTH_WEIGHT = 0.10\n    RECENCY_WEIGHT = 0.05\n    \n    def __init__(\n        self,\n        use_graph: bool = True,\n        use_health: bool = True,\n        use_recency: bool = True\n    ):\n        \"\"\"\n        Initialize hybrid ranker.\n        \n        Args:\n            use_graph: Whether to use graph relevance\n            use_health: Whether to use code health scores\n            use_recency: Whether to use recency scores\n        \"\"\"\n        self.use_graph = use_graph\n        self.use_health = use_health\n        self.use_recency = use_recency\n    \n    def rank(\n        self,\n        results: List[Dict[str, Any]],\n        graph_scores: Optional[Dict[str, float]] = None,\n        health_scores: Optional[Dict[str, float]] = None,\n        normalize: bool = True\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rank results using hybrid scoring.\n        \n        Args:\n            results: List of search results with semantic scores\n            graph_scores: Optional dict of chunk_id -> graph relevance score\n            health_scores: Optional dict of file_path -> health score\n            normalize: Whether to normalize final scores to 0-1\n        \n        Returns:\n            Ranked list of results with hybrid scores\n        \"\"\"\n        if not results:\n            return []\n        \n        # Calculate hybrid scores\n        for result in results:\n            chunk_id = result.get('chunk_id', '')\n            file_path = result.get('file_path', '')\n            \n            # Semantic score (already provided)\n            semantic_score = result.get('score', 0.0)\n            \n            # Graph relevance score\n            graph_score = 0.0\n            if self.use_graph and graph_scores:\n                graph_score = graph_scores.get(chunk_id, 0.0)\n            \n            # Code health score\n            health_score = 0.0\n            if self.use_health and health_scores:\n                health_score = health_scores.get(file_path, 0.5)  # Default to 0.5\n                health_score = health_score / 100.0  # Normalize to 0-1\n            \n            # Recency score\n            recency_score = 0.0\n            if self.use_recency:\n                recency_score = self._calculate_recency_score(result)\n            \n            # Hybrid score\n            hybrid_score = (\n                self.SEMANTIC_WEIGHT * semantic_score +\n                self.GRAPH_WEIGHT * graph_score +\n                self.HEALTH_WEIGHT * health_score +\n                self.RECENCY_WEIGHT * recency_score\n            )\n            \n            result['hybrid_score'] = hybrid_score\n            result['score_breakdown'] = {\n                'semantic': semantic_score,\n                'graph': graph_score,\n                'health': health_score,\n                'recency': recency_score\n            }\n        \n        # Sort by hybrid score\n        results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)\n        \n        # Normalize if requested\n        if normalize and results:\n            max_score = results[0]['hybrid_score']\n            min_score = results[-1]['hybrid_score']\n            \n            if max_score > min_score:\n                for result in results:\n                    normalized = (result['hybrid_score'] - min_score) / (max_score - min_score)\n                    result['normalized_score'] = normalized\n        \n        return results\n    \n    def _calculate_recency_score(self, result: Dict[str, Any]) -> float:\n        \"\"\"\n        Calculate recency score based on last modified time.\n        \n        Recent files get higher scores.\n        \"\"\"\n        # This would use actual file modification time\n        # For now, return default\n        return 0.5\n    \n    def rerank_with_context(\n        self,\n        results: List[Dict[str, Any]],\n        query_context: Dict[str, Any]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Re-rank results based on query context.\n        \n        Args:\n            results: Initial results\n            query_context: Context information (error type, file, function, etc.)\n        \n        Returns:\n            Re-ranked results\n        \"\"\"\n        # Boost scores for results matching context\n        error_file = query_context.get('file')\n        error_function = query_context.get('function')\n        \n        for result in results:\n            boost = 0.0\n            \n            # Same file boost\n            if error_file and result.get('file_path') == error_file:\n                boost += 0.2\n            \n            # Same function boost (if we had function info)\n            if error_function:\n                # Would check if result contains the function\n                boost += 0.1\n            \n            # Apply boost\n            if boost > 0:\n                result['hybrid_score'] = result.get('hybrid_score', 0) * (1 + boost)\n        \n        # Re-sort\n        results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)\n        \n        return results\n    \n    def deduplicate(\n        self,\n        results: List[Dict[str, Any]],\n        similarity_threshold: float = 0.95\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Remove duplicate or highly overlapping results.\n        \n        Args:\n            results: Ranked results\n            similarity_threshold: Threshold for considering duplicates\n        \n        Returns:\n            Deduplicated results\n        \"\"\"\n        if not results:\n            return []\n        \n        unique_results = [results[0]]  # Always keep top result\n        \n        for result in results[1:]:\n            is_duplicate = False\n            \n            for unique in unique_results:\n                # Check if from same file and overlapping lines\n                if (result.get('file_path') == unique.get('file_path') and\n                    self._chunks_overlap(result, unique)):\n                    is_duplicate = True\n                    break\n            \n            if not is_duplicate:\n                unique_results.append(result)\n        \n        logger.info(f\"Deduplicated: {len(results)} -> {len(unique_results)}\")\n        \n        return unique_results\n    \n    def _chunks_overlap(self, chunk1: Dict[str, Any], chunk2: Dict[str, Any]) -> bool:\n        \"\"\"Check if two chunks overlap significantly.\"\"\"\n        start1 = chunk1.get('start_line', 0)\n        end1 = chunk1.get('end_line', 0)\n        start2 = chunk2.get('start_line', 0)\n        end2 = chunk2.get('end_line', 0)\n        \n        # Calculate overlap\n        overlap_start = max(start1, start2)\n        overlap_end = min(end1, end2)\n        \n        if overlap_end <= overlap_start:\n            return False  # No overlap\n        \n        overlap_lines = overlap_end - overlap_start\n        total_lines = max(end1 - start1, end2 - start2)\n        \n        # If overlap is more than 80% of the smaller chunk\n        return overlap_lines / total_lines > 0.8\n\n\ndef main():\n    \"\"\"CLI entry point for testing.\"\"\"\n    # Test data\n    results = [\n        {'chunk_id': 'c1', 'file_path': 'file1.py', 'score': 0.9, 'start_line': 1, 'end_line': 10},\n        {'chunk_id': 'c2', 'file_path': 'file2.py', 'score': 0.85, 'start_line': 20, 'end_line': 30},\n        {'chunk_id': 'c3', 'file_path': 'file1.py', 'score': 0.8, 'start_line': 40, 'end_line': 50},\n    ]\n    \n    graph_scores = {'c1': 0.7, 'c2': 0.9, 'c3': 0.5}\n    health_scores = {'file1.py': 80.0, 'file2.py': 60.0}\n    \n    ranker = HybridRanker(use_graph=True, use_health=True)\n    \n    ranked = ranker.rank(results, graph_scores, health_scores)\n    \n    print(\"\\n\ud83d\udcca Hybrid Ranking Results:\\n\")\n    for i, r in enumerate(ranked, 1):\n        print(f\"{i}. {r['file_path']} (chunk: {r['chunk_id']})\")\n        print(f\"   Hybrid Score: {r['hybrid_score']:.4f}\")\n        print(f\"   Breakdown: {r['score_breakdown']}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\retrieval\\semantic_search.py": "\"\"\"\nSemantic Search\n\nPerforms vector similarity search using FAISS.\n\"\"\"\n\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\nimport logging\n\nfrom ..embeddings.vector_store import VectorStore\nfrom ..embeddings.embedding_service import EmbeddingService\n\nlogger = logging.getLogger(__name__)\n\n\nclass SemanticSearch:\n    \"\"\"\n    Semantic code search using vector embeddings.\n    \"\"\"\n    \n    def __init__(\n        self,\n        vector_store: VectorStore,\n        embedding_service: EmbeddingService\n    ):\n        \"\"\"\n        Initialize semantic search.\n        \n        Args:\n            vector_store: Vector store instance\n            embedding_service: Embedding service instance\n        \"\"\"\n        self.vector_store = vector_store\n        self.embedding_service = embedding_service\n    \n    def search(\n        self,\n        query: str,\n        top_k: int = 50,\n        filter_language: Optional[str] = None,\n        min_score: float = 0.0\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for semantically similar code.\n        \n        Args:\n            query: Search query (natural language or code)\n            top_k: Number of results to return\n            filter_language: Optional language filter\n            min_score: Minimum similarity score\n        \n        Returns:\n            List of search results with metadata and scores\n        \"\"\"\n        logger.info(f\"Semantic search: '{query[:50]}...' (top_k={top_k})\")\n        \n        # Generate query embedding\n        query_embedding = self.embedding_service.encode(query, normalize=True)\n        \n        # Search vector store\n        results = self.vector_store.search(\n            query_embedding,\n            k=top_k,\n            return_scores=True\n        )\n        \n        # Filter by language if specified\n        if filter_language:\n            results = [r for r in results if r.get('language') == filter_language]\n        \n        # Filter by minimum score\n        if min_score > 0:\n            results = [r for r in results if r.get('score', 0) >= min_score]\n        \n        logger.info(f\"Found {len(results)} results\")\n        \n        return results\n    \n    def search_similar_chunks(\n        self,\n        chunk_id: str,\n        top_k: int = 20\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Find chunks similar to a given chunk.\n        \n        Args:\n            chunk_id: Chunk ID to find similar chunks for\n            top_k: Number of results\n        \n        Returns:\n            List of similar chunks\n        \"\"\"\n        # Find the chunk in metadata\n        chunk_metadata = None\n        for idx, meta in self.vector_store.metadata.items():\n            if meta.get('chunk_id') == chunk_id:\n                chunk_metadata = meta\n                break\n        \n        if not chunk_metadata:\n            logger.warning(f\"Chunk not found: {chunk_id}\")\n            return []\n        \n        # Get the embedding (we need to re-embed or store embeddings)\n        # For now, search by the chunk's file context\n        file_path = chunk_metadata.get('file_path', '')\n        \n        query = f\"Code from {file_path}\"\n        \n        return self.search(query, top_k=top_k)\n    \n    def search_by_function(\n        self,\n        function_name: str,\n        top_k: int = 10\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for code related to a function.\n        \n        Args:\n            function_name: Function name\n            top_k: Number of results\n        \n        Returns:\n            List of related code chunks\n        \"\"\"\n        query = f\"Function: {function_name}\"\n        return self.search(query, top_k=top_k)\n    \n    def multi_query_search(\n        self,\n        queries: List[str],\n        top_k_per_query: int = 10\n    ) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Perform multiple searches in parallel.\n        \n        Args:\n            queries: List of search queries\n            top_k_per_query: Results per query\n        \n        Returns:\n            Dictionary mapping queries to results\n        \"\"\"\n        results = {}\n        \n        for query in queries:\n            results[query] = self.search(query, top_k=top_k_per_query)\n        \n        return results\n    \n    def get_context_chunks(\n        self,\n        file_path: str,\n        start_line: int,\n        end_line: int,\n        expand_lines: int = 10\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get chunks overlapping with a specific code region.\n        \n        Args:\n            file_path: File path\n            start_line: Start line\n            end_line: End line\n            expand_lines: Lines to expand before/after\n        \n        Returns:\n            List of overlapping chunks\n        \"\"\"\n        # Search for chunks from the same file\n        results = []\n        \n        for idx, meta in self.vector_store.metadata.items():\n            if meta.get('file_path') != file_path:\n                continue\n            \n            chunk_start = meta.get('start_line', 0)\n            chunk_end = meta.get('end_line', 0)\n            \n            # Check for overlap (with expansion)\n            if (chunk_start <= end_line + expand_lines and \n                chunk_end >= start_line - expand_lines):\n                results.append(meta)\n        \n        # Sort by start line\n        results.sort(key=lambda x: x.get('start_line', 0))\n        \n        return results\n\n\ndef main():\n    \"\"\"CLI entry point.\"\"\"\n    import sys\n    from ..app.config import get_settings\n    \n    if len(sys.argv) < 2:\n        print(\"Usage: python semantic_search.py '<search query>'\")\n        sys.exit(1)\n    \n    query = sys.argv[1]\n    \n    settings = get_settings()\n    \n    # Initialize services\n    embedding_service = EmbeddingService(\n        model_path=settings.embedding_model_path,\n        use_gpu=True\n    )\n    \n    vector_store = VectorStore(\n        dimension=embedding_service.embedding_dim,\n        index_path=settings.vector_db_path\n    )\n    \n    # Load existing index\n    vector_store.load()\n    \n    # Perform search\n    search = SemanticSearch(vector_store, embedding_service)\n    results = search.search(query, top_k=10)\n    \n    print(f\"\\n\ud83d\udd0d Search Results for: '{query}'\")\n    print(f\"Found {len(results)} results\\n\")\n    \n    for i, result in enumerate(results, 1):\n        print(f\"{i}. {result.get('file_path', 'unknown')}\")\n        print(f\"   Lines {result.get('start_line')}-{result.get('end_line')}\")\n        print(f\"   Score: {result.get('score', 0):.4f}\")\n        print(f\"   Language: {result.get('language', 'unknown')}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "src\\retrieval\\__init__.py": "\"\"\"Empty __init__.py to make this a package\"\"\"\n",
    "tests\\test_api.py": "\"\"\"\nIntegration Tests for API Endpoints\n\"\"\"\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom src.app.main import app\n\n\n@pytest.fixture\ndef client():\n    \"\"\"Create test client.\"\"\"\n    return TestClient(app)\n\n\nclass TestHealthEndpoints:\n    \"\"\"Test health check endpoints.\"\"\"\n    \n    def test_health_check(self, client):\n        \"\"\"Test /health endpoint.\"\"\"\n        response = client.get(\"/health\")\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"status\"] == \"healthy\"\n        assert \"timestamp\" in data\n    \n    def test_root_endpoint(self, client):\n        \"\"\"Test root endpoint.\"\"\"\n        response = client.get(\"/\")\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"name\" in data\n        assert \"version\" in data\n\n\nclass TestIndexingEndpoints:\n    \"\"\"Test indexing API endpoints.\"\"\"\n    \n    def test_start_indexing(self, client):\n        \"\"\"Test POST /index/start.\"\"\"\n        payload = {\n            \"repo_path\": \"/fake/path\",\n            \"force_reindex\": False\n        }\n        \n        response = client.post(\"/index/start\", json=payload)\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"session_id\" in data\n        assert data[\"status\"] == \"started\"\n    \n    def test_get_stats(self, client):\n        \"\"\"Test GET /index/stats.\"\"\"\n        response = client.get(\"/index/stats\")\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"total_files\" in data\n\n\nclass TestSearchEndpoints:\n    \"\"\"Test search API endpoints.\"\"\"\n    \n    def test_semantic_search(self, client):\n        \"\"\"Test POST /search/semantic.\"\"\"\n        payload = {\n            \"query\": \"authentication functions\",\n            \"top_k\": 10\n        }\n        \n        response = client.post(\"/search/semantic\", json=payload)\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"query\" in data\n        assert \"results\" in data\n\n\nclass TestChatEndpoints:\n    \"\"\"Test chat API endpoints.\"\"\"\n    \n    def test_chat_completion(self, client):\n        \"\"\"Test POST /chat/completion.\"\"\"\n        payload = {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Hello\"}\n            ],\n            \"stream\": False\n        }\n        \n        response = client.post(\"/chat/completion\", json=payload)\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"message\" in data\n\n\nclass TestDebugEndpoints:\n    \"\"\"Test debug API endpoints.\"\"\"\n    \n    def test_debug_error(self, client):\n        \"\"\"Test POST /debug/error.\"\"\"\n        payload = {\n            \"error\": {\n                \"error_type\": \"TypeError\",\n                \"error_message\": \"Cannot read property\",\n                \"file_path\": \"test.py\",\n                \"line_number\": 42\n            },\n            \"auto_fix\": False\n        }\n        \n        response = client.post(\"/debug/error\", json=payload)\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"session_id\" in data\n    \n    def test_get_hotspots(self, client):\n        \"\"\"Test GET /debug/hotspots.\"\"\"\n        response = client.get(\"/debug/hotspots\")\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"hotspots\" in data\n\n\nclass TestGraphEndpoints:\n    \"\"\"Test graph API endpoints.\"\"\"\n    \n    def test_graph_overview(self, client):\n        \"\"\"Test GET /graph/overview.\"\"\"\n        response = client.get(\"/graph/overview\")\n        \n        assert response.status_code == 200\n        data = response.json()\n        assert \"nodes\" in data\n        assert \"relationships\" in data\n",
    "tests\\test_embeddings.py": "\"\"\"\nUnit Tests for Embedding Service\n\"\"\"\n\nimport pytest\nimport numpy as np\n\nfrom src.embeddings.embedding_service import EmbeddingService\n\n\n# Skip if sentence-transformers not available\npytest.importorskip(\"sentence_transformers\")\n\n\n@pytest.fixture\ndef mock_model_path(tmp_path):\n    \"\"\"Mock model path (tests will need actual model or mocking).\"\"\"\n    return str(tmp_path / \"bge-m3\")\n\n\nclass TestEmbeddingService:\n    \"\"\"Test suite for EmbeddingService.\"\"\"\n    \n    @pytest.mark.skip(reason=\"Requires BGE-M3 model download\")\n    def test_initialization(self, mock_model_path):\n        \"\"\"Test service initialization.\"\"\"\n        service = EmbeddingService(mock_model_path, use_gpu=False)\n        \n        assert service.device == 'cpu'\n        assert service.embedding_dim > 0\n    \n    @pytest.mark.skip(reason=\"Requires BGE-M3 model download\")\n    def test_encode_single_text(self, mock_model_path):\n        \"\"\"Test encoding single text.\"\"\"\n        service = EmbeddingService(mock_model_path, use_gpu=False)\n        \n        text = \"def hello(): pass\"\n        embedding = service.encode(text)\n        \n        assert isinstance(embedding, np.ndarray)\n        assert embedding.shape[0] == service.embedding_dim\n    \n    @pytest.mark.skip(reason=\"Requires BGE-M3 model download\")\n    def test_encode_batch(self, mock_model_path):\n        \"\"\"Test batch encoding.\"\"\"\n        service = EmbeddingService(mock_model_path, use_gpu=False)\n        \n        texts = [\"def func1(): pass\", \"def func2(): pass\"]\n        embeddings = service.encode(texts)\n        \n        assert isinstance(embeddings, np.ndarray)\n        assert embeddings.shape[0] == 2\n        assert embeddings.shape[1] == service.embedding_dim\n    \n    @pytest.mark.skip(reason=\"Requires BGE-M3 model download\")\n    def test_similarity(self, mock_model_path):\n        \"\"\"Test similarity calculation.\"\"\"\n        service = EmbeddingService(mock_model_path, use_gpu=False)\n        \n        text1 = \"authentication function\"\n        text2 = \"login handler\"\n        text3 = \"database connection\"\n        \n        # Similar texts should have higher similarity\n        sim_12 = service.similarity(text1, text2)\n        sim_13 = service.similarity(text1, text3)\n        \n        assert -1 <= sim_12 <= 1\n        assert -1 <= sim_13 <= 1\n        # Authentication/login should be more similar than authentication/database\n        # (This might not always hold, depends on model)\n    \n    @pytest.mark.skip(reason=\"Requires BGE-M3 model download\")\n    def test_caching(self, mock_model_path):\n        \"\"\"Test embedding caching.\"\"\"\n        service = EmbeddingService(mock_model_path, use_gpu=False)\n        \n        text = \"def test(): pass\"\n        \n        # First encoding\n        service.encode(text, use_cache=True)\n        assert service.get_cache_size() == 1\n        \n        # Second encoding (should use cache)\n        service.encode(text, use_cache=True)\n        assert service.get_cache_size() == 1\n        \n        # Clear cache\n        service.clear_cache()\n        assert service.get_cache_size() == 0\n",
    "tests\\test_error_memory.py": "\"\"\"\nUnit Tests for Error Memory Database\n\"\"\"\n\nimport pytest\nimport tempfile\nfrom pathlib import Path\n\nfrom src.memory.error_memory import ErrorMemoryDB\n\n\n@pytest.fixture\ndef temp_db():\n    \"\"\"Create temporary database for testing.\"\"\"\n    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:\n        db_path = f.name\n    \n    db = ErrorMemoryDB(db_path)\n    yield db\n    \n    db.close()\n    Path(db_path).unlink()\n\n\nclass TestErrorMemoryDB:\n    \"\"\"Test suite for ErrorMemoryDB.\"\"\"\n    \n    def test_initialization(self, temp_db):\n        \"\"\"Test database initialization.\"\"\"\n        assert temp_db.conn is not None\n    \n    def test_save_error_snapshot(self, temp_db):\n        \"\"\"Test saving error snapshot.\"\"\"\n        error_data = {\n            'error_hash': 'test_hash_123',\n            'error_type': 'TypeError',\n            'error_message': 'Cannot read property',\n            'file_path': 'test.py',\n            'line_number': 42,\n        }\n        \n        error_hash = temp_db.save_error_snapshot(error_data)\n        \n        assert error_hash == 'test_hash_123'\n    \n    def test_save_duplicate_error(self, temp_db):\n        \"\"\"Test that duplicate errors increment occurrence count.\"\"\"\n        error_data = {\n            'error_hash': 'duplicate_123',\n            'error_type': 'ValueError',\n            'error_message': 'Invalid value',\n        }\n        \n        # Save twice\n        temp_db.save_error_snapshot(error_data)\n        temp_db.save_error_snapshot(error_data)\n        \n        # Check occurrence count (would need to query database)\n        # For now, just verify no errors\n    \n    def test_save_error_resolution(self, temp_db):\n        \"\"\"Test saving error resolution.\"\"\"\n        # First save error\n        error_data = {\n            'error_hash': 'res_test_123',\n            'error_type': 'RuntimeError',\n            'error_message': 'Test error',\n        }\n        temp_db.save_error_snapshot(error_data)\n        \n        # Save resolution\n        resolution_data = {\n            'error_hash': 'res_test_123',\n            'resolution_type': 'patch',\n            'patch_applied': 'def fix(): pass',\n            'llm_model': 'gemini-2.0-flash-exp',\n            'success': True,\n            'resolution_time_ms': 1500,\n        }\n        \n        temp_db.save_error_resolution(resolution_data)\n        # No errors = success\n    \n    def test_create_debug_session(self, temp_db):\n        \"\"\"Test creating debug session.\"\"\"\n        session_id = 'session_123'\n        query = 'Why is this failing?'\n        \n        result = temp_db.create_debug_session(session_id, query)\n        \n        assert result == session_id\n    \n    def test_conversation_history(self, temp_db):\n        \"\"\"Test conversation history storage and retrieval.\"\"\"\n        session_id = 'conv_session_123'\n        \n        temp_db.create_debug_session(session_id, 'Initial query')\n        \n        # Save conversation turns\n        temp_db.save_conversation_turn(session_id, 'user', 'Hello')\n        temp_db.save_conversation_turn(session_id, 'assistant', 'Hi there!')\n        \n        # Retrieve history\n        history = temp_db.get_conversation_history(session_id)\n        \n        assert len(history) == 2\n        assert history[0]['role'] == 'user'\n        assert history[1]['role'] == 'assistant'\n    \n    def test_get_error_stats(self, temp_db):\n        \"\"\"Test error statistics retrieval.\"\"\"\n        # Add some test errors\n        for i in range(3):\n            temp_db.save_error_snapshot({\n                'error_hash': f'stat_test_{i}',\n                'error_type': 'TestError',\n                'error_message': 'Test',\n            })\n        \n        stats = temp_db.get_error_stats()\n        \n        assert stats['total_errors'] >= 3\n        assert 'resolved_errors' in stats\n        assert 'top_error_types' in stats\n",
    "tests\\test_ranking.py": "\"\"\"\nUnit Tests for Hybrid Ranking\n\"\"\"\n\nimport pytest\nfrom src.retrieval.ranking import HybridRanker\n\n\nclass TestHybridRanker:\n    \"\"\"Test suite for HybridRanker.\"\"\"\n    \n    def test_initialization(self):\n        \"\"\"Test ranker initialization.\"\"\"\n        ranker = HybridRanker()\n        \n        assert ranker.use_graph is True\n        assert ranker.use_health is True\n        assert ranker.use_recency is True\n    \n    def test_rank_with_semantic_only(self):\n        \"\"\"Test ranking with only semantic scores.\"\"\"\n        ranker = HybridRanker(use_graph=False, use_health=False, use_recency=False)\n        \n        results = [\n            {'chunk_id': 'c1', 'score': 0.9},\n            {'chunk_id': 'c2', 'score': 0.7},\n            {'chunk_id': 'c3', 'score': 0.8},\n        ]\n        \n        ranked = ranker.rank(results)\n        \n        assert ranked[0]['chunk_id'] == 'c1'  # Highest score first\n        assert ranked[1]['chunk_id'] == 'c3'\n        assert ranked[2]['chunk_id'] == 'c2'\n    \n    def test_rank_with_graph_boost(self):\n        \"\"\"Test ranking with graph relevance boost.\"\"\"\n        ranker = HybridRanker(use_graph=True, use_health=False, use_recency=False)\n        \n        results = [\n            {'chunk_id': 'c1', 'score': 0.7},\n            {'chunk_id': 'c2', 'score': 0.9},\n        ]\n        \n        graph_scores = {\n            'c1': 1.0,  # Very relevant in graph\n            'c2': 0.0,  # Not relevant in graph\n        }\n        \n        ranked = ranker.rank(results, graph_scores=graph_scores)\n        \n        # c1 should rank higher despite lower semantic score\n        assert ranked[0]['chunk_id'] == 'c1'\n    \n    def test_rank_with_health_scores(self):\n        \"\"\"Test ranking with code health consideration.\"\"\"\n        ranker = HybridRanker(use_graph=False, use_health=True, use_recency=False)\n        \n        results = [\n            {'chunk_id': 'c1', 'file_path': 'good.py', 'score': 0.8},\n            {'chunk_id': 'c2', 'file_path': 'bad.py', 'score': 0.8},\n        ]\n        \n        health_scores = {\n            'good.py': 90.0,\n            'bad.py': 30.0,\n        }\n        \n        ranked = ranker.rank(results, health_scores=health_scores)\n        \n        # Good health file should rank higher\n        assert ranked[0]['file_path'] == 'good.py'\n    \n    def test_deduplication(self):\n        \"\"\"Test duplicate chunk removal.\"\"\"\n        ranker = HybridRanker()\n        \n        results = [\n            {'chunk_id': 'c1', 'file_path': 'test.py', 'start_line': 1, 'end_line': 10, 'score': 0.9},\n            {'chunk_id': 'c2', 'file_path': 'test.py', 'start_line': 5, 'end_line': 15, 'score': 0.8},  # Overlaps\n            {'chunk_id': 'c3', 'file_path': 'other.py', 'start_line': 1, 'end_line': 10, 'score': 0.7},\n        ]\n        \n        deduplicated = ranker.deduplicate(results)\n        \n        # Should remove c2 as it overlaps with c1\n        assert len(deduplicated) == 2\n        assert any(r['chunk_id'] == 'c1' for r in deduplicated)\n        assert any(r['chunk_id'] == 'c3' for r in deduplicated)\n    \n    def test_normalized_scores(self):\n        \"\"\"Test score normalization.\"\"\"\n        ranker = HybridRanker()\n        \n        results = [\n            {'chunk_id': 'c1', 'score': 0.9},\n            {'chunk_id': 'c2', 'score': 0.5},\n        ]\n        \n        ranked = ranker.rank(results, normalize=True)\n        \n        assert 'normalized_score' in ranked[0]\n        assert ranked[0]['normalized_score'] == 1.0  # Top score normalized to 1.0\n        assert ranked[1]['normalized_score'] == 0.0  # Bottom score normalized to 0.0\n",
    "tests\\test_static_analyzer.py": "\"\"\"\nUnit Tests for Static Analyzer\n\"\"\"\n\nimport pytest\nfrom src.intelligence.static_analyzer import StaticAnalyzer\n\n\nclass TestStaticAnalyzer:\n    \"\"\"Test suite for StaticAnalyzer.\"\"\"\n    \n    def test_initialization(self):\n        \"\"\"Test analyzer initialization.\"\"\"\n        analyzer = StaticAnalyzer()\n        assert analyzer is not None\n    \n    def test_detect_long_function(self):\n        \"\"\"Test detection of long functions.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        # Create a long function (>50 lines)\n        code = \"def long_function():\\n\" + \"    pass\\n\" * 60\n        \n        smells = analyzer.analyze(code, \"python\")\n        \n        long_func_smells = [s for s in smells if s['type'] == 'long_function']\n        assert len(long_func_smells) > 0\n    \n    def test_detect_high_complexity(self):\n        \"\"\"Test detection of high cyclomatic complexity.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        # Create complex function with many branches\n        code = \"\"\"\ndef complex_function(x):\n    if x > 0:\n        if x > 10:\n            if x > 20:\n                return 1\n            return 2\n        return 3\n    return 4\n\"\"\"\n        \n        smells = analyzer.analyze(code, \"python\")\n        complexity = analyzer.calculate_complexity(code)\n        \n        assert complexity > 1\n    \n    def test_detect_many_parameters(self):\n        \"\"\"Test detection of functions with too many parameters.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        code = \"def many_params(a, b, c, d, e, f, g): pass\"\n        \n        smells = analyzer.analyze(code, \"python\")\n        \n        param_smells = [s for s in smells if s['type'] == 'too_many_parameters']\n        assert len(param_smells) > 0\n    \n    def test_detect_commented_code(self):\n        \"\"\"Test detection of commented-out code.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        code = \"\"\"\ndef test():\n    # old_function_call()\n    # another_line = 42\n    pass\n\"\"\"\n        \n        smells = analyzer.analyze(code, \"python\")\n        \n        commented_smells = [s for s in smells if s['type'] == 'commented_code']\n        assert len(commented_smells) > 0\n    \n    def test_maintainability_index(self):\n        \"\"\"Test maintainability index calculation.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        # Simple, maintainable code\n        simple_code = \"def add(a, b):\\n    return a + b\"\n        \n        index = analyzer.calculate_maintainability_index(simple_code)\n        \n        assert 0 <= index <= 100\n        assert index > 50  # Simple code should have decent maintainability\n    \n    def test_no_smells_for_clean_code(self):\n        \"\"\"Test that clean code has minimal smells.\"\"\"\n        analyzer = StaticAnalyzer()\n        \n        clean_code = \"\"\"\ndef calculate_sum(numbers):\n    \\\"\\\"\\\"Calculate sum of numbers.\\\"\\\"\\\"\n    total = 0\n    for num in numbers:\n        total += num\n    return total\n\"\"\"\n        \n        smells = analyzer.analyze(clean_code, \"python\")\n        \n        # Should have no major smells\n        assert len(smells) == 0 or all(s['severity'] == 'low' for s in smells)\n",
    "tests\\test_walker.py": "\"\"\"\nUnit Tests for File Walker\n\"\"\"\n\nimport pytest\nfrom pathlib import Path\nimport tempfile\nimport os\n\nfrom src.indexing.walker import FileWalker\n\n\n@pytest.fixture\ndef temp_repo():\n    \"\"\"Create a temporary repository structure for testing.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        repo_path = Path(tmpdir)\n        \n        # Create test files\n        (repo_path / \"main.py\").write_text(\"def main():\\n    pass\\n\")\n        (repo_path / \"utils.py\").write_text(\"def helper():\\n    return True\\n\")\n        \n        # Create subdirectory\n        sub_dir = repo_path / \"src\"\n        sub_dir.mkdir()\n        (sub_dir / \"module.py\").write_text(\"class Module:\\n    pass\\n\")\n        \n        # Create files to be excluded\n        (repo_path / \".git\").mkdir()\n        (repo_path / \"__pycache__\").mkdir()\n        (repo_path / \"node_modules\").mkdir()\n        \n        yield repo_path\n\n\nclass TestFileWalker:\n    \"\"\"Test suite for FileWalker.\"\"\"\n    \n    def test_initialization(self, temp_repo):\n        \"\"\"Test FileWalker initialization.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        \n        assert walker.root_path == temp_repo\n        assert walker.exclude_dirs is not None\n        assert walker.supported_extensions is not None\n    \n    def test_walk_finds_python_files(self, temp_repo):\n        \"\"\"Test that walker finds Python files.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        manifest = walker.walk()\n        \n        files = manifest['files']\n        file_paths = [f['path'] for f in files]\n        \n        assert any('main.py' in p for p in file_paths)\n        assert any('utils.py' in p for p in file_paths)\n        assert any('module.py' in p for p in file_paths)\n    \n    def test_walk_excludes_directories(self, temp_repo):\n        \"\"\"Test that walker excludes specified directories.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        manifest = walker.walk()\n        \n        files = manifest['files']\n        file_paths = [f['path'] for f in files]\n        \n        # Should not include files from excluded dirs\n        assert not any('.git' in p for p in file_paths)\n        assert not any('__pycache__' in p for p in file_paths)\n        assert not any('node_modules' in p for p in file_paths)\n    \n    def test_language_detection(self, temp_repo):\n        \"\"\"Test language detection.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        manifest = walker.walk()\n        \n        files = manifest['files']\n        \n        for file_data in files:\n            if file_data['path'].endswith('.py'):\n                assert file_data['language'] == 'python'\n    \n    def test_metadata_extraction(self, temp_repo):\n        \"\"\"Test that metadata is extracted correctly.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        manifest = walker.walk()\n        \n        files = manifest['files']\n        \n        assert len(files) > 0\n        \n        for file_data in files:\n            assert 'path' in file_data\n            assert 'relative_path' in file_data\n            assert 'language' in file_data\n            assert 'size_bytes' in file_data\n            assert file_data['size_bytes'] > 0\n    \n    def test_manifest_statistics(self, temp_repo):\n        \"\"\"Test manifest statistics.\"\"\"\n        walker = FileWalker(str(temp_repo))\n        manifest = walker.walk()\n        \n        assert 'stats' in manifest\n        assert manifest['stats']['total_files'] == 3\n        assert 'total_size_bytes' in manifest['stats']\n\n\n@pytest.mark.integration\ndef test_walker_with_real_structure(temp_repo):\n    \"\"\"Integration test with realistic file structure.\"\"\"\n    # Create more complex structure\n    (temp_repo / \"tests\").mkdir()\n    (temp_repo / \"tests\" / \"test_main.py\").write_text(\"def test_func():\\n    pass\\n\")\n    \n    walker = FileWalker(str(temp_repo))\n    manifest = walker.walk()\n    \n    assert manifest['stats']['total_files'] == 4\n    assert len(manifest['files']) == 4\n"
  }
}